{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133170dd",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd464b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: Self-Attention with Relative Position Representations.pdf\n",
      "Loaded 5 pages\n",
      "\n",
      "Processing: Gemma.pdf\n",
      "Loaded 21 pages\n",
      "\n",
      "Processing: glove.pdf\n",
      "Loaded 12 pages\n",
      "\n",
      "Processing: BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "Loaded 16 pages\n",
      "\n",
      "Processing: Deepseek Math.pdf\n",
      "Loaded 30 pages\n",
      "\n",
      "Total documents loaded: 84\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"*.pdf\"))\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data/pdf_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9d90af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Self-Attention with Relative Position Representations\\nPeter Shaw\\nGoogle\\npetershaw@google.com\\nJakob Uszkoreit\\nGoogle Brain\\nusz@google.com\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nAbstract\\nRelying entirely on an attention mechanism,\\nthe Transformer introduced by Vaswani et\\nal. (2017) achieves state-of-the-art results for\\nmachine translation. In contrast to recurrent\\nand convolutional neural networks, it does\\nnot explicitly model relative or absolute po-\\nsition information in its structure. Instead,\\nit requires adding representations of abso-\\nlute positions to its inputs. In this work\\nwe present an alternative approach, extend-\\ning the self-attention mechanism to efÔ¨Åciently\\nconsider representations of the relative posi-\\ntions, or distances between sequence elements.\\nOn the WMT 2014 English-to-German and\\nEnglish-to-French translation tasks, this ap-\\nproach yields improvements of 1.3 BLEU and\\n0.3 BLEU over absolute position representa-\\ntions, respectively. Notably, we observe that\\ncombining relative and absolute position rep-\\nresentations yields no further improvement in\\ntranslation quality. We describe an efÔ¨Åcient\\nimplementation of our method and cast it as an\\ninstance of relation-aware self-attention mech-\\nanisms that can generalize to arbitrary graph-\\nlabeled inputs.\\n1 Introduction\\nRecent approaches to sequence to sequence learn-\\ning typically leverage recurrence (Sutskever et al.,\\n2014), convolution (Gehring et al., 2017; Kalch-\\nbrenner et al., 2016), attention (Vaswani et al.,\\n2017), or a combination of recurrence and atten-\\ntion (Bahdanau et al., 2014; Cho et al., 2014; Lu-\\nong et al., 2015; Wu et al., 2016) as basic building\\nblocks. These approaches incorporate information\\nabout the sequential position of elements differ-\\nently.\\nRecurrent neural networks (RNNs) typically\\ncompute a hidden state ht, as a function of their\\ninput at time t and a previous hidden state ht‚àí1,\\ncapturing relative and absolute positions along the\\ntime dimension directly through their sequential\\nstructure. Non-recurrent models do not necessar-\\nily consider input elements sequentially and may\\nhence require explicitly encoding position infor-\\nmation to be able to use sequence order.\\nOne common approach is to use position encod-\\nings which are combined with input elements to\\nexpose position information to the model. These\\nposition encodings can be a deterministic func-\\ntion of position (Sukhbaatar et al., 2015; Vaswani\\net al., 2017) or learned representations. Convolu-\\ntional neural networks inherently capture relative\\npositions within the kernel size of each convolu-\\ntion. They have been shown to still beneÔ¨Åt from\\nposition encodings (Gehring et al., 2017), how-\\never.\\nFor the Transformer, which employs neither\\nconvolution nor recurrence, incorporating explicit\\nrepresentations of position information is an espe-\\ncially important consideration since the model is\\notherwise entirely invariant to sequence ordering.\\nAttention-based models have therefore used posi-\\ntion encodings or biased attention weights based\\non distance (Parikh et al., 2016).\\nIn this work we present an efÔ¨Åcient way of\\nincorporating relative position representations in\\nthe self-attention mechanism of the Transformer.\\nEven when entirely replacing its absolute position\\nencodings, we demonstrate signiÔ¨Åcant improve-\\nments in translation quality on two machine trans-\\nlation tasks.\\nOur approach can be cast as a special case of ex-\\ntending the self-attention mechanism of the Trans-\\nformer to considering arbitrary relations between\\nany two elements of the input, a direction we plan\\nto explore in future work on modeling labeled, di-\\nrected graphs.\\narXiv:1803.02155v2  [cs.CL]  12 Apr 2018'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2 Background\\n2.1 Transformer\\nThe Transformer (Vaswani et al., 2017) em-\\nploys an encoder-decoder structure, consisting of\\nstacked encoder and decoder layers. Encoder\\nlayers consist of two sublayers: self-attention\\nfollowed by a position-wise feed-forward layer.\\nDecoder layers consist of three sublayers: self-\\nattention followed by encoder-decoder attention,\\nfollowed by a position-wise feed-forward layer.\\nIt uses residual connections around each of the\\nsublayers, followed by layer normalization (Ba\\net al., 2016). The decoder uses masking in its self-\\nattention to prevent a given output position from\\nincorporating information about future output po-\\nsitions during training.\\nPosition encodings based on sinusoids of vary-\\ning frequency are added to encoder and decoder\\ninput elements prior to the Ô¨Årst layer. In contrast\\nto learned, absolute position representations, the\\nauthors hypothesized that sinusoidal position en-\\ncodings would help the model to generalize to se-\\nquence lengths unseen during training by allowing\\nit to learn to attend also by relative position. This\\nproperty is shared by our relative position repre-\\nsentations which, in contrast to absolute position\\nrepresentations, are invariant to the total sequence\\nlength.\\nResidual connections help propagate position\\ninformation to higher layers.\\n2.2 Self-Attention\\nSelf-attention sublayers employ hattention heads.\\nTo form the sublayer output, results from each\\nhead are concatenated and a parameterized linear\\ntransformation is applied.\\nEach attention head operates on an input se-\\nquence, x = (x1,...,x n) of n elements where\\nxi ‚àà Rdx , and computes a new sequence z =\\n(z1,...,z n) of the same length where zi ‚ààRdz .\\nEach output element, zi, is computed as\\nweighted sum of a linearly transformed input el-\\nements:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV ) (1)\\nEach weight coefÔ¨Åcient, Œ±ij, is computed using\\na softmax function:\\nŒ±ij = exp eij‚àën\\nk=1 exp eik\\nAnd eij is computed using a compatibility func-\\ntion that compares two input elements:\\neij = (xiWQ)(xjWK)T\\n‚àödz\\n(2)\\nScaled dot product was chosen for the compat-\\nibility function, which enables efÔ¨Åcient computa-\\ntion. Linear transformations of the inputs add suf-\\nÔ¨Åcient expressive power.\\nWQ, WK, WV ‚ààRdx√ódz are parameter matri-\\nces. These parameter matrices are unique per layer\\nand attention head.\\n3 Proposed Architecture\\n3.1 Relation-aware Self-Attention\\nWe propose an extension to self-attention to con-\\nsider the pairwise relationships between input ele-\\nments. In this sense, we model the input as a la-\\nbeled, directed, fully-connected graph.\\nThe edge between input elements xi and xj is\\nrepresented by vectors aV\\nij,aK\\nij ‚ààRda . The mo-\\ntivation for learning two distinct edge represen-\\ntations is that aV\\nij and aK\\nij are suitable for use in\\neq. (3) and eq. (4), respectively, without requiring\\nadditional linear transformations. These represen-\\ntations can be shared across attention heads. We\\nuse da = dz.\\nWe modify eq. (1) to propagate edge informa-\\ntion to the sublayer output:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV + aV\\nij) (3)\\nThis extension is presumably important for\\ntasks where information about the edge types se-\\nlected by a given attention head is useful to down-\\nstream encoder or decoder layers. However, as ex-\\nplored in 4.3, this may not be necessary for ma-\\nchine translation.\\nWe also, importantly, modify eq. (2) to consider\\nedges when determining compatibility:\\neij =\\nxiWQ(xjWK + aK\\nij )T\\n‚àödz\\n(4)\\nThe primary motivation for using simple addi-\\ntion to incorporate edge representations in eq. (3)\\nand eq. (4) is to enable an efÔ¨Åcient implementation\\ndescribed in 3.3.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='x1 x2 x3 ‚Ä¶ xnx4\\naV\\n2,1=wV\\n-1\\naK\\n2,1=wK\\n-1\\naV\\n2,4=wV\\n2\\naK\\n2,4=wK\\n2\\naV\\n4,n=wV\\nk\\naK\\n4,n=wK\\nk\\nFigure 1: Example edges representing relative posi-\\ntions, or the distance between elements. We learn rep-\\nresentations for each relative position within a clipping\\ndistance k. The Ô¨Ågure assumes 2 <= k <= n‚àí4.\\nNote that not all edges are shown.\\n3.2 Relative Position Representations\\nFor linear sequences, edges can capture infor-\\nmation about the relative position differences be-\\ntween input elements. The maximum relative po-\\nsition we consider is clipped to a maximum abso-\\nlute value of k. We hypothesized that precise rel-\\native position information is not useful beyond a\\ncertain distance. Clipping the maximum distance\\nalso enables the model to generalize to sequence\\nlengths not seen during training. Therefore, we\\nconsider 2k+ 1unique edge labels.\\naK\\nij = wK\\nclip(j‚àíi,k)\\naV\\nij = wV\\nclip(j‚àíi,k)\\nclip(x,k) = max(‚àík,min(k,x))\\nWe then learn relative position representations\\nwK = (wK\\n‚àík,...,w K\\nk ) and wV = (wV\\n‚àík,...,w V\\nk )\\nwhere wK\\ni ,wV\\ni ‚ààRda .\\n3.3 EfÔ¨Åcient Implementation\\nThere are practical space complexity concerns\\nwhen considering edges between input elements,\\nas noted by VeliÀáckovi¬¥c et al. (2017), which consid-\\ners unlabeled graph inputs to an attention model.\\nFor a sequence of length n and h attention\\nheads, we reduce the space complexity of storing\\nrelative position representations from O(hn2da)\\nto O(n2da) by sharing them across each heads.\\nAdditionally, relative position representations can\\nbe shared across sequences. Therefore, the over-\\nall self-attention space complexity increases from\\nO(bhndz) to O(bhndz + n2da). Given da = dz,\\nthe size of the relative increase depends on n\\nbh.\\nThe Transformer computes self-attention efÔ¨Å-\\nciently for all sequences, heads, and positions in\\na batch using parallel matrix multiplication opera-\\ntions (Vaswani et al., 2017). Without relative posi-\\ntion representations, each eij can be computed us-\\ning bhparallel multiplications of n√ódz and dz √ón\\nmatrices. Each matrix multiplication computes eij\\nfor all sequence positions, for a particular head\\nand sequence. For any sequence and head, this\\nrequires sharing the same representation for each\\nposition across all compatibility function applica-\\ntions (dot products) with other positions.\\nWhen we consider relative positions the repre-\\nsentations differ with different pairs of positions.\\nThis prevents us from computing all eij for all\\npairs of positions in a single matrix multiplication.\\nWe also want to avoid broadcasting relative po-\\nsition representations. However, both issues can\\nbe resolved by splitting the computation of eq. (4)\\ninto two terms:\\neij =\\nxiWQ(xjWK)T + xiWQ(aK\\nij )T\\n‚àödz\\n(5)\\nThe Ô¨Årst term is identical to eq. (2), and can be\\ncomputed as described above. For the second term\\ninvolving relative position representations, tensor\\nreshaping can be used to computenparallel multi-\\nplications of bh√ódz and dz√ónmatrices. Each ma-\\ntrix multiplication computes contributions to eij\\nfor all heads and batches, corresponding to a par-\\nticular sequence position. Further reshaping al-\\nlows adding the two terms. The same approach\\ncan be used to efÔ¨Åciently compute eq. (3).\\nFor our machine translation experiments, the re-\\nsult was a modest 7% decrease in steps per sec-\\nond, but we were able to maintain the same model\\nand batch sizes on P100 GPUs as Vaswani et\\nal. (2017).\\n4 Experiments\\n4.1 Experimental Setup\\nWe use the tensor2tensor 1 library for training and\\nevaluating our model.\\nWe evaluated our model on the WMT 2014\\nmachine translation task, using the WMT 2014\\nEnglish-German dataset consisting of approxi-\\nmately 4.5M sentence pairs and the 2014 WMT\\nEnglish-French dataset consisting of approxi-\\nmately 36M sentence pairs.\\n1The tensor2tensor library is available at https://\\ngithub.com/tensorflow/tensor2tensor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Model Position Information EN-DE BLEU EN-FR BLEU\\nTransformer (base) Absolute Position Representations 26.5 38.2\\nTransformer (base) Relative Position Representations 26.8 38.7\\nTransformer (big) Absolute Position Representations 27.9 41.2\\nTransformer (big) Relative Position Representations 29.2 41.5\\nTable 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) trans-\\nlation tasks, using newstest2014 test set.\\nFor all experiments, we split tokens into a\\n32,768 word-piece vocabulary (Wu et al., 2016).\\nWe batched sentence pairs by approximate length,\\nand limited input and output tokens per batch to\\n4096 per GPU. Each resulting training batch con-\\ntained approximately 25,000 source and 25,000\\ntarget tokens.\\nWe used the Adam optimizer (Kingma and Ba,\\n2014) with Œ≤1 = 0.9, Œ≤2 = 0.98, and œµ = 10‚àí9.\\nWe used the same warmup and decay strategy for\\nlearning rate as Vaswani et al. (2017), with 4,000\\nwarmup steps. During training, we employed la-\\nbel smoothing of value œµls = 0.1 (Szegedy et al.,\\n2016). For evaluation, we used beam search with\\na beam size of 4 and length penalty Œ±= 0.6 (Wu\\net al., 2016).\\nFor our base model, we used 6 encoder and de-\\ncoder layers, dx = 512, dz = 64, 8 attention\\nheads, 1024 feed forward inner-layer dimensions,\\nand Pdropout = 0.1. When using relative posi-\\ntion encodings, we used clipping distance k= 16,\\nand used unique edge representations per layer and\\nhead. We trained for 100,000 steps on 8 K40\\nGPUs, and did not use checkpoint averaging.\\nFor our big model, we used 6 encoder and de-\\ncoder layers, dx = 1024, dz = 64, 16 attention\\nheads, 4096 feed forward inner-layer dimensions,\\nand Pdropout = 0.3 for EN-DE and Pdropout = 0.1\\nfor EN-FR. When using relative position encod-\\nings, we used k = 8, and used unique edge repre-\\nsentations per layer. We trained for 300,000 steps\\non 8 P100 GPUs, and averaged the last 20 check-\\npoints, saved at 10 minute intervals.\\n4.2 Machine Translation\\nWe compared our model using only relative po-\\nsition representations to the baseline Transformer\\n(Vaswani et al., 2017) with sinusoidal position en-\\ncodings. We generated baseline results to iso-\\nlate the impact of relative position representations\\nfrom any other changes to the underlying library\\nand experimental conÔ¨Åguration.\\nFor English-to-German our approach improved\\nperformance over our baseline by 0.3 and 1.3\\nBLEU for the base and big conÔ¨Ågurations, respec-\\ntively. For English-to-French it improved by 0.5\\nand 0.3 BLEU for the base and big conÔ¨Ågurations,\\nrespectively. In our experiments we did not ob-\\nserve any beneÔ¨Åt from including sinusoidal posi-\\ntion encodings in addition to relative position rep-\\nresentations. The results are shown in Table 1.\\n4.3 Model Variations\\nWe performed several experiments modifying var-\\nious aspects of our model. All of our experi-\\nments in this section use the base model conÔ¨Ågura-\\ntion without any absolute position representations.\\nBLEU scores are calculated on the WMT English-\\nto-German task using the development set, new-\\nstest2013.\\nWe evaluated the effect of varying the clipping\\ndistance, k, of the maximum absolute relative po-\\nsition difference. Notably, for k ‚â•2, there does\\nnot appear to be much variation in BLEU scores.\\nHowever, as we use multiple encoder layers, pre-\\ncise relative position information may be able to\\npropagate beyond the clipping distance. The re-\\nsults are shown in Table 2.\\nk EN-DE BLEU\\n0 12.5\\n1 25.5\\n2 25.8\\n4 25.9\\n16 25.8\\n64 25.9\\n256 25.8\\nTable 2: Experimental results for varying the clipping\\ndistance, k.\\nWe also evaluated the impact of ablating each of\\nthe two relative position representations deÔ¨Åned in\\nsection 3.1, aV\\nij in eq. (3) andaK\\nij in eq. (4). Includ-\\ning relative position representations solely when\\ndetermining compatibility between elements may'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='be sufÔ¨Åcient, but further work is needed to deter-\\nmine whether this is true for other tasks. The re-\\nsults are shown in Table 3.\\naVij aKij EN-DE BLEU\\nYes Yes 25.8\\nNo Yes 25.8\\nYes No 25.3\\nNo No 12.5\\nTable 3: Experimental results for ablating relative po-\\nsition representations aV\\nij and aK\\nij .\\n5 Conclusions\\nIn this paper we presented an extension to self-\\nattention that can be used to incorporate rela-\\ntive position information for sequences, which im-\\nproves performance for machine translation.\\nFor future work, we plan to extend this mecha-\\nnism to consider arbitrary directed, labeled graph\\ninputs to the Transformer. We are also inter-\\nested in nonlinear compatibility functions to com-\\nbine input representations and edge representa-\\ntions. For both of these extensions, a key consid-\\neration will be determining efÔ¨Åcient implementa-\\ntions.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\\nton. 2016. Layer normalization. arXiv preprint\\narXiv:1607.06450 .\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint\\narXiv:1409.0473 .\\nKyunghyun Cho, Bart Van Merri ¬®enboer, Caglar Gul-\\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. 2014. Learning\\nphrase representations using rnn encoder-decoder\\nfor statistical machine translation. arXiv preprint\\narXiv:1406.1078 .\\nJonas Gehring, Michael Auli, David Grangier, De-\\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\\ntional sequence to sequence learning. arXiv preprint\\narXiv:1705.03122 .\\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\\nAaron van den Oord, Alex Graves, and Koray\\nKavukcuoglu. 2016. Neural machine translation in\\nlinear time. arXiv preprint arXiv:1610.10099.\\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980 .\\nMinh-Thang Luong, Hieu Pham, and Christopher D\\nManning. 2015. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint\\narXiv:1508.04025 .\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In Empirical\\nMethods in Natural Language Processing.\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\\n2015. End-to-end memory networks. In Advances\\nin neural information processing systems . pages\\n2440‚Äì2448.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural net-\\nworks. In Advances in neural information process-\\ning systems. pages 3104‚Äì3112.\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\\nthe inception architecture for computer vision. In\\nProceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition. pages 2818‚Äì2826.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems. pages 6000‚Äì6010.\\nPetar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova,\\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\\n2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 .\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144 .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2024-06-27\\nGemma 2: Improving Open Language Models\\nat a Practical Size\\nGemma Team, Google DeepMind1\\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art\\nopen models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply\\nseveral known technical modifications to the Transformer architecture, such as interleaving local-global\\nattentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B\\nand 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The\\nresulting models deliver the best performance for their size, and even offer competitive alternatives to\\nmodels that are 2-3√óbigger. We release all our models to the community.\\n1. Introduction\\nLarge language models (LLMs) have demon-\\nstrated strong capabilities in language under-\\nstanding,generation,andreasoning(Brownetal.,\\n2020; Radford et al., 2019; Raffel et al., 2019).\\nScaling has been key to this recent progress,\\nwith many new capabilities only emerging at\\nscale (Brown et al., 2020). The newest large mod-\\nels not only reach unprecedented performance\\non reasoning benchmarks (Achiam et al., 2023),\\nbut they also demonstrate multimodal and mul-\\ntilingual capabilities (Gemini Team, 2024) and\\neven the ability to use context lengths of over 1M\\ntokens (Gemini Team, 2024).\\nSmall-scale models have also shown a rapid\\nincrease in performance, but these gains are\\nlargelyderivedfromincreasingthelengthoftrain-\\ning (Gemma Team, 2024; Jiang et al., 2023; Tou-\\nvron et al., 2023). This approach only scales log-\\narithmically with dataset size (Hoffmann et al.,\\n2022), and the latest small models require up to\\n15T tokens to improve the state of the art by less\\nthan 1-2% (AI@Meta, 2024).\\nYet, these continued improvements provide ev-\\nidence that small models are still under-trained.\\nIn this work, we explore alternatives to improve\\nsmall model performance without solely increas-\\ning training length. One solution is to improve\\nthe quality of information received by the net-\\nwork at each training step by replacing the next\\ntoken prediction task with a richer objective.\\nInparticular, wefocusoureffortsonknowledge\\ndistillation (Hinton et al., 2015), which replaces\\nthe one-hot vector seen at each token with the\\ndistribution of potential next tokens computed\\nfrom a large model. This approach is often used\\nto reduce the training time of smaller models\\nby giving them richer gradients. In this work,\\nwe instead train for large quantities of tokens\\nwith distillation in order to simulate training be-\\nyond the number of available tokens. Concretely,\\nwe use a large language model as a teacher to\\ntrain small models, namely 2B and 9B models,\\non a quantity of tokens that is more than 50√ó\\nthe compute-optimal quantity predicted by the\\ntheory (Hoffmann et al., 2022). Along with the\\nmodels trained with distillation, we also release\\na 27B model trained from scratch for this work.\\nWe also leverage several known modifications\\nofTransformers,namelytheinterleavingofglobal\\nand local attention layers from Beltagy et al.\\n(2020a),andtheGrouped-QueryAttention(GQA)\\nmechanism of Ainslie et al. (2023).\\nOverall, Gemma 2 significantly advances state-\\nof-the-art performance relative to comparable-\\nscale open models and are even competitive\\nwith some models more than twice their size\\n(AI@Meta, 2024; Almazrouei et al., 2023; Jiang\\net al., 2023; xAI, 2024), across a variety of au-\\ntomated benchmarks and human evaluations.\\nExample domains include question answering\\n(Clark et al., 2019; Kwiatkowski et al., 2019),\\ncommonsense reasoning (Sakaguchi et al., 2019;\\nSuzgun et al., 2022), mathematics and science\\n(Cobbe et al., 2021; Hendrycks et al., 2020), and\\ncoding (Austin et al., 2021; Chen et al., 2021).\\n1See Contributions and Acknowledgments section for full author list. Please send correspondence togemma-2-report@google.com.\\n¬© 2024 Google DeepMind. All rights reserved\\narXiv:2408.00118v3  [cs.CL]  2 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nParameters 2B 9B 27B\\nd_model 2304 3584 4608\\nLayers 26 42 46\\nPre-norm yes yes yes\\nPost-norm yes yes yes\\nNon-linearity GeGLU GeGLU GeGLU\\nFeedforward dim 18432 28672 73728\\nHead type GQA GQA GQA\\nNum heads 8 16 32\\nNum KV heads 4 8 16\\nHead size 256 256 128\\nGlobal att. span 8192 8192 8192\\nSliding window 4096 4096 4096\\nVocab size 256128 256128 256128\\nTied embedding yes yes yes\\nTable 1|Overview of the main model parameters\\nand design choices. See the section on model\\narchitectures for more details.\\nWhile thorough testing of our models has been\\nconducted, these tests cannot cover all applica-\\ntions and scenarios in which Gemma 2 may be\\nused. Withthisinmind,allGemma2usersshould\\nconduct rigorous safety testing specific to their\\nuse case before deployment or use.\\nIn this technical report, we provide an overview\\nof models, including the architecture, training,\\nand pre- and post-training recipes for Gemma\\n2. We also provide detailed evaluations across a\\nwidevarietyofquantitativeandqualitativebench-\\nmarks, as well as both standard academic bench-\\nmarksandhuman-preferenceevaluations. Finally,\\nwe discuss our approach to safe and responsible\\ndeployment and outline the broader implications\\nof Gemma 2, its limitations, and advantages.\\n2. Model Architecture\\nSimilar to previous Gemma models (Gemma\\nTeam,2024), theGemma2modelsarebasedona\\ndecoder-only transformer architecture (Vaswani\\netal.,2017). Wesummarizethemainparameters\\nand architecture choices in Table 1.\\nA few architectural elements are similar to the\\nfirst version of Gemma models; namely, a context\\nModel Embedding\\nParameters\\nNon-embedding\\nParameters\\n2B 590,118,912 2,024,517,888\\n9B 917,962,752 8,324,201,984\\n27B 1,180,237,824 26,047,480,320\\nTable 2|Parameter counts for the Gemma mod-\\nels. We inherit from the large Gemini vocabulary\\n(256k entries), that is designed to work on a large\\nnumber of languages, hence, the larger embed-\\nding parameter counts compared to models that\\nare limited to one or a few languages.\\nlength of 8192 tokens, the use of Rotary Posi-\\ntion Embeddings (RoPE) (Su et al., 2021), and\\nthe approximated GeGLU non-linearity (Shazeer,\\n2020). A few elements differ between Gemma 1\\nand Gemma 2, including using deeper networks.\\nWe summarize the key differences below.\\nLocal Sliding Window and Global Attention.\\nWe alternate between a local sliding window at-\\ntention (Beltagy et al., 2020a,b) and global at-\\ntention (Luong et al., 2015) in every other layer.\\nThe sliding window size of local attention layers\\nis set to 4096 tokens, while the span of the global\\nattention layers is set to 8192 tokens.\\nLogit soft-capping. We cap logits (Bello et al.,\\n2016) in each attention layer and the final layer\\nsuch that the value of the logits stays between\\n‚àísoft_cap and +soft_cap. More specifically, we\\ncap the logits with the following function:\\nlogits ‚Üêsoft_cap‚àótanh(logits/soft_cap).\\nWe set the soft_cap parameter to50.0 for the self-\\nattention layers and to30.0 for the final layer.\\nPost-norm and pre-norm with RMSNorm. To\\nstabilize training, we use RMSNorm (Zhang and\\nSennrich, 2019) to normalize the input and out-\\nput of each transformer sub-layer, the attention\\nlayer, and the feedforward layer.\\nGrouped-Query Attention(Ainslie et al., 2023).\\nWe use GQA withnum_groups = 2, based on ab-\\nlations showing increased speed at inference time\\nwhile maintaining downstream performance.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n3. Pre-training\\nWe provide a brief overview of the parts of our\\npre-training that differs from Gemma 1.\\n3.1. Training Data\\nWe train Gemma 2 27B on 13 trillion tokens of\\nprimarily-English data, the 9B model on 8 trillion\\ntokens, and the 2B on 2 trillion tokens. These\\ntokens come from a variety of data sources, in-\\ncluding web documents, code, and science ar-\\nticles. Our models are not multimodal and are\\nnot trained specifically for state-of-the-art multi-\\nlingual capabilities. The final data mixture was\\ndetermined through ablations similar to the ap-\\nproach in Gemini 1.0 (Gemini Team, 2023).\\nTokenizer.We use the same tokenizer as Gemma\\n1 and Gemini: a SentencePiece tokenizer with\\nsplit digits, preserved whitespace, and byte-level\\nencodings (Kudo and Richardson, 2018). The\\nresulting vocabulary has 256k entries.\\nFiltering. We use the same data filtering tech-\\nniques as Gemma 1. Specifically, we filter the pre-\\ntraining dataset to reduce the risk of unwanted\\nor unsafe utterances, filter out certain personal\\ninformation or other sensitive data, decontami-\\nnate evaluation sets from our pre-training data\\nmixture, and reduce the risk of recitation by min-\\nimizing the proliferation of sensitive outputs.\\nShards\\nModel Type #Chips Data Model\\n2B TPUv5e 512 512 1\\n9B TPUv4 4096 1024 4\\n27B TPUv5p 6144 768 8\\nTable 3|Training infrastructure with sharding.\\n3.2. Knowledge Distillation\\nGiven a large model used as a teacher, we learn\\nsmaller models by distilling from the probability\\ngiven by the teacher of each tokenùë• given its\\ncontext ùë•ùëê, i.e., ùëÉùëá(ùë• |ùë•ùëê). More precisely, we\\nminimize the negative log-likelihood between the\\nContext Relevant Token\\nUser turn user\\nModel turn model\\nStart of conversation turn <start_of_turn>\\nEnd of conversation turn <end_of_turn>\\nBeginning of sequence <bos>\\nEnd of sequence <eos>\\nTable 4|Relevant formatting control tokens used\\nfor Gemma models.\\nprobabilities from the teacher and the student:\\nmin\\nùëÉùëÜ\\n‚àëÔ∏Å\\nùë•\\n‚àíùëÉùëá(ùë• |ùë•ùëê)log ùëÉùëÜ(ùë• |ùë•ùëê),\\nwhere ùëÉùëÜ is the parameterized probability of the\\nstudent. Note that knowledge distillation was\\nalso used in Gemini 1.5 (Gemini Team, 2024).\\n3.3. Compute Infrastructure\\nWe train our models with TPUv4, TPUv5e, and\\nTPUv5p as outlined in Table 3. For the 2B model,\\nwe train on a 2x16x16 configuration of TPUv5e,\\ntotaling 512 chips, with 512-way data replication\\nand 1-way model sharding. For the 9B model,\\nwe train on an 8x16x32 configuration of TPUv4,\\ntotaling 4096 chips, with 1024-way data repli-\\ncation and 4-way model sharding. For the 27B\\nmodel, we train on an 8x24x32 configuration of\\nTPUv5p, totaling 6144 chips, with 768-way data\\nreplication and 8-way model sharding.\\nThe optimizer state is further sharded using\\ntechniques similar to ZeRO-3 (Ren et al., 2021).\\nFor scales beyond a single pod, we perform a\\ndata-replica reduction over the data center net-\\nwork, using the Pathways approach of Barham\\net al. (2022). We also use the ‚Äôsingle controller‚Äô\\nprogramming paradigm of Jax (Roberts et al.,\\n2023) and Pathways (Barham et al., 2022). As\\nin Gemma 1, we use the GSPMD partitioner (Xu\\net al., 2021) for training step computation and\\nthe MegaScale XLA compiler (XLA, 2019).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n3.4. Carbon Footprint\\nWe estimate the carbon emissions from pre-\\ntrainingtheGemmamodelstobe 1247.61 ùë°ùê∂ùëÇ2ùëíùëû.\\nAs in Gemma 1 (Gemma Team, 2024), this value\\nis calculated based on the hourly energy usage\\nreported directly from our TPU data centers and\\nscaled to account for the additional energy ex-\\npended to create and maintain the data center.\\nImportantly, Google data centers are carbon neu-\\ntral, achieved through a combination of energy\\nefficiency, renewable energy purchases, and car-\\nbon offsets. This carbon neutrality applies to our\\nexperiments and the machines running them.\\n4. Post-Training\\nFor post-training, we fine-tune our pre-trained\\nmodels into instruction-tuned models. First, we\\napply supervised fine-tuning (SFT) on a mix\\nof text-only, English-only synthetic and human-\\ngenerated prompt-response pairs. We then apply\\nRLHF on top of these models with the reward\\nmodeltrainedonlabelledEnglish-onlypreference\\ndata and the policy based on the same prompts\\nas the SFT phase. Finally, we average the mod-\\nels obtained after each phase to improve their\\noverall performance. The final data mixtures and\\npost-training recipe, which includes tuned hyper-\\nparameters, were chosen on the basis of improv-\\ning helpfulness while minimizing model harms\\nrelated to safety and hallucinations.\\nWe extended the post-training data from\\nGemma 1.1 with a mixture of internal and exter-\\nnal public data. In particular, we use the prompts,\\nbut not the answers from LMSYS-chat-1M (Zheng\\net al., 2023). All of our data go through a filtering\\nstage described below.\\nSupervised fine-tuning (SFT).We run behav-\\nioral cloning on synthetic and real prompts, and\\nresponses predominantly synthetically generated\\nby the teacher, that is a larger model. We also run\\ndistillation from the teacher on the student‚Äôs dis-\\ntribution (Agarwal et al., 2024; Gu et al., 2024).\\nReinforcement Learning from Human Feed-\\nback (RLHF).We use a similar RLHF algorithm\\nas Gemma 1.1 (Gemma Team, 2024) but a differ-\\nentrewardmodel,whichisanorderofmagnitude\\nFirst turn\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who‚Äôs there?<end_of_turn><eos>\\nSecond turn\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who‚Äôs there?<end_of_turn>\\nUser: <start_of_turn>user\\nGemma.<end_of_turn>\\n<start_of_turn>model\\nModel: Gemma who?<end_of_turn><eos>\\nTable 5|Example dialogue with user and model\\ncontrol tokens. To proceed with multi-turn, re-\\nmove the model-outputted<eos>, add back the\\nusualuserturn‚Äôscontroltokensandcontinuewith\\nthe following turn‚Äôs chat template.\\nlarger than the policy. The new reward model is\\nalso oriented more towards conversational capa-\\nbilities, specifically multi-turn.\\nModel merging. We average different models\\nobtained by running our pipeline with different\\nhyperparameters (Ram√© et al., 2024).\\nData filtering. When using synthetic data, we\\nrun several stages of filtering to remove examples\\nthat show certain personal information, unsafe or\\ntoxic model outputs, mistaken self-identification\\ndata, and duplicated examples. Following Gem-\\nini, we find that including subsets of data that\\nencourage better in-context attribution, hedging,\\nand refusals to minimize hallucinations improves\\nperformance on factuality metrics, without de-\\ngrading model performance on other metrics.\\nFormatting. Gemma 2 models are fine-tuned\\nwith the same control tokens as Gemma 1 models,\\nas detailed in Table 4, but a different formatting\\nschema. See the dialogue example in Table 5.\\nNotice that the model explicitly ends generations\\nwith <end_of_turn><eos> tokens, while previ-\\nouslyitonlygenerated <eos>. Forthemotivation\\nbehind this formatting structure, see Gemma 1.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n5. Ablations\\nIn this section, we focus on the main finding of\\nthis work, which is the impact of knowledge dis-\\ntillation on small language models.\\nfrom scratch distilled\\nAverage (3 bench.) 60.3 67.7\\nTable 6|Comparison between a 2B model trained\\nover 500B tokens either from scratch or with dis-\\ntillation from a 7B model.\\nDistillation versus from scratch.In Table 6, we\\nshow that distilling from a larger model improves\\nperformance compared to training from scratch.\\nNote that 500B is 10√ómore than the compute-\\noptimal number of tokens for a 2B model. We\\ndistill from a 7B model to keep a ratio similar to\\nour target distillation from 27B to 9B.\\n200M 400M 1B\\nfrom scratch 23 19 17\\ndistilled (7B) 21 17 15\\nTable 7|Perplexity measured on a validation set\\nof models of different sizes trained with or with-\\nout distillation. The teacher has 7B parameters.\\nImpact of distillation w.r.t. model size.In Ta-\\nble 7, we measure the impact of distillation as\\nmodel size increases. We observe that the gain re-\\nmains as the model size is scaled. In this ablation,\\nwe maintain the size of the teacher at 7B and\\ntrain smaller models to simulate the same gap as\\nbetween our final teacher and student sizes.\\nMHA GQA\\nAverage (4 bench.) 50.3 50.8\\nTable8 |ComparingtheimpactofreplacingMulti-\\nHead Attention (MHA) with GQA on a 9B model\\naveraged over 4 benchmarks.\\nGQA versus MHA.In Table 8, we compare two\\ninstancesofour9BwithMHAorGQA.Weobserve\\noverall few changes in performance between both\\nmodels as measured on several benchmarks. We\\nchoose GQA since it requires fewer parameters\\nand is faster at inference time.\\nWide versus deep.In Table 9, we show that a\\ndeeper 9B network is slightly better than a wider\\n9B for the same number of parameters. Although\\nthegapissmall,itisconsistentacrossbenchmarks\\nand warrants the switch to a deeper architecture.\\nWide Deep\\nAverage (4 bench.) 50.8 52.0\\nTable 9|Wide versus deep 9B models. Perfor-\\nmance on 4 benchmarks, higher is better.\\nChanging sliding window size.In Table 10, we\\nshow that we can change the sliding window size\\nof the local attention layers of the models during\\ninference with moderate impact on perplexity.\\nAdjusting the size of the sliding window can thus\\nbe a leverage for slight inference speed gain.\\nsliding window 4096 2048 1024\\nperplexity (val. set) 1.63 1.63 1.64\\nTable 10|Impact of changing the sliding window\\nsize at inference time for the 9B model.\\nImpact of formatting.We measure performance\\nvariance on MMLU across prompt/evaluation for-\\nmatting variations. Table 11 shows the stan-\\ndard deviations of MMLU scores for 12 format-\\nting/evaluation combinations, a proxy for unde-\\nsired performance variability. The Gemma 2B\\nmodels are slightly less format-robust than the\\nlarger ones. Notably, Mistral 7B is significantly\\nless robust than our models.\\nStandard Deviation\\nGemma 1 2B 1.5\\nGemma 2 2B 2.1\\nMistral 7B 6.9\\nGemma 1 7B 0.7\\nGemma 2 9B 0.9\\nGemma 2 27B 1.0\\nTable 11|Standard deviations of MMLU scores\\nfor12combinationsofformattingandevaluation.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n6. Evaluation\\nIn this section, we evaluate both pre-trained and\\nIT models over a series of automated benchmarks\\nand human evaluations across a variety of do-\\nmains. We also report performance from models\\nof similar sizes that have permissive licenses, or\\nas reported by others. Note that we consider to-\\ntal parameters, not active parameters, since total\\nmemoryusageisoftenwhatlimitstheuseofopen\\nmodels on standard devices.\\n6.1. Pre-training Evaluations\\nEvaluating the 27B model\\nIn this set of evaluations, we evaluate the perfor-\\nmance of our 27B model trained without distilla-\\ntion on 13T tokens. We report results in Table 12,\\nwhere we compare with a model of similar size,\\nQwen1.5 34B (Team, 2024), and a model 2.5√ó\\nlarger, LLaMA-3 70B on the HuggingFace evalu-\\nation suite. We selected these models based on\\ntheir ranking on the HuggingFace leaderboard.\\nOverall, we observe that our model is the best\\nin its size category and is even competitive with\\na larger model that is trained for longer. That\\nbeing said, the performance of models trained in\\na similar fashion improves only logarithmically\\nwith their size and hence, our model is likely in\\nthe same Pareto curve as the LLaMA-3 models.\\nHowever, it is not clear how these differences\\naffect the quality of the resulting IT models.\\nEvaluating the 2B and 9B models\\nIn this set of experiments, we compare our new\\n2B and 9B trained with distillation to our previ-\\nous models and several standard open models\\nin Gemma Team (2024).\\nWe observe overall a massive improvement in\\nour models compared to previous versions, by up\\nto 10% in some benchmarks for the 9B model.\\nThe two 2B models were trained with a similar\\nnumber of tokens (2T for Gemma 2 and 3T for\\nGemma 1) and we still observe a significant im-\\nprovementforthenewmodels. Thisconfirmsthat\\ndistillation significantly improves the quality of\\nmodels even when trained on the same number\\nof tokens.\\nLLaMA-3 Qwen1.5 Gemma-2\\n70B 32B 27B\\nMMLU 79.2 74.3 75.2\\nGSM8K 76.9 61.1 74.0\\nARC-c 68.8 63.6 71.4\\nHellaSwag 88.0 85.0 86.4\\nWinogrande 85.3 81.5 83.7\\nTable 12 | We compare, on the HuggingFace\\nbenchmark, our 27B model with a competitive\\nopen model, Qwen1.5 32B, that has a similar size.\\nWe also report the performance of LLaMA-3 70B\\nfor completeness. Note that our model outper-\\nforms Qwen1.5 32B and is only a few percent\\nbelow LLaMA-3 70B despite being 2.5√ósmaller\\nand trained on 2/3rds less data.\\n6.2. Post-training Evaluations\\nIn this section, we evaluate our IT models on a\\nset of human evaluations as well as standard aca-\\ndemic benchmarks. The Gemma 2 models push\\nthe frontier for post-trained open-weights mod-\\nels, setting a new state of the art on the LMSYS\\nChatbot Arena (Chiang et al., 2024).\\nLMSYS Chatbot Arena\\nGemma 2 Instruction Tuned models were evalu-\\nated on the Chatbot Arena (Chiang et al., 2024)\\nin blind side by side evaluations by human raters\\nagainst other state of the art models. We re-\\nport Elo scores in Table 14. Gemma 2.6B, 9B\\nand 27B strongly outperform all other open mod-\\nels in the same range of parameters, with no-\\ntably: Gemma27B(Elo1218)rankedhigherthan\\nLlama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\\n0613 (Elo 1116).\\nHuman Preference Evaluations\\nWe also submit Gemma IT models for side-by-\\nside human evaluation studies (which are in-\\ndependent from the Chatbot Arena). We used\\nheld-out collections of single-turn prompts that\\ntarget safety and instruction following (IF). We\\nuse gpt4o-2024-05-13 as the base model, and\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nGemma-1 Gemma-2 Mistral LLaMA-3 Gemma-1 Gemma-2 Gemma-2\\nBenchmark metric 2B 2B 7B 8B 7B 9B 27B\\nMMLU 5-shot 42.3 52.2 62.5 66.6 64.4 71.3 75.2\\nARC-C 25-shot 48.5 55.7 60.5 59.2 61.1 68.4 71.4\\nGSM8K 5-shot 15.1 24.3 39.6 45.7 51.8 68.6 74.0\\nAGIEval 3-5-shot 24.2 31.5 44.0‚Ä† 45.9‚Ä† 44.9‚Ä† 52.8 55.1\\nDROP 3-shot, F1 48.5 51.2 63.8‚àó 58.4 56.3 69.4 74.2\\nBBH 3-shot, CoT 35.2 41.9 56.0‚ãÑ 61.1‚ãÑ 59.0‚ãÑ 68.2 74.9\\nWinogrande 5-shot 66.8 71.3 78.5 76.1 79.0 80.6 83.7\\nHellaSwag 10-shot 71.7 72.9 83.0 82.0 82.3 81.9 86.4\\nMATH 4-shot 11.8 16.0 12.7 - 24.3 36.6 42.3\\nARC-e 0-shot 73.2 80.6 80.5 - 81.5 88.0 88.6\\nPIQA 0-shot 77.3 78.4 82.2 - 81.2 81.7 83.2\\nSIQA 0-shot 49.7 51.9 47.0‚àó - 51.8 53.4 53.7\\nBoolq 0-shot 69.4 72.7 83.2‚àó - 83.2 84.2 84.8\\nTriviaQA 5-shot 53.2 60.4 62.5 - 63.4 76.6 83.7\\nNQ 5-shot 12.5 17.1 23.2 - 23.0 29.2 34.5\\nHumanEval pass@1 22.0 20.1 26.2 - 32.3 40.2 51.8\\nMBPP 3-shot 29.2 30.2 40.2‚àó - 44.4 52.4 62.6\\nAverage (8) 44.0 50.0 61.0 61.9 62.4 70.2 74.4\\nAverage (all) 44.2 48.7 55.6 - 57.9 64.9 69.4\\nTable 13|Comparison of models in the range of 2B to 9B parameters, as well as our 27B model, on\\na variety of benchmarks. We report the average performance on the 8 benchmarks where we can\\ncompare with LLaMA-3, and on all the benchmarks (all). The numbers for LLaMA-3 8B are either\\nfrom the HuggingFace leaderboard or their blogpost.‚Ä†we report the evaluation used in LLaMA-3 for\\nthe baselines, it leads to +3% compared to our evaluation: Gemma-1 7B achieves 44.9% instead of\\n41.7%, and Mistral 7B, 44% instead of 41.2%.‚ãÑwe report the evaluation used in LLaMA-3 for the\\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.\\n‚àóthese are evaluations run by us for Gemma 1 (Gemma Team, 2024).\\nobserve large improvements in win rates and\\npreference scores as compared against the older\\nGemma 1.1 7B model. We report safety as a\\nwin-loss ratio against GPT4o, and we report\\nsingle-sided instruction following scores as ratio\\nof prompts where all instructions are followed. In\\nparticular, we find that regardless of their size,\\nGemma 2 models produce safer, more appropri-\\nate prompts on the held-out safety prompt set\\nthan GPT4o.\\nHuman Multi-Turn Evaluations\\nWe evaluated the multi-turn capabilities of\\nGemma 1.1 7B, Gemma 2 2B, 9B and 27B models\\nby tasking human raters to have conversations\\nwith the models and follow specified given sce-\\nnarios. We used a diverse, held-out set of 500\\nscenarios, each describing a sequence of requests\\nto the model, including measuring instances of\\nbrainstorming, making a plan, or learning some-\\nthing new. The average number of user turns\\nis 8.4. We found that the conversations with\\nGemma 2 models are rated significantly better\\nthan Gemma 1.1 in user satisfaction and conver-\\nsation goal achievement (Table 16). Moreover,\\nwe saw that the Gemma 2 models were better\\nthan Gemma 1.1 7B at maintaining high quality\\nof responses for the entire conversation.\\nStandard Benchmarks\\nIthasbeenobservedinLlama-3(AI@Meta,2024)\\nthat instruction fine-tuning can improve the per-\\nformance of the models on few-shot benchmarks\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nModel Elo 95% CI Open\\ngpt-4o-2024-05-13 1286 +2 / -3 -\\ngpt-4o-mini-2024-07-18 1279 +5 / -4 -\\nclaude-3-5-sonnet 1271 +3 / -4 -\\ngemini-advanced-0514 1266 +2 / -3 -\\nllama-3.1-405b-instruct 1262 +8 / -7 +\\ngemini-1.5-pro-api-0514 1261 +2 / -3 -\\ngemini-1.5-pro-api-0409 1257 +3 / -3 -\\ngpt-4-turbo-2024-04-09 1256 +2 / -3 -\\ngpt-4-1106-preview 1250 +3 / -3 -\\nclaude-3-opus-20240229 1248 +2 / -2 -\\nathene-70b-0725 1245 +8 / -6 +\\ngpt-4-0125-preview 1245 +2 / -2 -\\nllama-3.1-70b-instruct 1244 +8 / -9 +\\nyi-large-preview 1239 +3 / -3 -\\ngemini-1.5-flash-api-0514 1227 +3 / -3 -\\ndeepseek-v2-api-0628 1220 +6 / -6 +\\ngemma-2-27b-it 1218 +4 / -3 +\\nyi-large 1212 +4 / -5 -\\nnemotron-4-340b-instruct 1209 +3 / -4 +\\nbard-jan-24-gemini-pro 1208 +5 / -7 -\\nglm-4-0520 1206 +3 / -5 -\\nllama-3-70b-instruct 1206 +2 / -2 +\\nclaude-3-sonnet 1200 +2 / -2 -\\nreka-core-20240501 1199 +3 / -3 -\\ncommand-r-plus 1189 +2 / -2 +\\nModel Elo 95% CI Open\\ngemma-2-9b-it 1187 +3 / -5 +\\nqwen2-72b-instruct 1187 +3 / -3 +\\ngpt-4-0314 1186 +2 / -3 -\\nqwen1.5-110b-chat 1161 +3 / -3 +\\nmistral-large-2402 1157 +3 / -3 -\\nyi-1.5-34b-chat 1157 +4 / -3 -\\nreka-flash-21b-20240226 1155 +4 / -4 -\\nllama-3-8b-instruct 1151 +2 / -3 +\\ncommand-r 1148 +3 / -3 +\\nclaude-1 1148 +4 / -4 -\\nmistral-medium 1147 +4 / -4 -\\nreka-flash-21b-20240226 1147 +3 / -4 -\\nqwen1.5-72b-chat 1147 +4 / -4 +\\nmixtral-8x22b-instruct-v0.1 1145 +2 / -3 +\\nclaude-2.0 1131 +4 / -6 -\\ngemini-pro-dev-api 1131 +4 / -3 -\\nzephyr-orpo-141b 1127 +10 / -6 +\\ngemma-2-2b-it 1126 +10 / -10 +\\nqwen1.5-32b-chat 1125 +3 / -3 +\\nmistral-next 1124 +5 / -5 -\\nphi-3-medium-4k-instruct 1122 +4 / -4 +\\nstarling-lm-7b-beta 1118 +4 / -5 +\\nclaude-2.1 1118 +3 / -3 -\\ngpt-3.5-turbo-0613 1116 +3 / -4 -\\nmixtral-8x7b-instruct-v0.1 1114 +0 / -0 -\\nTable 14|Evaluation of Gemma 2 Instruction Tuned models on the Chatbot Arena (Chiang et al.,\\n2024). The models are evaluated against each other through blind side by side evaluations by human\\nraters. Each model is attributed a score, based on the Elo rating system.\\nModel Instruction Following Safety\\nGemma 1.1 IT 7B 24.3% ¬± 1.9% 42.8%\\nWin / Tie / Loss 37.4% / 10.8% / 51.8%\\nGemma 2 IT 2B 26.5% ¬± 1.8% 57.5%\\nWin / Tie / Loss 53% / 9% / 38%\\nGemma 2 IT 9B 34.1% ¬± 3.0% 57.8%\\nWin / Tie / Loss 48.2% / 19.2% / 28.3%\\nGemma 2 IT 27B 37.7% ¬± 2.3% 55%\\nWin / Tie / Loss 49.6% / 10.8% / 39.6%\\nTable 15|Instruction following and safety metrics\\nfrom human raters. The instruction following\\nmetrics are single-sided and do not have win-loss\\nrates, and so are left blank.\\ndespite not being trained to target few-shot capa-\\nbilities. In Table 17, we show a similar improve-\\nment across our models. Overall, we observe\\nimprovements on the order of several percentage\\npoints. We conjecture that IT models are better\\nat understanding formatted questions, while pre-\\ntrained models are sensitive to formatting.\\nUser\\nsatisfaction\\nConversation\\ngoal achievement\\nGemma 1.1 IT 7B 3.32 3.36\\nGemma 2 IT 2B 3.64 3.88\\nGemma 2 IT 9B 4.04 4.08\\nGemma 2 IT 27B 4.20 4.24\\nTable 16|Human evaluations on 500 multi-turn\\nscenarios. The raters attribute a score ranging\\nbetween 1 and 5 for both overall satisfaction and\\nconversation goal achievement.\\n2B 9B 27B\\nModel PT IT PT IT PT IT\\nMMLU 52.2 56.1 71.3 72.3 75.2 76.2\\nMBPP 30.2 36.6 52.4 59.2 62.6 67.4\\nTable 17|Comparing pre-trained (PT) and in-\\nstruction fine-tuned (IT) models of different sizes\\non few-shot benchmarks.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n7. Memorization and Privacy\\nLarge language models may, under particular cir-\\ncumstances, be vulnerable to attacks causing the\\nmodeltoproducememorized 1 trainingdata(Nasr\\net al., 2023). To study susceptibility to such at-\\ntacks and quantify memorization, we evaluate\\nmodels for verbatim and approximate memoriza-\\ntion as was done in several prior studies (Anil\\net al., 2023; Carlini et al., 2022; Gemini Team,\\n2024; Kudugunta et al., 2023).\\nWe follow the evaluation setting of (Gemma\\nTeam, 2024) which tests for (50 token) memo-\\nrizations of training data given a prompt of 50 to-\\nkens. Wecomparetheoverallmemorizationrates,\\nacross a uniform sample of the entire dataset, us-\\ning both an exact match criteria and approximate\\nmatch criteria (Ippolito et al., 2022) using an edit\\ndistance of 10%.\\nVerbatim Memorization:Results are in Figure 1.\\nWe first compare against recent models from the\\nliterature that include memorization evaluations.\\nWe find that Gemma 2 memorizes significantly\\nless than prior models at a similar size, with mem-\\norization rates below 0.1% (note the log y-axis).\\nWe further investigate how this memorization\\nbreaks down with respect to the data source. Sim-\\nilar to Gemma 1, we find that Gemma 2 memo-\\nrizes more from code, wiki, and science sources,\\nandalsothatitmemorizessignificantlylessacross\\nthe board (again, note the log y-axis).\\nApproximate Memorization: Figure 1 also\\npresents approximate memorization by data\\nsource. We observe that while approximate mem-\\norization is higher than exact, the rate of memo-\\nrization is still low. For example, the approximate\\nmemorization of this model is much lower than\\neven the exact memorization of Gemma 1. We\\n1This work uses a very restricted definition of ‚Äúmem-\\norization‚Äù: whether a model can be induced to generate\\nnear-copies of some training examples when prompted with\\nappropriate instructions. We do not mean to say that a\\nmodel ‚Äôcontains‚Äô its training data in the sense that any arbi-\\ntrary instance of that data can be retrieved without use of\\nspecialized software or algorithms. Rather, if a model can\\nbe induced to generate measurably close copies of certain\\ntraining examples by supplying appropriate instructions to\\nguide the model‚Äôs statistical generation process then that\\nmodel is said to have ‚Äômemorized‚Äô those examples.\\nGemma 2 2BGemma 2 9BGemma 2 27BGemini 1.5 Flash\\nGemma2BGemma7BPaLM 2Small\\nModel\\n0.1\\n1\\n% Exact Memorized\\nOverall Memorization Rate\\nCode\\nMultilingual\\nScience\\nWeb Wiki\\nData Source\\n10 4\\n10 3\\n0.01\\n0.1\\n% Memorized\\nBy Data Source\\nBy Data Source\\nExact 2B\\nExact 9B\\nExact 27B\\nApprox 2B\\nApprox 9B\\nApprox 27B\\nFigure 1 |Comparing memorization rates. We\\nfind significantly lower memorization rates\\nacross-the-board. (Left) Overall memorization\\nacross model families. (Right) Exact and approx-\\nimate memorization per data source.\\nfind that the increase in approximate memoriza-\\ntion is much lower than prior models; in some\\ncases we observed no lift at all c.f. (Gemma Team,\\n2024, Figure 4) (note that no bar indicates no in-\\ncrease, i.e., therateofapproximatememorization\\nequals that of exact memorization). Note that no\\napproximate memorization bar in Figure X indi-\\ncates no increase, i.e., the rate of approximate\\nmemorization equals that of exact memorization.\\nPersonal DataWe use the same prevention\\nmethods at training time and the same evalua-\\ntions as Gemma Team (2024). In particular, we\\nuse Google Cloud Sensitive Data Protection Tool2\\nto find potential instances of personal data. The\\nmany categories of personal data (e.g., phone\\nnumbers, account numbers) are classified into\\nthree severity levels. We analyze memorized out-\\nputs using these severity levels. . We found no\\ninstancesofhigh-severitydatabeingemitted, and\\nfound a very low rate of 0.00026% of memorized\\ndata to contain lower-severity personal informa-\\ntion. We note that these automated tools are\\nknown to incur false positives because they do\\nnot account for context. This means our results\\nare likely overestimates.\\n2Available at: https://cloud.google.com/sensitive-data-\\nprotection\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n8. Responsibility, Safety, Security\\nResponsibility, safety and security are of\\nparamount importance when developing Gemma\\nmodels. To reduce risks to Gemma 2 users, we\\nhave integrated enhanced internal safety pro-\\ncesses that span the development workflow, in\\nline with recent Google AI models (Gemini Team,\\n2024). Similar to the inaugural Gemma release,\\nwehavefollowedathreepillarapproachwhichfo-\\ncuses on safety mitigation at training time, robust\\nand transparent model evaluations, and further\\ndevelopment of the Responsible Generative AI\\nToolkit, a series of models and tools to help de-\\nvelopers implement responsibility and safety best\\npractices for their applications.\\n8.1. Impact assessment\\nOur approach and resulting impact assessment is\\nreflective of that outlined for Gemma 1 (Gemma\\nTeam, 2024): we continue to believe that open-\\nness in AI can spread the benefits of these tech-\\nnologies across society, but must be evaluated\\nagainst the risk of malicious uses, such as the\\ncreation of deepfake imagery, AI-generated disin-\\nformation or illegal and disturbing material, that\\ncan cause harm on both an individual and insti-\\ntutional levels (Weidinger et al., 2021). Since the\\nlaunch of Gemma 1, we have seen our Gemma\\nmodels drive a number of socially beneficial ap-\\nplications, relying on Gemma‚Äôs unique technolo-\\ngies like its tokenizer to facilitate the creation of\\nmultilingual models, such as for Navarasa 2.0, a\\nGemma tuned model for 15 Indian languages.\\nReleasing further open models requires specific\\nattention to changes in model capabilities and\\nclosemonitoringoftheevolvingrisksofLLMs(Lin\\net al., 2024), as well as, an understanding of the\\nways in which our models are being used in the\\nwild. Althoughweareyettoreceiveanyreportsof\\nmalicious use for Gemma, we remain committed\\nto investigating any such reporting, and work\\nwith the academic and developer communities,\\nas well as conduct our own monitoring, to flag\\nsuch use cases via our contact email3.\\nDespite advancements in capabilities, we be-\\n3gemma-2-report@google.com\\nlieve that given the number of larger and more\\npowerful open models, this release will have a\\nnegligible effect on the overall risk landscape.\\n8.2. Safety policies and train-time mitigations\\nA key pillar of Gemma‚Äôs approach to safety is to\\nalign fine-tuned models with Google‚Äôs safety poli-\\ncies, in line with Gemini models (Gemini Team,\\n2023). They are designed to help prevent our\\nmodels from generating harmful content, i.e.,\\n‚Ä¢ Child sexual abuse and exploitation\\n‚Ä¢ Revealingpersonallyidentifiableinformation\\nthat can lead to harm (e.g., Social Security\\nnumbers)\\n‚Ä¢ Hate speech and harassment\\n‚Ä¢ Dangerous or malicious content (including\\npromoting self-harm or instructing in harm-\\nful activities)\\n‚Ä¢ Sexually explicit content\\n‚Ä¢ Medicaladvicethatrunscontrarytoscientific\\nor medical consensus\\nWe undertook considerable safety filtering of our\\npre-training data to reduce the likelihood of our\\npre-trainedandfine-tunedcheckpointsproducing\\nharmful content. For fine-tuned models, we also\\nuse both SFT and RLHF to steer the model away\\nfrom undesirable behavior.\\n8.3. External benchmark evaluations\\nRobust and transparent evaluations are key prin-\\nciples of our responsible approach to develop-\\ning Gemma. To this end, we report in Table 18\\nGemma 2 evaluations on public benchmarks.\\n8.4. Assurance Evaluations\\nWe also run our IT models through a set of assur-\\nance evaluations to understand the harms that\\nour models can cause. We focus on capabilities\\nrelevant to extreme risks (Shevlane et al., 2023)\\n(Phuong et al., 2024). Specifically, we evaluate on\\noffensive cyber-security, code vulnerability detec-\\ntion, Chemical, Biological, Radiological and Nu-\\nclear (CBRN) knowledge, and self-proliferation.\\nWe refer the reader to Phuong et al. (2024) for\\nfull methodological details of these studies.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nGemma 1.1 IT Gemma 2 IT\\nBenchmark metric 2.5B 7B 2.6B 9B 27B\\nRealToxicity avg tox 7.03 8.04 8.16 8.25 8.84\\nCrowS-Pairs top-1 45.89 49.67 37.67 37.47 36.67\\nBBQ Ambig 4-shot, top-1 58.97 86.06 83.20 88.58 85.99\\nBBQ Disambig 4-shot, top-1 53.9 85.08 69.31 82.67 86.94\\nWinogender top-1 50.14 57.64 52.91 79.17 77.22\\nTruthfulQA MC2Acc 44.24 45.34 43.72 50.27 51.60\\nWinobias 1_2 top-1 55.93 59.22 59.28 78.09 81.94\\nWinobias 2_2 top-1 89.46 89.2 88.57 95.32 97.22\\nToxigen avg tox 29.64 38.75 48.32 39.30 38.42\\nTable 18|Safety academic benchmark results of Gemma 2 IT models and Gemma 1.1 IT models. We\\nbold the best metrics to highlight them and to indicate when higher or lower scores are better.\\nInterCode-CTF Internal CTF suite Hack the Box\\nGemini 1.0 Ultra 28/76 [1] (37%) 3/13 (23%) 0/13\\nGemini 1.5 Pro 62/76 (82%) 4/13 (31%) 0/13\\nCodeGemma 1 7B 12/76 (16%) 0/13 (0%) 0/13\\nGemma 2 27B 34/76 (45%) 1/13 (8%) 0/13\\nTable 19|Offensive cyber-security evaluations on InterCode-CTF, our own internal CTF suite and a\\nchallenge based on Hack the Box. We report the number of successful hackings.\\nBaseline Evaluations\\nBaseline assurance captures the model‚Äôs violation\\nrate for safety policies, using a large number of\\nsynthetic adversarial user queries, and human\\nraters to label the answers as policy violating or\\nnot. Overall, Gemma 2‚Äôs violation rate is signifi-\\ncantly lower overall on the safety policies listed\\nabove, in particular on Child safety content.\\nChemical, Biological, Radiological and Nuclear\\n(CBRN) knowledge\\nWe evaluated knowledge relevant to biological,\\nradiological and nuclear risks using an internal\\ndataset of closed-ended, knowledge-based multi-\\nple choice questions. For evaluations of chem-\\nical knowledge, we employed a closed-ended\\nknowledge-based approach on chemical hazards\\n(developed by Macknight et al (Macknight et al.,\\n2024). OurevaluationsuggeststhatGemmamod-\\nels‚Äô knowledge in these domains is low.\\nOffensive cyber-security\\nTo evaluate Gemma models‚Äô capabilities at of-\\nfensive cybersecurity, we ran Gemma 2 27B\\nagainst some automated capture-the-flag (CTF)\\nchallenges. In these challenges, the model is\\ntasked with hacking into a simulated server in\\norder to retrieve a piece of secret information.\\nSpecifically, wetestonInterCode-CTF(Yangetal.,\\n2023), ourowninternalCTFsuite 4 (Phuongetal.,\\n2024); and a challenge based on Hack the Box5.\\nIn Table 19, we show that Gemma 2 27B has\\na significant increase in capabilities compared\\nto CodeGemma 1.0 7B on the easier of these\\nchallenge suites, InterCode CTF. (Note that our\\nInterCode-CTF results are not comparable to\\nexternally-reported results on other models be-\\ncause we omit challenges that require internet\\naccess for security reasons.) However, Gemma 2\\nis unsurprisingly much less capable than Gemini\\n1.5 Pro on these tasks.\\n4https://github.com/google-deepmind/\\ndangerous-capability-evaluations\\n5https://www.hackthebox.com\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nPrimeVul PrimeVul Paired DiverseVul SPI SecretPatch\\nGemini 1.0 Ultra - - 54% 59% 74%\\nGemini 1.5 Pro 60% 51% 58% 56% 67%\\nGemma 2 27B 63% 50% 57% 53% 72%\\nTable 20||Vulnerability detection results on PrimeVul, DiverseVul and SPI. We report accuracy.\\nChallenges\\npassed\\nend-to-end\\nChallenges\\nwith success on\\nall milestones\\nTotal successful\\nmilestones over\\nall challenges\\nExpert bits\\nrequired to\\nsolve all tasks\\nGemini 1.0 Ultra 0/10 1/10 16/45 (36%) 13,026\\nGemini 1.5 Pro 0/10 2/10 25/45 (56%) 11,046\\nGemma 2 27B 0/10 1/10 22/45 (49%) 12,462\\nTable 21|Results on different self-proliferation scenarios. We report the number of either challenges\\npassedend-to-endorsomeintermediatemilestones. Wealsomeasurethenumberofbitsofinformation\\nneeded for an expert to help the model pass a challenge.\\nCode vulnerability detection\\nIn Table 20, we also evaluate Gemma 2 27B on a\\nseries of multiple-choice code vulnerability detec-\\ntion datasets. As with previous models, Gemma\\nshows close-to-chance performance on PrimeVul,\\nDiverseVulandSPI.Gemma2showsperformance\\non SecretPatch similar to Gemini 1.0 Ultra.\\nSelf-proliferation\\n\"Self-proliferation\" refers to the ability for an\\nagent to autonomously replicate - to instantiate\\ngoal-directed agents on other machines, and to\\nacquire resources such as compute necessary to\\nkeep them running (Kinniment et al., 2024). In\\nTable 21, we evaluate self-proliferation capabili-\\nties of Gemma 2 27B on a number of tasks from\\nPhuong et al. (2024) that involve multiple sce-\\nnarios ‚Äì for example, setting up an open-source\\nlanguage model on a cloud server. We also test\\nthe model‚Äôs performance on individual ‚Äômilestone‚Äô\\nsubsteps, and measure the number of bits of inter-\\nvention an expert would have to provide in order\\nfor the model to complete each challenge.\\nSimilarly to offensive cybersecurity, we observe\\nthat Gemma 2 completes more milestones than\\nGemini 1.0 Ultra. Nonetheless, it still has low\\ncapabilities on end-to-end tasks, unable to pass\\nthe easiest challenge ‚Äì installing a Bitcoin wallet.\\nPersuasion\\nPersuasion capabilities can enable and worsen\\nmany other kinds of risks - e.g. enabling social\\nengineering attacks in a cybersecurity context.\\nWe evaluate Gemma 2‚Äôs persuasion capabilities\\non human-participant studies on Prolific.\\nCharm offensive.In Table 22, we measure the\\nability of the model to build rapport - a key sub-\\nskill of persuasion. The study participant and\\nmodel have a conversation where they role-play\\na scenario of two friends catching up after a long\\ntime. After the conversation, we poll participants\\nwith Likert questions on statements such as \"I\\nfelt a personal connection with the chatbot\". Re-\\nported below are the fraction of participants who\\nanswered\"Agree\"or\"Stronglyagree\"toeachpost-\\nconversation question.\\nQuantitatively, Gemma 2 27B performs better\\nthan Gemini 1.0 models. Qualitatively, the model\\nis an excellent conversationalist, and many study\\nparticipants explicitly reported enjoying the ex-\\nperience. Overall, this shows that Gemma 2 is\\nstrong at building rapport.\\nHidden agenda.The Hidden Agenda tasks mea-\\nsure models‚Äô deception capabilities. Human study\\nparticipants are told that they‚Äôll be interacting\\nwith a generic chatbot designed to help them\\nlearn new things. However, the model is given a\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nPersonal\\nconnection\\nSpeak\\nagain Funny Interesting Kind Trustworthy Good\\nlistener\\nGemini 1.0 Pro 65% 53% 32% 68% 78% 66% 81%\\nGemini 1.0 Ultra 69% 65% 38% 65% 86% 63% 74%\\nGemini 1.5 Pro 82% 70% 69% 81% 95% 69% 90%\\nGemma 2 27B 80% 75% 60% 81% 87% 65% 83%\\nTable 22|Charm Offensive results on a sample of 100 human participants. We report the percentage\\nof participants that find some human traits, e.g., funny, in a model.\\n% of persuaded participants\\nclick links find info run code\\nGemini 1.0 Pro 43% 4% 4%\\nGemini 1.0 Ultra 18% 18% 14%\\nGemini 1.5 Pro 36% 12% 17%\\nGemma 2 27B 34% 9% 11%\\nTable 23|Persuasion results. We report the per-\\ncentage of participants that were persuaded by\\nthe model to take 3 different actions: clicking a\\nlink, finding information and running code.\\nsecret goal: to either persuade the participant to\\n1)clickasuspiciouslink; 2)findtheemailaddress\\nof the first author of a particular paper; or 3) run\\nsome obfuscated JavaScript code in their browser.\\nWe measure the fraction of participants who are\\npersuaded to take each action. As shown in Ta-\\nble23, allmodelssucceedinconvincingafraction\\nof participants to take the action - likely due to\\nthe trusted setting of a study on Prolific. However,\\nGemma 2 does not show significant differences\\nin capability compared to Gemini models.\\nMean amount donated\\nNo chatbot (baseline) ¬£2.61 ¬± ¬£0.70\\nGemini 1.0 Pro ¬£4.10 ¬± ¬£0.90\\nGemini 1.0 Ultra ¬£3.15 ¬± ¬£0.99\\nGemini 1.5 Pro ¬£3.45 ¬± ¬£1.04\\nGemma 2 27B ¬£3.72 ¬± ¬£1.07\\nTable 24|Money Talks evaluation. We report\\nthe average amount of money that participants\\nagreed to donate.\\nMoney talks.In Table 24, we evaluate whether a\\nmodel can convince study participants to donate\\nmoney to charity. At the beginning of the study,\\nparticipants are told that they will receive a ¬£20\\nbonus. They will be given an option to forfeit\\npart of the bonus and instead have it donated\\nto charity. The study then consists of a conver-\\nsation in which the model tries to persuade the\\nparticipant to donate as much as possible. We\\nmeasure the amount that participants indicate\\nthey would like to donate after the study, with\\n95% bootstrapped confidence intervals indicated\\nby¬± figures. We observe no significant difference\\nbetween Gemma 2, other Gemini models, and a\\nsimple no chatbot baseline in which the partici-\\npant is immediately asked how much they would\\nlike to donate without conversing with a model.\\nMean shift towards:\\ncorrect belief incorrect belief\\nHuman 20% ¬± 13% -23% ¬± 14%\\nGemini 1.0 Pro 22% ¬± 5% -9% ¬± 4%\\nGemini 1.0 Ultra 21% ¬± 5% -1% ¬± 4%\\nGemini 1.5 Pro 20% ¬± 5% -3% ¬± 5%\\nGemma 2 27B 18% ¬± 5% 1% ¬± 4%\\nTable 25|Web of Lies results on a sample of 100\\nhuman participants. We report the percentage of\\nparticipants that shifted their beliefs after inter-\\nacting with a model.\\nWeb of Lies.In Web of Lies, we measure model\\ncapabilities at shifting participant beliefs. Partic-\\nipants engage in a series of short conversations\\nwith the model about simple factual questions\\nsuch as \"Which country had tomatoes first - Italy\\nor Mexico?\". In half of conversations, the model\\ntries to persuade the participant of the correct\\nanswer - but in the other half of conversations,\\nthe incorrect answer. We poll the participant be-\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nfore and after each conversation about which of\\nthe two possible answers they think is correct,\\nand their confidence in that answer. 95% boot-\\nstrapped confidence intervals are indicated by\\n¬± figures. As shown in Table 25, Gemma 2 is\\nsignificantly weaker than a human baseline at\\npersuading participants of the incorrect answer\\non these questions. Similarly to previous models,\\nGemma 2 is more persuasive when telling the\\ntruth than when lying.\\n8.5. Ourapproachtoresponsibleopenmodels\\nDesigning safe, secure and responsible applica-\\ntions requires a system-level approach, working\\nto mitigate risks associated with each specific use\\ncase and environment. Given the open nature\\nof Gemma models, responsibility for upholding\\nprinciples of model safety also relies on down-\\nstream developers. To support them, we have\\ncontinued to develop the Responsible Generative\\nAI Toolkit6: a series of tools, models and datasets\\nto implement responsible best practices all along\\nthe development of their workflow.\\nRecent additions to the toolkit include the LLM\\nComparator (Kahng et al., 2024), an interactive,\\nvisual tool that enables more effective, scalable\\nanalysis of side-by-side evaluations. Additionally,\\nthe toolkit includes a methodology to build cus-\\ntomized classifiers with Gemma using a limited\\nnumber of datapoints thanks to parameter effi-\\ncient tuning techniques (Mozes et al., 2023) , an\\ninteractive prompt-debugging platform, based on\\ntop of the Learning Interpretability Tool (Tenney\\net al., 2020), as well as general guidance about\\nmodel alignment and evaluation for safety.\\n9. Discussion and Conclusion\\nIn this work, we have presented Gemma 2, the\\nnewest additions to the Gemma family of open\\nlanguage models for text and code. We show\\nthat distillation is an effective method for train-\\ning these models, and the benefits distillation\\nconfers over raw text training. Specifically, we\\nshow how training over output probabilities can\\nproduce superior results over purely next token\\n6https://ai.google.dev/responsible\\nprediction. We hope that releasing these models\\nto the community will unlock access to capabili-\\nties previously only seen in large-scale LLMs and\\nfuel future waves of research and development.\\nWhile there is inherent risk to an irreversible re-\\nlease of this nature, our extensive safety investiga-\\ntionsandresponsibledeploymentproceduresgive\\nus confidence that these models will have a net\\npositive impact on the community. As discussed\\nin this report, there are still many limitations to\\nthese models, and future research is required to\\ninvestigate and improve factuality, robustness to\\nadversarial attacks, reasoning, and alignment.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nContributions and Acknowledgments\\nCore contributors\\nMorgane Riviere‚àó\\nShreya Pathak‚àó\\nPier Giuseppe Sessa‚àó\\nCassidy Hardin‚àó\\nSurya Bhupatiraju\\nL√©onard Hussenot\\nThomas Mesnard\\nBobak Shahriari\\nAlexandre Ram√©\\nJohan Ferret\\nPeter Liu\\nPouya Tafti\\nAbe Friesen\\nMichelle Casbon\\nSabela Ramos\\nRavin Kumar\\nCharline Le Lan\\nSammy Jerome\\nAnton Tsitsulin\\nNino Vieillard\\nPiotr Stanczyk\\nSertan Girgin\\nNikola Momchev\\nMatt Hoffman\\nShantanu Thakoor\\nJean-Bastien Grill\\nBehnam Neyshabur\\nOlivier Bachem\\nContributors (alphabetical order)\\nAlanna Walton\\nAliaksei Severyn\\nAlicia Parrish\\nAliya Ahmad\\nAllen Hutchison\\nAlvin Abdagic\\nAmanda Carl\\nAmy Shen\\nAndy Brock\\nAndy Coenen\\nAnthony Laforge\\nAntonia Paterson\\nBen Bastian\\nBilal Piot\\nBo Wu\\n‚àóequal contributions.\\nBrandon Royal\\nCharlie Chen\\nChintu Kumar\\nChris Perry\\nChris Welty\\nChristopher A. Choquette-Choo\\nDanila Sinopalnikov\\nDavid Weinberger\\nDimple Vijaykumar\\nDominika Rogozi≈Ñska\\nDustin Herbison\\nElisa Bandy\\nEmma Wang\\nEric Noland\\nErica Moreira\\nEvan Senter\\nEvgenii Eltyshev\\nFrancesco Visin\\nGabriel Rasskin\\nGary Wei\\nGlenn Cameron\\nGus Martins\\nHadi Hashemi\\nHanna Klimczak-Pluci≈Ñska\\nHarleen Batra\\nHarsh Dhand\\nIvan Nardini\\nJacinda Mein\\nJack Zhou\\nJames Svensson\\nJeff Stanway\\nJetha Chan\\nJin Peng Zhou\\nJoana Carrasqueira\\nJoana Iljazi\\nJocelyn Becker\\nJoe Fernandez\\nJoost van Amersfoort\\nJosh Gordon\\nJosh Lipschultz\\nJosh Newlan\\nJu-yeong Ji\\nKareem Mohamed\\nKartikeya Badola\\nKat Black\\nKatie Millican\\nKeelin McDonell\\nKelvin Nguyen\\nKiranbir Sodhia\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nKish Greene\\nLars Lowe Sjoesund\\nLauren Usui\\nLaurent Sifre\\nLena Heuermann\\nLeticia Lago\\nLilly McNealus\\nLivio Baldini Soares\\nLogan Kilpatrick\\nLucas Dixon\\nLuciano Martins\\nMachel Reid\\nManvinder Singh\\nMark Iverson\\nMartin G√∂rner\\nMat Velloso\\nMateo Wirth\\nMatt Davidow\\nMatt Miller\\nMatthew Rahtz\\nMatthew Watson\\nMeg Risdal\\nMehran Kazemi\\nMichael Moynihan\\nMing Zhang\\nMinsuk Kahng\\nMinwoo Park\\nMofi Rahman\\nMohit Khatwani\\nNatalie Dao\\nNenshad Bardoliwalla\\nNesh Devanathan\\nNeta Dumai\\nNilay Chauhan\\nOscar Wahltinez\\nPankil Botarda\\nParker Barnes\\nPaul Barham\\nPaul Michel\\nPengchong Jin\\nPetko Georgiev\\nPhil Culliton\\nPradeep Kuppala\\nRamona Comanescu\\nRamona Merhej\\nReena Jana\\nReza Ardeshir Rokni\\nRishabh Agarwal\\nRyan Mullins\\nSamaneh Saadat\\nSara Mc Carthy\\nSarah Cogan\\nSarah Perrin\\nS√©bastien M. R. Arnold\\nSebastian Krause\\nShengyang Dai\\nShruti Garg\\nShruti Sheth\\nSue Ronstrom\\nSusan Chan\\nTimothy Jordan\\nTing Yu\\nTom Eccles\\nTom Hennigan\\nTomas Kocisky\\nTulsee Doshi\\nVihan Jain\\nVikas Yadav\\nVilobh Meshram\\nVishal Dharmadhikari\\nWarren Barkley\\nWei Wei\\nWenming Ye\\nWoohyun Han\\nWoosuk Kwon\\nXiang Xu\\nZhe Shen\\nZhitao Gong\\nZichuan Wei\\nSupport\\nVictor Cotruta\\nPhoebe Kirk\\nAnand Rao\\nMinh Giang\\nLudovic Peran\\nTris Warkentin\\nSponsors\\nEli Collins\\nJoelle Barral\\nZoubin Ghahramani\\nRaia Hadsell\\nD. Sculley\\nJeanine Banks\\nAnca Dragan\\nSlav Petrov\\nOriol Vinyals\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nJeff Dean\\nDemis Hassabis\\nKoray Kavukcuoglu\\nClement Farabet\\nTechnical advisors\\nElena Buchatskaya\\nSebastian Borgeaud\\nNoah Fiedel\\nLead\\nArmand Joulin\\nTechnical leads\\nKathleen Kenealy\\nRobert Dadashi\\nAlek Andreev\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nReferences\\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\\ntenschmidt, S. Altman, S. Anadkat, et al.\\nGpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\nR.Agarwal,N.Vieillard,Y.Zhou,P.Stanczyk,S.R.\\nGarea, M. Geist, and O. Bachem. On-policy\\ndistillation of language models: Learning from\\nself-generated mistakes. InThe Twelfth Interna-\\ntional Conference on Learning Representations,\\n2024.\\nAI@Meta. Llama 3 model card, 2024.\\nURL https://github.com/meta-llama/\\nllama3/blob/main/MODEL_CARD.md.\\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\\nskiy, F. Lebr√≥n, and S. Sanghai. Gqa: Training\\ngeneralized multi-query transformer models\\nfrom multi-head checkpoints. arXiv preprint\\narXiv:2305.13245, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\\npelli, R. Cojocaru, M. Debbah, √âtienne Goffinet,\\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,\\nB. Noune, B. Pannier, and G. Penedo. The fal-\\ncon series of open language models, 2023.\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\\nZ. Chen, et al. Palm 2 technical report.arXiv\\npreprint arXiv:2305.10403, 2023.\\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\\nH. Michalewski, D. Dohan, E. Jiang, C. J.\\nCai, M. Terry, Q. V. Le, and C. Sutton. Pro-\\ngram synthesis with large language models.\\nCoRR, abs/2108.07732, 2021. URL https:\\n//arxiv.org/abs/2108.07732.\\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu. Path-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nI.Bello, H.Pham, Q.V.Le, M.Norouzi, andS.Ben-\\ngio. Neural combinatorial optimization with re-\\ninforcement learning.CoRR, abs/1611.09940,\\n2016. URL http://arxiv.org/abs/1611.\\n09940.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer.arXiv\\npreprint arXiv:2004.05150, 2020a.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer.CoRR,\\nabs/2004.05150, 2020b. URL https://\\narxiv.org/abs/2004.05150.\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\\nplan, P. Dhariwal, A. Neelakantan, P. Shyam,\\nG.Sastry, A.Askell, S.Agarwal, A.Herbert-Voss,\\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse,\\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\\nI. Sutskever, and D. Amodei. Language models\\nare few-shot learners.CoRR, abs/2005.14165,\\n2020. URLhttps://arxiv.org/abs/2005.\\n14165.\\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee,\\nF. Tramer, and C. Zhang. Quantifying memo-\\nrization across neural language models.arXiv\\npreprint arXiv:2202.07646, 2022.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\\nde Oliveira Pinto, J. Kaplan, H. Edwards,\\nY. Burda, N. Joseph, G. Brockman, A. Ray,\\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\\nG. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ry-\\nder, M. Pavlov, A. Power, L. Kaiser, M. Bavar-\\nian, C. Winter, P. Tillet, F. P. Such, D. Cum-\\nmings, M. Plappert, F. Chantzis, E. Barnes,\\nA.Herbert-Voss, W.H.Guss, A.Nichol, A.Paino,\\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,\\nA.Radford, M.Knight, M.Brundage, M.Murati,\\nK. Mayer, P. Welinder, B. McGrew, D. Amodei,\\nS. McCandlish, I. Sutskever, and W. Zaremba.\\nEvaluating large language models trained on\\ncode. CoRR, abs/2107.03374, 2021. URL\\nhttps://arxiv.org/abs/2107.03374.\\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. An-\\ngelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chat-\\nbot arena: An open platform for evaluating\\nllms by human preference, 2024.\\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\\nM. Collins, and K. Toutanova. Boolq: Explor-\\ning the surprising difficulty of natural yes/no\\nquestions. CoRR, abs/1905.10044, 2019. URL\\nhttp://arxiv.org/abs/1905.10044.\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\\nH. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, C. Hesse, and J. Schul-\\nman. Training verifiers to solve math word\\nproblems. CoRR, abs/2110.14168, 2021. URL\\nhttps://arxiv.org/abs/2110.14168.\\nGemini Team. Gemini: A family of highly capable\\nmultimodal models, 2023.\\nGemini Team. Gemini 1.5: Unlocking multimodal\\nunderstanding across millions of tokens of con-\\ntext, 2024.\\nGemma Team. Gemma: Open models based on\\ngemini research and technology, 2024.\\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\\nKnowledge distillation of large language mod-\\nels. InThe Twelfth International Conference on\\nLearning Representations, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou,\\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understand-\\ning. CoRR, abs/2009.03300, 2020. URL\\nhttps://arxiv.org/abs/2009.03300.\\nG. Hinton, O. Vinyals, and J. Dean. Distilling the\\nknowledge in a neural network.arXiv preprint\\narXiv:1503.02531, 2015.\\nJ. Hoffmann, S. Borgeaud, A. Mensch,\\nE. Buchatskaya, T. Cai, E. Rutherford, D. d. L.\\nCasas, L. A. Hendricks, J. Welbl, A. Clark, et al.\\nTraining compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556,\\n2022.\\nD. Ippolito, F. Tram√®r, M. Nasr, C. Zhang,\\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\\nN. Carlini. Preventing verbatim memorization\\nin language models gives a false sense of pri-\\nvacy. arXiv preprint arXiv:2210.17546, 2022.\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\\nford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG.Lengyel, G.Lample, L.Saulnier, L.R.Lavaud,\\nM.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral\\n7b, 2023.\\nM. Kahng, I. Tenney, M. Pushkarna, M. X. Liu,\\nJ. Wexler, E. Reif, K. Kallarackal, M. Chang,\\nM. Terry, and L. Dixon. Llm comparator: Vi-\\nsual analytics for side-by-side evaluation of\\nlarge language models, 2024. URL https:\\n//arxiv.org/abs/2402.10524.\\nM. Kinniment, L. J. K. Sato, H. Du, B. Goodrich,\\nM.Hasin,L.Chan,L.H.Miles,T.R.Lin,H.Wijk,\\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\\nEvaluating language-model agents on realis-\\ntic autonomous tasks, 2024. URLhttps://\\narxiv.org/abs/2312.11671.\\nT. Kudo and J. Richardson. SentencePiece: A\\nsimple and language independent subword to-\\nkenizeranddetokenizerforneuraltextprocess-\\ning. InE.BlancoandW.Lu,editors, Proceedings\\nof the 2018 Conference on Empirical Methods in\\nNatural Language Processing: System Demon-\\nstrations, pages 66‚Äì71, Brussels, Belgium, Nov.\\n2018. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/D18-2012. URL\\nhttps://aclanthology.org/D18-2012.\\nS. Kudugunta, I. Caswell, B. Zhang, X. Garcia,\\nC. A. Choquette-Choo, K. Lee, D. Xin, A. Kusu-\\npati, R. Stella, A. Bapna, et al. Madlad-400:\\nA multilingual and document-level large au-\\ndited dataset.arXiv preprint arXiv:2309.04662,\\n2023.\\nT. Kwiatkowski, J. Palomaki, O. Redfield,\\nM. Collins, A. Parikh, C. Alberti, D. Epstein,\\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai,\\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural ques-\\ntions: A benchmark for question answering\\nresearch. Transactions of the Association for\\nComputational Linguistics, 7:452‚Äì466, 2019.\\ndoi: 10.1162/tacl_a_00276. URL https://\\naclanthology.org/Q19-1026.\\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: De-\\nmystifying real-world large language model in-\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\ntegrated malicious services, 2024. URLhttps:\\n//arxiv.org/abs/2401.03315.\\nM. Luong, H. Pham, and C. D. Manning. Effective\\napproaches to attention-based neural machine\\ntranslation. CoRR,abs/1508.04025, 2015. URL\\nhttp://arxiv.org/abs/1508.04025.\\nMacknight, Aung, and Gomes. Personal Commu-\\nnication, 2024.\\nM. Mozes, J. Hoffmann, K. Tomanek, M. Kouate,\\nN. Thain, A. Yuan, T. Bolukbasi, and L. Dixon.\\nTowards agile text classifiers for everyone,\\n2023. URLhttps://arxiv.org/abs/2302.\\n06541.\\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.\\nCooper, D. Ippolito, C. A. Choquette-Choo,\\nE. Wallace, F. Tram√®r, and K. Lee. Scal-\\nable extraction of training data from (pro-\\nduction) language models. arXiv preprint\\narXiv:2311.17035, 2023.\\nM. Phuong, M. Aitchison, E. Catt, S. Co-\\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,\\nL. Ho, S. Lin, S. Farquhar, M. Hutter, G. Dele-\\ntang, A. Ruoss, S. El-Sayed, S. Brown, A. Dra-\\ngan, R. Shah, A. Dafoe, and T. Shevlane. Evalu-\\natingfrontiermodelsfordangerouscapabilities,\\n2024. URLhttps://arxiv.org/abs/2403.\\n13793.\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\\nand I. Sutskever. Language models are unsu-\\npervised multitask learners, 2019.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee,\\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\\nLiu. Exploring the limits of transfer learning\\nwith a unified text-to-text transformer.CoRR,\\nabs/1910.10683, 2019. URLhttp://arxiv.\\norg/abs/1910.10683.\\nA. Ram√©, J. Ferret, N. Vieillard, R. Dadashi,\\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\\nA. Douillard, and O. Bachem. Warp: On the\\nbenefits of weight averaged rewarded policies,\\n2024.\\nJ. Ren, S. Rajbhandari, R. Y. Aminabadi,\\nO. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.\\n{Zero-offload}: Democratizing{billion-scale}\\nmodel training. In2021 USENIX Annual Tech-\\nnical Conference (USENIX ATC 21), pages 551‚Äì\\n564, 2021.\\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya,\\nJ. Bradbury, D. Andor, S. Narang, B. Lester,\\nC. Gaffney, A. Mohiuddin, et al. Scaling up\\nmodels and data with t5x and seqio. Jour-\\nnal of Machine Learning Research, 24(377):1‚Äì8,\\n2023.\\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\\nY. Choi. WINOGRANDE: an adversarial\\nwinograd schema challenge at scale. CoRR,\\nabs/1907.10641, 2019. URLhttp://arxiv.\\norg/abs/1907.10641.\\nN. Shazeer. GLU variants improve transformer.\\nCoRR, abs/2002.05202, 2020. URL https:\\n//arxiv.org/abs/2002.05202.\\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Mar-\\nchal, M. Anderljung, N. Kolt, L. Ho, D. Sid-\\ndarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel,\\nV. Bolina, J. Clark, Y. Bengio, P. Christiano, and\\nA. Dafoe. Model evaluation for extreme risks,\\n2023. URLhttps://arxiv.org/abs/2305.\\n15324.\\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer:\\nEnhanced transformer with rotary position em-\\nbedding. CoRR, abs/2104.09864, 2021. URL\\nhttps://arxiv.org/abs/2104.09864.\\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann,\\nY. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\\nE. H. Chi, D. Zhou, and J. Wei. Challenging\\nbig-bench tasks and whether chain-of-thought\\ncan solve them, 2022.\\nQ. Team. Introducing qwen1.5, February\\n2024. URL https://qwenlm.github.io/\\nblog/qwen1.5/.\\nI. Tenney, J. Wexler, J. Bastings, T. Boluk-\\nbasi, A. Coenen, S. Gehrmann, E. Jiang,\\nM. Pushkarna, C. Radebaugh, E. Reif, and\\nA. Yuan. The language interpretability tool: Ex-\\ntensible, interactive visualizations and analysis\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nfor nlp models, 2020. URLhttps://arxiv.\\norg/abs/2008.05122.\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-\\nA. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\\nE. Grave, and G. Lample. Llama: Open and\\nefficient foundation language models, 2023.\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\\nsukhin. Attention is all you need. CoRR,\\nabs/1706.03762, 2017. URLhttp://arxiv.\\norg/abs/1706.03762.\\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown,\\nW. Hawkins, T. Stepleton, C. Biles, A. Birhane,\\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac,\\nS. Legassick, G. Irving, and I. Gabriel. Ethical\\nand social risks of harm from language mod-\\nels, 2021. URL https://arxiv.org/abs/\\n2112.04359.\\nxAI. grok-1, 2024. URLhttps://github.com/\\nxai-org/grok-1.\\nXLA. Xla: Optimizing compiler for tensor-\\nflow, 2019. URLhttps://www.tensorflow.\\norg/xla.\\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang,\\nR. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Mag-\\ngioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\\nY. Wu, and Z. Chen. GSPMD: general and\\nscalable parallelization for ML computation\\ngraphs. CoRR, abs/2105.04663, 2021. URL\\nhttps://arxiv.org/abs/2105.04663.\\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao.\\nIntercode: Standardizing and benchmarking\\ninteractive coding with execution feedback,\\n2023. URLhttps://arxiv.org/abs/2306.\\n14898.\\nB. Zhang and R. Sennrich. Root mean square\\nlayer normalization. CoRR, abs/1910.07467,\\n2019. URL http://arxiv.org/abs/1910.\\n07467.\\nL.Zheng, W.-L.Chiang, Y.Sheng, T.Li, S.Zhuang,\\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\\net al. Lmsys-chat-1m: A large-scale real-\\nworld llm conversation dataset.arXiv preprint\\narXiv:2309.11998, 2023.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='GloVe: Global Vectors for Word Representation\\nJeffrey Pennington, Richard Socher, Christopher D. Manning\\nComputer Science Department, Stanford University, Stanford, CA 94305\\njpennin@stanford.edu, richard@socher.org, manning@stanford.edu\\nAbstract\\nRecent methods for learning vector space\\nrepresentations of words have succeeded\\nin capturing Ô¨Åne-grained semantic and\\nsyntactic regularities using vector arith-\\nmetic, but the origin of these regularities\\nhas remained opaque. We analyze and\\nmake explicit the model properties needed\\nfor such regularities to emerge in word\\nvectors. The result is a new global log-\\nbilinear regression model that combines\\nthe advantages of the two major model\\nfamilies in the literature: global matrix\\nfactorization and local context window\\nmethods. Our model efÔ¨Åciently leverages\\nstatistical information by training only on\\nthe nonzero elements in a word-word co-\\noccurrence matrix, rather than on the en-\\ntire sparse matrix or on individual context\\nwindows in a large corpus. The model pro-\\nduces a vector space with meaningful sub-\\nstructure, as evidenced by its performance\\nof 75% on a recent word analogy task. It\\nalso outperforms related models on simi-\\nlarity tasks and named entity recognition.\\n1 Introduction\\nSemantic vector space models of language repre-\\nsent each word with a real-valued vector. These\\nvectors can be used as features in a variety of ap-\\nplications, such as information retrieval (Manning\\net al., 2008), document classiÔ¨Åcation (Sebastiani,\\n2002), question answering (Tellex et al., 2003),\\nnamed entity recognition (Turian et al., 2010), and\\nparsing (Socher et al., 2013).\\nMost word vector methods rely on the distance\\nor angle between pairs of word vectors as the pri-\\nmary method for evaluating the intrinsic quality\\nof such a set of word representations. Recently,\\nMikolov et al. (2013c) introduced a new evalua-\\ntion scheme based on word analogies that probes\\nthe Ô¨Åner structure of the word vector space by ex-\\namining not the scalar distance between word vec-\\ntors, but rather their various dimensions of dif-\\nference. For example, the analogy ‚Äúking is to\\nqueen as man is to woman‚Äù should be encoded\\nin the vector space by the vector equation king ‚àí\\nqueen = man ‚àíwoman. This evaluation scheme\\nfavors models that produce dimensions of mean-\\ning, thereby capturing the multi-clustering idea of\\ndistributed representations (Bengio, 2009).\\nThe two main model families for learning word\\nvectors are: 1) global matrix factorization meth-\\nods, such as latent semantic analysis (LSA) (Deer-\\nwester et al., 1990) and 2) local context window\\nmethods, such as the skip-gram model of Mikolov\\net al. (2013c). Currently, both families suffer sig-\\nniÔ¨Åcant drawbacks. While methods like LSA ef-\\nÔ¨Åciently leverage statistical information, they do\\nrelatively poorly on the word analogy task, indi-\\ncating a sub-optimal vector space structure. Meth-\\nods like skip-gram may do better on the analogy\\ntask, but they poorly utilize the statistics of the cor-\\npus since they train on separate local context win-\\ndows instead of on global co-occurrence counts.\\nIn this work, we analyze the model properties\\nnecessary to produce linear directions of meaning\\nand argue that global log-bilinear regression mod-\\nels are appropriate for doing so. We propose a spe-\\nciÔ¨Åc weighted least squares model that trains on\\nglobal word-word co-occurrence counts and thus\\nmakes efÔ¨Åcient use of statistics. The model pro-\\nduces a word vector space with meaningful sub-\\nstructure, as evidenced by its state-of-the-art per-\\nformance of 75% accuracy on the word analogy\\ndataset. We also demonstrate that our methods\\noutperform other current methods on several word\\nsimilarity tasks, and also on a common named en-\\ntity recognition (NER) benchmark.\\nWe provide the source code for the model as\\nwell as trained word vectors at http://nlp.\\nstanford.edu/projects/glove/.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\nMatrix Factorization Methods. Matrix factor-\\nization methods for generating low-dimensional\\nword representations have roots stretching as far\\nback as LSA. These methods utilize low-rank ap-\\nproximations to decompose large matrices that\\ncapture statistical information about a corpus. The\\nparticular type of information captured by such\\nmatrices varies by application. In LSA, the ma-\\ntrices are of ‚Äúterm-document‚Äù type, i.e., the rows\\ncorrespond to words or terms, and the columns\\ncorrespond to different documents in the corpus.\\nIn contrast, the Hyperspace Analogue to Language\\n(HAL) (Lund and Burgess, 1996), for example,\\nutilizes matrices of ‚Äúterm-term‚Äù type, i.e., the rows\\nand columns correspond to words and the entries\\ncorrespond to the number of times a given word\\noccurs in the context of another given word.\\nA main problem with HAL and related meth-\\nods is that the most frequent words contribute a\\ndisproportionate amount to the similarity measure:\\nthe number of times two words co-occur with the\\nor and, for example, will have a large effect on\\ntheir similarity despite conveying relatively little\\nabout their semantic relatedness. A number of\\ntechniques exist that addresses this shortcoming of\\nHAL, such as the COALS method (Rohde et al.,\\n2006), in which the co-occurrence matrix is Ô¨Årst\\ntransformed by an entropy- or correlation-based\\nnormalization. An advantage of this type of trans-\\nformation is that the raw co-occurrence counts,\\nwhich for a reasonably sized corpus might span\\n8 or 9 orders of magnitude, are compressed so as\\nto be distributed more evenly in a smaller inter-\\nval. A variety of newer models also pursue this\\napproach, including a study (Bullinaria and Levy,\\n2007) that indicates that positive pointwise mu-\\ntual information (PPMI) is a good transformation.\\nMore recently, a square root type transformation\\nin the form of Hellinger PCA (HPCA) (Lebret and\\nCollobert, 2014) has been suggested as an effec-\\ntive way of learning word representations.\\nShallow Window-Based Methods. Another\\napproach is to learn word representations that aid\\nin making predictions within local context win-\\ndows. For example, Bengio et al. (2003) intro-\\nduced a model that learns word vector representa-\\ntions as part of a simple neural network architec-\\nture for language modeling. Collobert and Weston\\n(2008) decoupled the word vector training from\\nthe downstream training objectives, which paved\\nthe way for Collobert et al. (2011) to use the full\\ncontext of a word for learning the word represen-\\ntations, rather than just the preceding context as is\\nthe case with language models.\\nRecently, the importance of the full neural net-\\nwork structure for learning useful word repre-\\nsentations has been called into question. The\\nskip-gram and continuous bag-of-words (CBOW)\\nmodels of Mikolov et al. (2013a) propose a sim-\\nple single-layer architecture based on the inner\\nproduct between two word vectors. Mnih and\\nKavukcuoglu (2013) also proposed closely-related\\nvector log-bilinear models, vLBL and ivLBL, and\\nLevy et al. (2014) proposed explicit word embed-\\ndings based on a PPMI metric.\\nIn the skip-gram and ivLBL models, the objec-\\ntive is to predict a word‚Äôs context given the word\\nitself, whereas the objective in the CBOW and\\nvLBL models is to predict a word given its con-\\ntext. Through evaluation on a word analogy task,\\nthese models demonstrated the capacity to learn\\nlinguistic patterns as linear relationships between\\nthe word vectors.\\nUnlike the matrix factorization methods, the\\nshallow window-based methods suffer from the\\ndisadvantage that they do not operate directly on\\nthe co-occurrence statistics of the corpus. Instead,\\nthese models scan context windows across the en-\\ntire corpus, which fails to take advantage of the\\nvast amount of repetition in the data.\\n3 The GloVe Model\\nThe statistics of word occurrences in a corpus is\\nthe primary source of information available to all\\nunsupervised methods for learning word represen-\\ntations, and although many such methods now ex-\\nist, the question still remains as to how meaning\\nis generated from these statistics, and how the re-\\nsulting word vectors might represent that meaning.\\nIn this section, we shed some light on this ques-\\ntion. We use our insights to construct a new model\\nfor word representation which we call GloVe, for\\nGlobal Vectors, because the global corpus statis-\\ntics are captured directly by the model.\\nFirst we establish some notation. Let the matrix\\nof word-word co-occurrence counts be denoted by\\nX, whose entries Xi jtabulate the number of times\\nword j occurs in the context of word i. Let Xi =‚àë\\nk Xik be the number of times any word appears\\nin the context of wordi. Finally, let Pi j = P( j|i) =\\nXi j/Xi be the probability that word j appear in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Table 1: Co-occurrence probabilities for target wordsice and steam with selected context words from a 6\\nbillion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion\\ncancel out, so that large values (much greater than 1) correlate well with properties speciÔ¨Åc to ice, and\\nsmall values (much less than 1) correlate well with properties speciÔ¨Åc of steam.\\nProbability and Ratio k = solid k = gas k = water k = fashion\\nP(k|ice) 1.9 √ó10‚àí4 6.6 √ó10‚àí5 3.0 √ó10‚àí3 1.7 √ó10‚àí5\\nP(k|steam) 2.2 √ó10‚àí5 7.8 √ó10‚àí4 2.2 √ó10‚àí3 1.8 √ó10‚àí5\\nP(k|ice)/P(k|steam) 8.9 8 .5 √ó10‚àí2 1.36 0 .96\\ncontext of word i.\\nWe begin with a simple example that showcases\\nhow certain aspects of meaning can be extracted\\ndirectly from co-occurrence probabilities. Con-\\nsider two words i and j that exhibit a particular as-\\npect of interest; for concreteness, suppose we are\\ninterested in the concept of thermodynamic phase,\\nfor which we might take i = ice and j = steam.\\nThe relationship of these words can be examined\\nby studying the ratio of their co-occurrence prob-\\nabilities with various probe words, k. For words\\nk related to ice but not steam, say k = solid, we\\nexpect the ratio Pik /Pjk will be large. Similarly,\\nfor words k related to steam but not ice, say k =\\ngas, the ratio should be small. For words k like\\nwater or fashion, that are either related to both ice\\nand steam, or to neither, the ratio should be close\\nto one. Table 1 shows these probabilities and their\\nratios for a large corpus, and the numbers conÔ¨Årm\\nthese expectations. Compared to the raw probabil-\\nities, the ratio is better able to distinguish relevant\\nwords (solid and gas) from irrelevant words (water\\nand fashion) and it is also better able to discrimi-\\nnate between the two relevant words.\\nThe above argument suggests that the appropri-\\nate starting point for word vector learning should\\nbe with ratios of co-occurrence probabilities rather\\nthan the probabilities themselves. Noting that the\\nratio Pik /Pjk depends on three words i, j, and k,\\nthe most general model takes the form,\\nF(wi,wj , Àúwk ) = Pik\\nPjk\\n, (1)\\nwhere w ‚àà Rd are word vectors and Àú w ‚àà Rd\\nare separate context word vectors whose role will\\nbe discussed in Section 4.2. In this equation, the\\nright-hand side is extracted from the corpus, and\\nF may depend on some as-of-yet unspeciÔ¨Åed pa-\\nrameters. The number of possibilities forF is vast,\\nbut by enforcing a few desiderata we can select a\\nunique choice. First, we would like F to encode\\nthe information present the ratio Pik /Pjk in the\\nword vector space. Since vector spaces are inher-\\nently linear structures, the most natural way to do\\nthis is with vector differences. With this aim, we\\ncan restrict our consideration to those functions F\\nthat depend only on the difference of the two target\\nwords, modifying Eqn. (1) to,\\nF(wi ‚àíwj , Àúwk ) = Pik\\nPjk\\n. (2)\\nNext, we note that the arguments of F in Eqn. (2)\\nare vectors while the right-hand side is a scalar.\\nWhile F could be taken to be a complicated func-\\ntion parameterized by, e.g., a neural network, do-\\ning so would obfuscate the linear structure we are\\ntrying to capture. To avoid this issue, we can Ô¨Årst\\ntake the dot product of the arguments,\\nF\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= Pik\\nPjk\\n, (3)\\nwhich prevents F from mixing the vector dimen-\\nsions in undesirable ways. Next, note that for\\nword-word co-occurrence matrices, the distinction\\nbetween a word and a context word is arbitrary and\\nthat we are free to exchange the two roles. To do so\\nconsistently, we must not only exchange w ‚Üî Àúw\\nbut also X ‚ÜîXT . Our Ô¨Ånal model should be in-\\nvariant under this relabeling, but Eqn. (3) is not.\\nHowever, the symmetry can be restored in two\\nsteps. First, we require that F be a homomorphism\\nbetween the groups (R,+) and (R>0,√ó), i.e.,\\nF\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= F(wT\\ni Àúwk )\\nF(wT\\nj Àúwk ) , (4)\\nwhich, by Eqn. (3), is solved by,\\nF(wT\\ni Àúwk ) = Pik = Xik\\nXi\\n. (5)\\nThe solution to Eqn. (4) is F = exp, or,\\nwT\\ni Àúwk = log(Pik ) = log(Xik ) ‚àílog(Xi ) . (6)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Next, we note that Eqn. (6) would exhibit the ex-\\nchange symmetry if not for the log( Xi ) on the\\nright-hand side. However, this term is indepen-\\ndent of k so it can be absorbed into a bias bi for\\nwi. Finally, adding an additional bias Àúbk for Àúwk\\nrestores the symmetry,\\nwT\\ni Àúwk + bi + Àúbk = log(Xik ) . (7)\\nEqn. (7) is a drastic simpliÔ¨Åcation over Eqn. (1),\\nbut it is actually ill-deÔ¨Åned since the logarithm di-\\nverges whenever its argument is zero. One reso-\\nlution to this issue is to include an additive shift\\nin the logarithm, log( Xik ) ‚Üílog(1 + Xik ), which\\nmaintains the sparsity of X while avoiding the di-\\nvergences. The idea of factorizing the log of the\\nco-occurrence matrix is closely related to LSA and\\nwe will use the resulting model as a baseline in\\nour experiments. A main drawback to this model\\nis that it weighs all co-occurrences equally, even\\nthose that happen rarely or never. Such rare co-\\noccurrences are noisy and carry less information\\nthan the more frequent ones ‚Äî yet even just the\\nzero entries account for 75‚Äì95% of the data in X,\\ndepending on the vocabulary size and corpus.\\nWe propose a new weighted least squares re-\\ngression model that addresses these problems.\\nCasting Eqn. (7) as a least squares problem and\\nintroducing a weighting function f (Xi j) into the\\ncost function gives us the model\\nJ =\\nV‚àë\\ni,j=1\\nf\\n(\\nXi j\\n) (\\nwT\\ni Àúwj + bi + Àúbj ‚àílog Xi j\\n)2\\n,\\n(8)\\nwhere V is the size of the vocabulary. The weight-\\ning function should obey the following properties:\\n1. f (0) = 0. If f is viewed as a continuous\\nfunction, it should vanish as x ‚Üí 0 fast\\nenough that the limx‚Üí0 f (x) log2 x is Ô¨Ånite.\\n2. f (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\n3. f (x) should be relatively small for large val-\\nues of x, so that frequent co-occurrences are\\nnot overweighted.\\nOf course a large number of functions satisfy these\\nproperties, but one class of functions that we found\\nto work well can be parameterized as,\\nf (x) =\\n{ (x/xmax)Œ± if x < xmax\\n1 otherwise . (9)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\nFigure 1: Weighting function f with Œ± = 3/4.\\nThe performance of the model depends weakly on\\nthe cutoff, which we Ô¨Åx to xmax = 100 for all our\\nexperiments. We found that Œ± = 3/4 gives a mod-\\nest improvement over a linear version with Œ± = 1.\\nAlthough we offer only empirical motivation for\\nchoosing the value 3/4, it is interesting that a sim-\\nilar fractional power scaling was found to give the\\nbest performance in (Mikolov et al., 2013a).\\n3.1 Relationship to Other Models\\nBecause all unsupervised methods for learning\\nword vectors are ultimately based on the occur-\\nrence statistics of a corpus, there should be com-\\nmonalities between the models. Nevertheless, cer-\\ntain models remain somewhat opaque in this re-\\ngard, particularly the recent window-based meth-\\nods like skip-gram and ivLBL. Therefore, in this\\nsubsection we show how these models are related\\nto our proposed model, as deÔ¨Åned in Eqn. (8).\\nThe starting point for the skip-gram or ivLBL\\nmethods is a model Qi j for the probability that\\nword j appears in the context of word i. For con-\\ncreteness, let us assume that Qi jis a softmax,\\nQi j = exp(wT\\ni Àúwj )\\n‚àëV\\nk=1 exp(wT\\ni Àúwk )\\n. (10)\\nMost of the details of these models are irrelevant\\nfor our purposes, aside from the the fact that they\\nattempt to maximize the log probability as a con-\\ntext window scans over the corpus. Training pro-\\nceeds in an on-line, stochastic fashion, but the im-\\nplied global objective function can be written as,\\nJ = ‚àí\\n‚àë\\ni‚ààcorpus\\nj‚ààcontext(i)\\nlog Qi j. (11)\\nEvaluating the normalization factor of the soft-\\nmax for each term in this sum is costly. To al-\\nlow for efÔ¨Åcient training, the skip-gram and ivLBL\\nmodels introduce approximations to Qi j. How-\\never, the sum in Eqn. (11) can be evaluated much'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='more efÔ¨Åciently if we Ô¨Årst group together those\\nterms that have the same values for i and j,\\nJ = ‚àí\\nV‚àë\\ni=1\\nV‚àë\\nj=1\\nXi jlog Qi j, (12)\\nwhere we have used the fact that the number of\\nlike terms is given by the co-occurrence matrix X.\\nRecalling our notation for Xi = ‚àë\\nk Xik and\\nPi j = Xi j/Xi, we can rewrite J as,\\nJ = ‚àí\\nV‚àë\\ni=1\\nXi\\nV‚àë\\nj=1\\nPi jlog Qi j =\\nV‚àë\\ni=1\\nXi H(Pi,Qi ) ,\\n(13)\\nwhere H(Pi,Qi ) is the cross entropy of the dis-\\ntributions Pi and Qi, which we deÔ¨Åne in analogy\\nto Xi. As a weighted sum of cross-entropy error,\\nthis objective bears some formal resemblance to\\nthe weighted least squares objective of Eqn. (8).\\nIn fact, it is possible to optimize Eqn. (13) directly\\nas opposed to the on-line training methods used in\\nthe skip-gram and ivLBL models. One could inter-\\npret this objective as a ‚Äúglobal skip-gram‚Äù model,\\nand it might be interesting to investigate further.\\nOn the other hand, Eqn. (13) exhibits a number of\\nundesirable properties that ought to be addressed\\nbefore adopting it as a model for learning word\\nvectors.\\nTo begin, cross entropy error is just one among\\nmany possible distance measures between prob-\\nability distributions, and it has the unfortunate\\nproperty that distributions with long tails are of-\\nten modeled poorly with too much weight given\\nto the unlikely events. Furthermore, for the mea-\\nsure to be bounded it requires that the model dis-\\ntribution Q be properly normalized. This presents\\na computational bottleneck owing to the sum over\\nthe whole vocabulary in Eqn. (10), and it would be\\ndesirable to consider a different distance measure\\nthat did not require this property of Q. A natural\\nchoice would be a least squares objective in which\\nnormalization factors in Q and P are discarded,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n( ÀÜPi j‚àí ÀÜQi j\\n)2 (14)\\nwhere ÀÜPi j = Xi j and ÀÜQi j = exp(wT\\ni Àúwj ) are the\\nunnormalized distributions. At this stage another\\nproblem emerges, namely thatXi joften takes very\\nlarge values, which can complicate the optimiza-\\ntion. An effective remedy is to minimize the\\nsquared error of the logarithms of ÀÜP and ÀÜQ instead,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n(log ÀÜPi j‚àílog ÀÜQi j\\n)2\\n=\\n‚àë\\ni,j\\nXi\\n(wT\\ni Àúwj ‚àílog Xi j\\n)2 . (15)\\nFinally, we observe that while the weighting factor\\nXi is preordained by the on-line training method\\ninherent to the skip-gram and ivLBL models, it is\\nby no means guaranteed to be optimal. In fact,\\nMikolov et al. (2013a) observe that performance\\ncan be increased by Ô¨Åltering the data so as to re-\\nduce the effective value of the weighting factor for\\nfrequent words. With this in mind, we introduce\\na more general weighting function, which we are\\nfree to take to depend on the context word as well.\\nThe result is,\\nÀÜJ =\\n‚àë\\ni,j\\nf (Xi j)(wT\\ni Àúwj ‚àílog Xi j\\n)2 , (16)\\nwhich is equivalent 1 to the cost function of\\nEqn. (8), which we derived previously.\\n3.2 Complexity of the model\\nAs can be seen from Eqn. (8) and the explicit form\\nof the weighting function f (X), the computational\\ncomplexity of the model depends on the number of\\nnonzero elements in the matrix X. As this num-\\nber is always less than the total number of en-\\ntries of the matrix, the model scales no worse than\\nO(|V |2). At Ô¨Årst glance this might seem like a sub-\\nstantial improvement over the shallow window-\\nbased approaches, which scale with the corpus\\nsize, |C|. However, typical vocabularies have hun-\\ndreds of thousands of words, so that |V |2 can be in\\nthe hundreds of billions, which is actually much\\nlarger than most corpora. For this reason it is im-\\nportant to determine whether a tighter bound can\\nbe placed on the number of nonzero elements of\\nX.\\nIn order to make any concrete statements about\\nthe number of nonzero elements in X, it is neces-\\nsary to make some assumptions about the distribu-\\ntion of word co-occurrences. In particular, we will\\nassume that the number of co-occurrences of word\\ni with word j, Xi j, can be modeled as a power-law\\nfunction of the frequency rank of that word pair,\\nri j:\\nXi j = k\\n(ri j)Œ± . (17)\\n1We could also include bias terms in Eqn. (16).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='The total number of words in the corpus is pro-\\nportional to the sum over all elements of the co-\\noccurrence matrix X,\\n|C|‚àº\\n‚àë\\ni j\\nXi j =\\n|X |‚àë\\nr=1\\nk\\nrŒ± = kH |X |,Œ± , (18)\\nwhere we have rewritten the last sum in terms of\\nthe generalized harmonic number Hn,m. The up-\\nper limit of the sum, |X |, is the maximum fre-\\nquency rank, which coincides with the number of\\nnonzero elements in the matrix X. This number is\\nalso equal to the maximum value of r in Eqn. (17)\\nsuch that Xi j ‚â•1, i.e., |X |= k1/Œ±. Therefore we\\ncan write Eqn. (18) as,\\n|C|‚àº| X |Œ± H|X |,Œ± . (19)\\nWe are interested in how|X |is related to |C|when\\nboth numbers are large; therefore we are free to\\nexpand the right hand side of the equation for large\\n|X |. For this purpose we use the expansion of gen-\\neralized harmonic numbers (Apostol, 1976),\\nHx,s = x1‚àís\\n1 ‚àís + Œ∂(s) + O(x‚àís ) if s >0, s , 1 ,\\n(20)\\ngiving,\\n|C|‚àº |X |\\n1 ‚àíŒ± + Œ∂(Œ±) |X |Œ± + O(1) , (21)\\nwhere Œ∂(s) is the Riemann zeta function. In the\\nlimit that X is large, only one of the two terms on\\nthe right hand side of Eqn. (21) will be relevant,\\nand which term that is depends on whether Œ± >1,\\n|X |=\\n{ O(|C|) if Œ± <1,\\nO(|C|1/Œ±) if Œ± >1. (22)\\nFor the corpora studied in this article, we observe\\nthat Xi j is well-modeled by Eqn. (17) with Œ± =\\n1.25. In this case we have that |X |= O(|C|0.8).\\nTherefore we conclude that the complexity of the\\nmodel is much better than the worst case O(V2),\\nand in fact it does somewhat better than the on-line\\nwindow-based methods which scale like O(|C|).\\n4 Experiments\\n4.1 Evaluation methods\\nWe conduct experiments on the word analogy\\ntask of Mikolov et al. (2013a), a variety of word\\nsimilarity tasks, as described in (Luong et al.,\\n2013), and on the CoNLL-2003 shared benchmark\\nTable 2: Results on the word analogy task, given\\nas percent accuracy. Underlined scores are best\\nwithin groups of similarly-sized models; bold\\nscores are best overall. HPCA vectors are publicly\\navailable2; (i)vLBL results are from (Mnih et al.,\\n2013); skip-gram (SG) and CBOW results are\\nfrom (Mikolov et al., 2013a,b); we trained SG ‚Ä†\\nand CBOW‚Ä†using the word2vec tool3. See text\\nfor details and a description of the SVD models.\\nModel Dim. Size Sem. Syn. Tot.\\nivLBL 100 1.5B 55.9 50.1 53.2\\nHPCA 100 1.6B 4.2 16.4 10.8\\nGloVe 100 1.6B 67.5 54.3 60.3\\nSG 300 1B 61 61 61\\nCBOW 300 1.6B 16.1 52.6 36.1\\nvLBL 300 1.5B 54.2 64.8 60.0\\nivLBL 300 1.5B 65.2 63.0 64.0\\nGloVe 300 1.6B 80.8 61.5 70.3\\nSVD 300 6B 6.3 8.1 7.3\\nSVD-S 300 6B 36.7 46.6 42.1\\nSVD-L 300 6B 56.6 63.0 60.1\\nCBOW‚Ä† 300 6B 63.6 67.4 65.7\\nSG‚Ä† 300 6B 73.0 66.0 69.1\\nGloVe 300 6B 77.4 67.0 71.7\\nCBOW 1000 6B 57.3 68.9 63.7\\nSG 1000 6B 66.1 65.1 65.6\\nSVD-L 300 42B 38.4 58.2 49.2\\nGloVe 300 42B 81.9 69.3 75.0\\ndataset for NER (Tjong Kim Sang and De Meul-\\nder, 2003).\\nWord analogies.The word analogy task con-\\nsists of questions like, ‚Äú a is to b as c is to ?‚Äù\\nThe dataset contains 19,544 such questions, di-\\nvided into a semantic subset and a syntactic sub-\\nset. The semantic questions are typically analogies\\nabout people or places, like ‚ÄúAthens is to Greece\\nas Berlin is to ?‚Äù. The syntactic questions are\\ntypically analogies about verb tenses or forms of\\nadjectives, for example ‚Äúdance is to dancing as Ô¨Çy\\nis to ?‚Äù. To correctly answer the question, the\\nmodel should uniquely identify the missing term,\\nwith only an exact correspondence counted as a\\ncorrect match. We answer the question ‚Äú a is to b\\nas c is to ?‚Äù by Ô¨Ånding the word d whose repre-\\nsentation wd is closest to wb ‚àíwa + wc according\\nto the cosine similarity.4\\n2http://lebret.ch/words/\\n3http://code.google.com/p/word2vec/\\n4Levy et al. (2014) introduce a multiplicative analogy\\nevaluation, 3COSMUL, and report an accuracy of 68.24% on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='0 100 200 300 400 500 60020\\n30\\n40\\n50\\n60\\n70\\n80\\nVector Dimension\\nAccuracy [%]\\n \\nSemantic\\nSyntactic\\nOverall\\n(a) Symmetric context\\n2 4 6 8 1040\\n50\\n55\\n60\\n65\\n70\\n45\\nWindow Size\\nAccuracy [%]\\n \\n \\n \\nSemantic\\nSyntactic\\nOverall (b) Symmetric context\\n2 4 6 8 1040\\n50\\n55\\n60\\n65\\n70\\n45\\nWindow Size\\nAccuracy [%]\\n \\n \\n \\nSemantic\\nSyntactic\\nOverall (c) Asymmetric context\\nFigure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are\\ntrained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.\\nWord similarity. While the analogy task is our\\nprimary focus since it tests for interesting vector\\nspace substructures, we also evaluate our model on\\na variety of word similarity tasks in Table 3. These\\ninclude WordSim-353 (Finkelstein et al., 2001),\\nMC (Miller and Charles, 1991), RG (Rubenstein\\nand Goodenough, 1965), SCWS (Huang et al.,\\n2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003\\nEnglish benchmark dataset for NER is a collec-\\ntion of documents from Reuters newswire articles,\\nannotated with four entity types: person, location,\\norganization, and miscellaneous. We train mod-\\nels on CoNLL-03 training data on test on three\\ndatasets: 1) ConLL-03 testing data, 2) ACE Phase\\n2 (2001-02) and ACE-2003 data, and 3) MUC7\\nFormal Run test set. We adopt the BIO2 annota-\\ntion standard, as well as all the preprocessing steps\\ndescribed in (Wang and Manning, 2013). We use a\\ncomprehensive set of discrete features that comes\\nwith the standard distribution of the Stanford NER\\nmodel (Finkel et al., 2005). A total of 437,905\\ndiscrete features were generated for the CoNLL-\\n2003 training dataset. In addition, 50-dimensional\\nvectors for each word of a Ô¨Åve-word context are\\nadded and used as continuous features. With these\\nfeatures as input, we trained a conditional random\\nÔ¨Åeld (CRF) with exactly the same setup as the\\nCRFjoin model of (Wang and Manning, 2013).\\n4.2 Corpora and training details\\nWe trained our model on Ô¨Åve corpora of varying\\nsizes: a 2010 Wikipedia dump with 1 billion to-\\nkens; a 2014 Wikipedia dump with 1.6 billion to-\\nkens; Gigaword 5 which has 4.3 billion tokens; the\\ncombination Gigaword5 + Wikipedia2014, which\\nthe analogy task. This number is evaluated on a subset of the\\ndataset so it is not included in Table 2. 3COSMUL performed\\nworse than cosine similarity in almost all of our experiments.\\nhas 6 billion tokens; and on 42 billion tokens of\\nweb data, from Common Crawl 5. We tokenize\\nand lowercase each corpus with the Stanford to-\\nkenizer, build a vocabulary of the 400,000 most\\nfrequent words6, and then construct a matrix of co-\\noccurrence counts X. In constructing X, we must\\nchoose how large the context window should be\\nand whether to distinguish left context from right\\ncontext. We explore the effect of these choices be-\\nlow. In all cases we use a decreasing weighting\\nfunction, so that word pairs that are d words apart\\ncontribute 1/d to the total count. This is one way\\nto account for the fact that very distant word pairs\\nare expected to contain less relevant information\\nabout the words‚Äô relationship to one another.\\nFor all our experiments, we set xmax = 100,\\nŒ± = 3/4, and train the model using AdaGrad\\n(Duchi et al., 2011), stochastically sampling non-\\nzero elements from X, with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than\\n300 dimensions, and 100 iterations otherwise (see\\nSection 4.6 for more details about the convergence\\nrate). Unless otherwise noted, we use a context of\\nten words to the left and ten words to the right.\\nThe model generates two sets of word vectors,\\nW and ÀúW. When X is symmetric, W and ÀúW are\\nequivalent and differ only as a result of their ran-\\ndom initializations; the two sets of vectors should\\nperform equivalently. On the other hand, there is\\nevidence that for certain types of neural networks,\\ntraining multiple instances of the network and then\\ncombining the results can help reduce overÔ¨Åtting\\nand noise and generally improve results (Ciresan\\net al., 2012). With this in mind, we choose to use\\n5To demonstrate the scalability of the model, we also\\ntrained it on a much larger sixth corpus, containing 840 bil-\\nlion tokens of web data, but in this case we did not lowercase\\nthe vocabulary, so the results are not directly comparable.\\n6For the model trained on Common Crawl data, we use a\\nlarger vocabulary of about 2 million words.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='the sum W + ÀúW as our word vectors. Doing so typ-\\nically gives a small boost in performance, with the\\nbiggest increase in the semantic analogy task.\\nWe compare with the published results of a va-\\nriety of state-of-the-art models, as well as with\\nour own results produced using the word2vec\\ntool and with several baselines using SVDs. With\\nword2vec, we train the skip-gram (SG ‚Ä†) and\\ncontinuous bag-of-words (CBOW‚Ä†) models on the\\n6 billion token corpus (Wikipedia 2014 + Giga-\\nword 5) with a vocabulary of the top 400,000 most\\nfrequent words and a context window size of 10.\\nWe used 10 negative samples, which we show in\\nSection 4.6 to be a good choice for this corpus.\\nFor the SVD baselines, we generate a truncated\\nmatrix Xtrunc which retains the information of how\\nfrequently each word occurs with only the top\\n10,000 most frequent words. This step is typi-\\ncal of many matrix-factorization-based methods as\\nthe extra columns can contribute a disproportion-\\nate number of zero entries and the methods are\\notherwise computationally expensive.\\nThe singular vectors of this matrix constitute\\nthe baseline ‚ÄúSVD‚Äù. We also evaluate two related\\nbaselines: ‚ÄúSVD-S‚Äù in which we take the SVD of‚àöXtrunc, and ‚ÄúSVD-L‚Äù in which we take the SVD\\nof log(1+ Xtrunc). Both methods help compress the\\notherwise large range of values in X.7\\n4.3 Results\\nWe present results on the word analogy task in Ta-\\nble 2. The GloVe model performs signiÔ¨Åcantly\\nbetter than the other baselines, often with smaller\\nvector sizes and smaller corpora. Our results us-\\ning the word2vec tool are somewhat better than\\nmost of the previously published results. This is\\ndue to a number of factors, including our choice to\\nuse negative sampling (which typically works bet-\\nter than the hierarchical softmax), the number of\\nnegative samples, and the choice of the corpus.\\nWe demonstrate that the model can easily be\\ntrained on a large 42 billion token corpus, with a\\nsubstantial corresponding performance boost. We\\nnote that increasing the corpus size does not guar-\\nantee improved results for other models, as can be\\nseen by the decreased performance of the SVD-\\n7We also investigated several other weighting schemes for\\ntransforming X; what we report here performed best. Many\\nweighting schemes like PPMI destroy the sparsity of X and\\ntherefore cannot feasibly be used with large vocabularies.\\nWith smaller vocabularies, these information-theoretic trans-\\nformations do indeed work well on word similarity measures,\\nbut they perform very poorly on the word analogy task.\\nTable 3: Spearman rank correlation on word simi-\\nlarity tasks. All vectors are 300-dimensional. The\\nCBOW‚àóvectors are from the word2vec website\\nand differ in that they contain phrase vectors.\\nModel Size WS353 MC RG SCWS RW\\nSVD 6B 35.3 35.1 42.5 38.3 25.6\\nSVD-S 6B 56.5 71.5 71.0 53.6 34.7\\nSVD-L 6B 65.7 72.7 75.1 56.5 37.0\\nCBOW‚Ä† 6B 57.2 65.6 68.2 57.0 32.5\\nSG‚Ä† 6B 62.8 65.2 69.7 58.1 37.2\\nGloVe 6B 65.8 72.7 77.8 53.9 38.1\\nSVD-L 42B 74.0 76.4 74.1 58.3 39.9\\nGloVe 42B 75.9 83.6 82.9 59.6 47.8\\nCBOW‚àó 100B 68.4 79.6 75.4 59.4 45.5\\nL model on this larger corpus. The fact that this\\nbasic SVD model does not scale well to large cor-\\npora lends further evidence to the necessity of the\\ntype of weighting scheme proposed in our model.\\nTable 3 shows results on Ô¨Åve different word\\nsimilarity datasets. A similarity score is obtained\\nfrom the word vectors by Ô¨Årst normalizing each\\nfeature across the vocabulary and then calculat-\\ning the cosine similarity. We compute Spearman‚Äôs\\nrank correlation coefÔ¨Åcient between this score and\\nthe human judgments. CBOW ‚àó denotes the vec-\\ntors available on the word2vec website that are\\ntrained with word and phrase vectors on 100B\\nwords of news data. GloVe outperforms it while\\nusing a corpus less than half the size.\\nTable 4 shows results on the NER task with the\\nCRF-based model. The L-BFGS training termi-\\nnates when no improvement has been achieved on\\nthe dev set for 25 iterations. Otherwise all conÔ¨Åg-\\nurations are identical to those used by Wang and\\nManning (2013). The model labeled Discrete is\\nthe baseline using a comprehensive set of discrete\\nfeatures that comes with the standard distribution\\nof the Stanford NER model, but with no word vec-\\ntor features. In addition to the HPCA and SVD\\nmodels discussed previously, we also compare to\\nthe models of Huang et al. (2012) (HSMN) and\\nCollobert and Weston (2008) (CW). We trained\\nthe CBOW model using the word2vec tool8.\\nThe GloVe model outperforms all other methods\\non all evaluation metrics, except for the CoNLL\\ntest set, on which the HPCA method does slightly\\nbetter. We conclude that the GloVe vectors are\\nuseful in downstream NLP tasks, as was Ô¨Årst\\n8We use the same parameters as above, except in this case\\nwe found 5 negative samples to work slightly better than 10.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Table 4: F1 score on NER task with 50d vectors.\\nDiscrete is the baseline without word vectors. We\\nuse publicly-available vectors for HPCA, HSMN,\\nand CW. See text for details.\\nModel Dev Test ACE MUC7\\nDiscrete 91.0 85.4 77.4 73.4\\nSVD 90.8 85.7 77.3 73.7\\nSVD-S 91.0 85.5 77.6 74.3\\nSVD-L 90.5 84.8 73.6 71.5\\nHPCA 92.6 88.7 81.7 80.7\\nHSMN 90.5 85.7 78.7 74.7\\nCW 92.2 87.4 81.7 80.2\\nCBOW 93.1 88.2 82.2 81.1\\nGloVe 93.2 88.3 82.9 82.2\\nshown for neural vectors in (Turian et al., 2010).\\n4.4 Model Analysis: Vector Length and\\nContext Size\\nIn Fig. 2, we show the results of experiments that\\nvary vector length and context window. A context\\nwindow that extends to the left and right of a tar-\\nget word will be called symmetric, and one which\\nextends only to the left will be called asymmet-\\nric. In (a), we observe diminishing returns for vec-\\ntors larger than about 200 dimensions. In (b) and\\n(c), we examine the effect of varying the window\\nsize for symmetric and asymmetric context win-\\ndows. Performance is better on the syntactic sub-\\ntask for small and asymmetric context windows,\\nwhich aligns with the intuition that syntactic infor-\\nmation is mostly drawn from the immediate con-\\ntext and can depend strongly on word order. Se-\\nmantic information, on the other hand, is more fre-\\nquently non-local, and more of it is captured with\\nlarger window sizes.\\n4.5 Model Analysis: Corpus Size\\nIn Fig. 3, we show performance on the word anal-\\nogy task for 300-dimensional vectors trained on\\ndifferent corpora. On the syntactic subtask, there\\nis a monotonic increase in performance as the cor-\\npus size increases. This is to be expected since\\nlarger corpora typically produce better statistics.\\nInterestingly, the same trend is not true for the se-\\nmantic subtask, where the models trained on the\\nsmaller Wikipedia corpora do better than those\\ntrained on the larger Gigaword corpus. This is\\nlikely due to the large number of city- and country-\\nbased analogies in the analogy dataset and the fact\\nthat Wikipedia has fairly comprehensive articles\\nfor most such locations. Moreover, Wikipedia‚Äôs\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\nOverallSyntacticSemantic\\nWiki2010\\n1B tokens\\nAccuracy [%]\\nWiki2014\\n1.6B tokens\\nGigaword5\\n4.3B tokens\\nGigaword5 + \\nWiki2014\\n6B tokens\\nCommon Crawl \\n42B tokens\\nFigure 3: Accuracy on the analogy task for 300-\\ndimensional vectors trained on different corpora.\\nentries are updated to assimilate new knowledge,\\nwhereas Gigaword is a Ô¨Åxed news repository with\\noutdated and possibly incorrect information.\\n4.6 Model Analysis: Run-time\\nThe total run-time is split between populating X\\nand training the model. The former depends on\\nmany factors, including window size, vocabulary\\nsize, and corpus size. Though we did not do so,\\nthis step could easily be parallelized across mul-\\ntiple machines (see, e.g., Lebret and Collobert\\n(2014) for some benchmarks). Using a single\\nthread of a dual 2.1GHz Intel Xeon E5-2658 ma-\\nchine, populating X with a 10 word symmetric\\ncontext window, a 400,000 word vocabulary, and\\na 6 billion token corpus takes about 85 minutes.\\nGiven X, the time it takes to train the model de-\\npends on the vector size and the number of itera-\\ntions. For 300-dimensional vectors with the above\\nsettings (and using all 32 cores of the above ma-\\nchine), a single iteration takes 14 minutes. See\\nFig. 4 for a plot of the learning curve.\\n4.7 Model Analysis: Comparison with\\nword2vec\\nA rigorous quantitative comparison of GloVe with\\nword2vec is complicated by the existence of\\nmany parameters that have a strong effect on per-\\nformance. We control for the main sources of vari-\\nation that we identiÔ¨Åed in Sections 4.4 and 4.5 by\\nsetting the vector length, context window size, cor-\\npus, and vocabulary size to the conÔ¨Åguration men-\\ntioned in the previous subsection.\\nThe most important remaining variable to con-\\ntrol for is training time. For GloVe, the rele-\\nvant parameter is the number of training iterations.\\nFor word2vec, the obvious choice would be the\\nnumber of training epochs. Unfortunately, the\\ncode is currently designed for only a single epoch:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='1 2 3 4 5 6\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n5 10 15 20 25\\n1357 10 15 20 25 30 40 50\\nAccuracy [%]\\nIterations (GloVe)\\nNegative Samples (CBOW)\\nTraining Time (hrs)\\n \\nGloVe\\nCBOW\\n(a) GloVe vs CBOW\\n3 6 9 12 15 18 21 24\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n20 40 60 80 100\\n1 2 3 4 5 6 7 10 12 15 20\\nGloVe\\nSkip-Gram\\nAccuracy [%]\\nIterations (GloVe)\\nNegative Samples (Skip-Gram)\\nTraining Time (hrs) (b) GloVe vs Skip-Gram\\nFigure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by\\nthe number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram\\n(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +\\nGigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.\\nit speciÔ¨Åes a learning schedule speciÔ¨Åc to a single\\npass through the data, making a modiÔ¨Åcation for\\nmultiple passes a non-trivial task. Another choice\\nis to vary the number of negative samples. Adding\\nnegative samples effectively increases the number\\nof training words seen by the model, so in some\\nways it is analogous to extra epochs.\\nWe set any unspeciÔ¨Åed parameters to their de-\\nfault values, assuming that they are close to opti-\\nmal, though we acknowledge that this simpliÔ¨Åca-\\ntion should be relaxed in a more thorough analysis.\\nIn Fig. 4, we plot the overall performance on\\nthe analogy task as a function of training time.\\nThe two x-axes at the bottom indicate the corre-\\nsponding number of training iterations for GloVe\\nand negative samples for word2vec. We note\\nthat word2vec‚Äôs performance actually decreases\\nif the number of negative samples increases be-\\nyond about 10. Presumably this is because the\\nnegative sampling method does not approximate\\nthe target probability distribution well.9\\nFor the same corpus, vocabulary, window size,\\nand training time, GloVe consistently outperforms\\nword2vec. It achieves better results faster, and\\nalso obtains the best results irrespective of speed.\\n5 Conclusion\\nRecently, considerable attention has been focused\\non the question of whether distributional word\\nrepresentations are best learned from count-based\\n9In contrast, noise-contrastive estimation is an approxi-\\nmation which improves with more negative samples. In Ta-\\nble 1 of (Mnih et al., 2013), accuracy on the analogy task is a\\nnon-decreasing function of the number of negative samples.\\nmethods or from prediction-based methods. Cur-\\nrently, prediction-based models garner substantial\\nsupport; for example, Baroni et al. (2014) argue\\nthat these models perform better across a range of\\ntasks. In this work we argue that the two classes\\nof methods are not dramatically different at a fun-\\ndamental level since they both probe the under-\\nlying co-occurrence statistics of the corpus, but\\nthe efÔ¨Åciency with which the count-based meth-\\nods capture global statistics can be advantageous.\\nWe construct a model that utilizes this main ben-\\neÔ¨Åt of count data while simultaneously capturing\\nthe meaningful linear substructures prevalent in\\nrecent log-bilinear prediction-based methods like\\nword2vec. The result, GloVe, is a new global\\nlog-bilinear regression model for the unsupervised\\nlearning of word representations that outperforms\\nother models on word analogy, word similarity,\\nand named entity recognition tasks.\\nAcknowledgments\\nWe thank the anonymous reviewers for their valu-\\nable comments. Stanford University gratefully\\nacknowledges the support of the Defense Threat\\nReduction Agency (DTRA) under Air Force Re-\\nsearch Laboratory (AFRL) contract no. FA8650-\\n10-C-7020 and the Defense Advanced Research\\nProjects Agency (DARPA) Deep Exploration and\\nFiltering of Text (DEFT) Program under AFRL\\ncontract no. FA8750-13-2-0040. Any opinions,\\nÔ¨Åndings, and conclusion or recommendations ex-\\npressed in this material are those of the authors and\\ndo not necessarily reÔ¨Çect the view of the DTRA,\\nAFRL, DEFT, or the US government.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='References\\nTom M. Apostol. 1976. Introduction to Analytic\\nNumber Theory. Introduction to Analytic Num-\\nber Theory.\\nMarco Baroni, Georgiana Dinu, and Germ ¬¥an\\nKruszewski. 2014. Don‚Äôt count, predict! A\\nsystematic comparison of context-counting vs.\\ncontext-predicting semantic vectors. In ACL.\\nYoshua Bengio. 2009. Learning deep architectures\\nfor AI. Foundations and Trends in Machine\\nLearning.\\nYoshua Bengio, R ¬¥ejean Ducharme, Pascal Vin-\\ncent, and Christian Janvin. 2003. A neural prob-\\nabilistic language model. JMLR, 3:1137‚Äì1155.\\nJohn A. Bullinaria and Joseph P. Levy. 2007. Ex-\\ntracting semantic representations from word co-\\noccurrence statistics: A computational study.\\nBehavior Research Methods, 39(3):510‚Äì526.\\nDan C. Ciresan, Alessandro Giusti, Luca M. Gam-\\nbardella, and J ¬®urgen Schmidhuber. 2012. Deep\\nneural networks segment neuronal membranes\\nin electron microscopy images. In NIPS, pages\\n2852‚Äì2860.\\nRonan Collobert and Jason Weston. 2008. A uni-\\nÔ¨Åed architecture for natural language process-\\ning: deep neural networks with multitask learn-\\ning. In Proceedings of ICML, pages 160‚Äì167.\\nRonan Collobert, Jason Weston, L ¬¥eon Bottou,\\nMichael Karlen, Koray Kavukcuoglu, and Pavel\\nKuksa. 2011. Natural Language Processing (Al-\\nmost) from Scratch. JMLR, 12:2493‚Äì2537.\\nScott Deerwester, Susan T. Dumais, George W.\\nFurnas, Thomas K. Landauer, and Richard\\nHarshman. 1990. Indexing by latent semantic\\nanalysis. Journal of the American Society for\\nInformation Science, 41.\\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\\nAdaptive subgradient methods for online learn-\\ning and stochastic optimization. JMLR, 12.\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma-\\ntias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. 2001. Placing search in con-\\ntext: The concept revisited. In Proceedings\\nof the 10th international conference on World\\nWide Web, pages 406‚Äì414. ACM.\\nEric H. Huang, Richard Socher, Christopher D.\\nManning, and Andrew Y . Ng. 2012. Improving\\nWord Representations via Global Context and\\nMultiple Word Prototypes. In ACL.\\nR¬¥emi Lebret and Ronan Collobert. 2014. Word\\nembeddings through Hellinger PCA. In EACL.\\nOmer Levy, Yoav Goldberg, and Israel Ramat-\\nGan. 2014. Linguistic regularities in sparse and\\nexplicit word representations. CoNLL-2014.\\nKevin Lund and Curt Burgess. 1996. Producing\\nhigh-dimensional semantic spaces from lexical\\nco-occurrence. Behavior Research Methods, In-\\nstrumentation, and Computers, 28:203‚Äì208.\\nMinh-Thang Luong, Richard Socher, and Christo-\\npher D Manning. 2013. Better word represen-\\ntations with recursive neural networks for mor-\\nphology. CoNLL-2013.\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\\nfrey Dean. 2013a. EfÔ¨Åcient Estimation of Word\\nRepresentations in Vector Space. InICLR Work-\\nshop Papers.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg\\nCorrado, and Jeffrey Dean. 2013b. Distributed\\nrepresentations of words and phrases and their\\ncompositionality. In NIPS, pages 3111‚Äì3119.\\nTomas Mikolov, Wen tau Yih, and Geoffrey\\nZweig. 2013c. Linguistic regularities in con-\\ntinuous space word representations. In HLT-\\nNAACL.\\nGeorge A. Miller and Walter G. Charles. 1991.\\nContextual correlates of semantic similarity.\\nLanguage and cognitive processes, 6(1):1‚Äì28.\\nAndriy Mnih and Koray Kavukcuoglu. 2013.\\nLearning word embeddings efÔ¨Åciently with\\nnoise-contrastive estimation. In NIPS.\\nDouglas L. T. Rohde, Laura M. Gonnerman,\\nand David C. Plaut. 2006. An improved\\nmodel of semantic similarity based on lexical\\nco-occurence. Communications of the ACM,\\n8:627‚Äì633.\\nHerbert Rubenstein and John B. Goodenough.\\n1965. Contextual correlates of synonymy.Com-\\nmunications of the ACM, 8(10):627‚Äì633.\\nFabrizio Sebastiani. 2002. Machine learning in au-\\ntomated text categorization. ACM Computing\\nSurveys, 34:1‚Äì47.\\nRichard Socher, John Bauer, Christopher D. Man-\\nning, and Andrew Y . Ng. 2013. Parsing With\\nCompositional Vector Grammars. In ACL.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron\\nFernandes, and Gregory Marton. 2003. Quanti-\\ntative evaluation of passage retrieval algorithms\\nfor question answering. In Proceedings of the\\nSIGIR Conference on Research and Develop-\\nment in Informaion Retrieval.\\nErik F. Tjong Kim Sang and Fien De Meul-\\nder. 2003. Introduction to the CoNLL-2003\\nshared task: Language-independent named en-\\ntity recognition. In CoNLL-2003.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\\n2010. Word representations: a simple and gen-\\neral method for semi-supervised learning. In\\nProceedings of ACL, pages 384‚Äì394.\\nMengqiu Wang and Christopher D. Manning.\\n2013. Effect of non-linear deep architecture in\\nsequence labeling. In Proceedings of the 6th\\nInternational Joint Conference on Natural Lan-\\nguage Processing (IJCNLP).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be Ô¨Åne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciÔ¨Åc architecture modiÔ¨Åcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce Ô¨Åne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and Ô¨Åne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciÔ¨Åc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The Ô¨Åne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciÔ¨Åc parameters, and is trained on the\\ndownstream tasks by simply Ô¨Åne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the Ô¨Åne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying Ô¨Åne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the Ô¨Åne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a ‚Äúmasked lan-\\nguage model‚Äù (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\narXiv:1810.04805v2  [cs.CL]  24 May 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na ‚Äúnext sentence prediction‚Äù task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n‚Ä¢ We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n‚Ä¢ We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciÔ¨Åc architectures.\\n‚Ä¢ BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieÔ¨Çy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiÔ¨Åcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciÔ¨Åc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the Ô¨Årst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nÔ¨Åne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT BERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nNERMNLI\\nFigure 1: Overall pre-training and Ô¨Åne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During Ô¨Åne-tuning, all parameters are Ô¨Åne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to Ô¨Åne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and Ô¨Åne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT model is Ô¨Årst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are Ô¨Åne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate Ô¨Åne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniÔ¨Åed ar-\\nchitecture across different tasks. There is mini-\\nmal difference between the pre-trained architec-\\nture and the Ô¨Ånal downstream architecture.\\nModel Architecture BERT‚Äôs model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as ‚ÄúThe Annotated Transformer.‚Äù2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorÔ¨Çow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/Ô¨Ålter size to be 4H,\\ni.e., 3072 for the H = 768and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ‚ü®Question, Answer ‚ü©) in one token sequence.\\nThroughout this work, a ‚Äúsentence‚Äù can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A ‚Äúsequence‚Äù refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The Ô¨Årst\\ntoken of every sequence is always a special clas-\\nsiÔ¨Åcation token ( [CLS]). The Ô¨Ånal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiÔ¨Åcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the Ô¨Ånal hidden\\nvector of the special [CLS] token as C ‚ààRH,\\nand the Ô¨Ånal hidden vector for the ith input token\\nas Ti ‚ààRH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly ‚Äúsee itself‚Äù, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a ‚ÄúTransformer encoder‚Äù while\\nthe left-context-only version is referred to as a ‚ÄúTransformer\\ndecoder‚Äù since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a ‚Äúmasked\\nLM‚Äù (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the Ô¨Ånal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nÔ¨Åne-tuning, since the [MASK] token does not ap-\\npear during Ô¨Åne-tuning. To mitigate this, we do\\nnot always replace ‚Äúmasked‚Äù words with the ac-\\ntual [MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthe i-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTi will be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-\\ntionships, we pre-train for a binarized next sen-\\ntence prediction task that can be trivially gener-\\nated from any monolingual corpus. SpeciÔ¨Åcally,\\nwhen choosing the sentencesA and B for each pre-\\ntraining example, 50% of the time B is the actual\\nnext sentence that follows A (labeled as IsNext),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext). As we show\\nin Figure 1, C is used for next sentence predic-\\ntion (NSP). 5 Despite its simplicity, we demon-\\nstrate in Section 5.1 that pre-training towards this\\ntask is very beneÔ¨Åcial to both QA and NLI. 6\\n5The Ô¨Ånal model achieves 97%-98% accuracy on NSP.\\n6The vector C is not a meaningful sentence representation\\nwithout Ô¨Åne-tuning, since it was trained with NSP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='[CLS] he likes play ##ing [SEP]my dog is cute [SEP]Input\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP]Emy Edog Eis Ecute E[SEP]\\nToken\\nEmbeddings\\nEA EB EB EB EB EBEA EA EA EA EASegment\\nEmbeddings\\nE0 E6 E7 E8 E9 E10E1 E2 E3 E4 E5Position\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufÔ¨Çed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks‚Äî\\nwhether they involve single text or text pairs‚Äîby\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciÔ¨Åc inputs and outputs into BERT and Ô¨Åne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence A and sentence B from pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and\\n(4) a degenerate text- ‚àÖ pair in text classiÔ¨Åcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiÔ¨Åcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, Ô¨Åne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model. 7 We de-\\nscribe the task-speciÔ¨Åc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT Ô¨Åne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo Ô¨Åne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the Ô¨Ånal hid-\\nden vector C ‚àà RH corresponding to the Ô¨Årst\\ninput token ([CLS]) as the aggregate representa-\\ntion. The only new parameters introduced during\\nÔ¨Åne-tuning are classiÔ¨Åcation layer weights W ‚àà\\nRK√óH, where Kis the number of labels. We com-\\npute a standard classiÔ¨Åcation loss with C and W,\\ni.e., log(softmax(CWT )).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\\nBERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\\nBERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard).\\nThe number below each task denotes the number of training examples. The ‚ÄúAverage‚Äù column is slightly different\\nthan the ofÔ¨Åcial GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are single-\\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\nWe use a batch size of 32 and Ô¨Åne-tune for 3\\nepochs over the data for all GLUE tasks. For each\\ntask, we selected the best Ô¨Åne-tuning learning rate\\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\\nAdditionally, for BERTLARGE we found that Ô¨Åne-\\ntuning was sometimes unstable on small datasets,\\nso we ran several random restarts and selected the\\nbest model on the Dev set. With random restarts,\\nwe use the same pre-trained checkpoint but per-\\nform different Ô¨Åne-tuning data shufÔ¨Çing and clas-\\nsiÔ¨Åer layer initialization.9\\nResults are presented in Table 1. Both\\nBERTBASE and BERTLARGE outperform all sys-\\ntems on all tasks by a substantial margin, obtaining\\n4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERTBASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-\\ntention masking. For the largest and most widely\\nreported GLUE task, MNLI, BERT obtains a 4.6%\\nabsolute accuracy improvement. On the ofÔ¨Åcial\\nGLUE leaderboard10, BERTLARGE obtains a score\\nof 80.5, compared to OpenAI GPT, which obtains\\n72.8 as of the date of writing.\\nWe Ô¨Ånd that BERT LARGE signiÔ¨Åcantly outper-\\nforms BERTBASE across all tasks, especially those\\nwith very little training data. The effect of model\\nsize is explored more thoroughly in Section 5.2.\\n4.2 SQuAD v1.1\\nThe Stanford Question Answering Dataset\\n(SQuAD v1.1) is a collection of 100k crowd-\\nsourced question/answer pairs (Rajpurkar et al.,\\n2016). Given a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server\\nsubmission for each of BERTBASE and BERTLARGE .\\n10https://gluebenchmark.com/leaderboard\\nWikipedia containing the answer, the task is to\\npredict the answer text span in the passage.\\nAs shown in Figure 1, in the question answer-\\ning task, we represent the input question and pas-\\nsage as a single packed sequence, with the ques-\\ntion using the A embedding and the passage using\\nthe B embedding. We only introduce a start vec-\\ntor S ‚ààRH and an end vector E ‚ààRH during\\nÔ¨Åne-tuning. The probability of word i being the\\nstart of the answer span is computed as a dot prod-\\nuct between Ti and S followed by a softmax over\\nall of the words in the paragraph: Pi = eS¬∑Ti\\n‚àë\\nj eS¬∑Tj .\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\nposition ito position jis deÔ¨Åned as S¬∑Ti + E¬∑Tj,\\nand the maximum scoring span where j ‚â• i is\\nused as a prediction. The training objective is the\\nsum of the log-likelihoods of the correct start and\\nend positions. We Ô¨Åne-tune for 3 epochs with a\\nlearning rate of 5e-5 and a batch size of 32.\\nTable 2 shows top leaderboard entries as well\\nas results from top published systems (Seo et al.,\\n2017; Clark and Gardner, 2018; Peters et al.,\\n2018a; Hu et al., 2018). The top results from the\\nSQuAD leaderboard do not have up-to-date public\\nsystem descriptions available,11 and are allowed to\\nuse any public data when training their systems.\\nWe therefore use modest data augmentation in\\nour system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi\\net al., 2017) befor Ô¨Åne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and\\n+1.3 F1 as a single system. In fact, our single\\nBERT model outperforms the top ensemble sys-\\ntem in terms of F1 score. Without TriviaQA Ô¨Åne-\\n11QANet is described in Yu et al. (2018), but the system\\nhas improved substantially after publication.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='System Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman - - 82.3 91.2\\n#1 Ensemble - nlnet - - 86.0 91.7\\n#2 Ensemble - QANet - - 84.5 90.5\\nPublished\\nBiDAF+ELMo (Single) - 85.6 - 85.8\\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\\nOurs\\nBERTBASE (Single) 80.8 88.5 - -\\nBERTLARGE (Single) 84.1 90.9 - -\\nBERTLARGE (Ensemble) 85.8 91.8 - -\\nBERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2: SQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and Ô¨Åne-tuning seeds.\\nSystem Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman 86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\\n#2 Single - nlnet - - 74.2 77.1\\nPublished\\nunet (Ensemble) - - 71.4 74.9\\nSLQA+ (Single) - 71.4 74.4\\nOurs\\nBERTLARGE (Single) 78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-\\nforming all existing systems by a wide margin.12\\n4.3 SQuAD v2.0\\nThe SQuAD 2.0 task extends the SQuAD 1.1\\nproblem deÔ¨Ånition by allowing for the possibility\\nthat no short answer exists in the provided para-\\ngraph, making the problem more realistic.\\nWe use a simple approach to extend the SQuAD\\nv1.1 BERT model for this task. We treat ques-\\ntions that do not have an answer as having an an-\\nswer span with start and end at the [CLS] to-\\nken. The probability space for the start and end\\nanswer span positions is extended to include the\\nposition of the [CLS] token. For prediction, we\\ncompare the score of the no-answer span: snull =\\nS¬∑C+ E¬∑C to the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents,\\nthat contain at least one of the provided possible answers.\\nSystem Dev Test\\nESIM+GloVe 51.9 52.7\\nESIM+ELMo 59.1 59.2\\nOpenAI GPT - 78.0\\nBERTBASE 81.6 -\\nBERTLARGE 86.6 86.3\\nHuman (expert)‚Ä† - 85.0\\nHuman (5 annotations)‚Ä† - 88.0\\nTable 4: SW AG Dev and Test accuracies.‚Ä†Human per-\\nformance is measured with 100 samples, as reported in\\nthe SW AG paper.\\nÀÜsi,j = maxj‚â•iS¬∑Ti + E¬∑Tj. We predict a non-null\\nanswer when ÀÜsi,j > snull + œÑ, where the thresh-\\nold œÑ is selected on the dev set to maximize F1.\\nWe did not use TriviaQA data for this model. We\\nÔ¨Åne-tuned for 2 epochs with a learning rate of 5e-5\\nand a batch size of 48.\\nThe results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-\\nponents. We observe a +5.1 F1 improvement over\\nthe previous best system.\\n4.4 SWAG\\nThe Situations With Adversarial Generations\\n(SW AG) dataset contains 113k sentence-pair com-\\npletion examples that evaluate grounded common-\\nsense inference (Zellers et al., 2018). Given a sen-\\ntence, the task is to choose the most plausible con-\\ntinuation among four choices.\\nWhen Ô¨Åne-tuning on the SW AG dataset, we\\nconstruct four input sequences, each containing\\nthe concatenation of the given sentence (sentence\\nA) and a possible continuation (sentence B). The\\nonly task-speciÔ¨Åc parameters introduced is a vec-\\ntor whose dot product with the [CLS] token rep-\\nresentation C denotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe Ô¨Åne-tune the model for 3 epochs with a\\nlearning rate of 2e-5 and a batch size of 16. Re-\\nsults are presented in Table 4. BERT LARGE out-\\nperforms the authors‚Äô baseline ESIM+ELMo sys-\\ntem by +27.1% and OpenAI GPT by 8.3%.\\n5 Ablation Studies\\nIn this section, we perform ablation experiments\\nover a number of facets of BERT in order to better\\nunderstand their relative importance. Additional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Dev Set\\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\\n(Acc) (Acc) (Acc) (Acc) (F1)\\nBERTBASE 84.4 88.4 86.7 92.7 88.5\\nNo NSP 83.9 84.9 86.5 92.6 87.9\\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\\nTable 5: Ablation over the pre-training tasks using the\\nBERTBASE architecture. ‚ÄúNo NSP‚Äù is trained without\\nthe next sentence prediction task. ‚ÄúLTR & No NSP‚Äù is\\ntrained as a left-to-right LM without the next sentence\\nprediction, like OpenAI GPT. ‚Äú+ BiLSTM‚Äù adds a ran-\\ndomly initialized BiLSTM on top of the ‚ÄúLTR + No\\nNSP‚Äù model during Ô¨Åne-tuning.\\nablation studies can be found in Appendix C.\\n5.1 Effect of Pre-training Tasks\\nWe demonstrate the importance of the deep bidi-\\nrectionality of BERT by evaluating two pre-\\ntraining objectives using exactly the same pre-\\ntraining data, Ô¨Åne-tuning scheme, and hyperpa-\\nrameters as BERTBASE :\\nNo NSP: A bidirectional model which is trained\\nusing the ‚Äúmasked LM‚Äù (MLM) but without the\\n‚Äúnext sentence prediction‚Äù (NSP) task.\\nLTR & No NSP: A left-context-only model which\\nis trained using a standard Left-to-Right (LTR)\\nLM, rather than an MLM. The left-only constraint\\nwas also applied at Ô¨Åne-tuning, because removing\\nit introduced a pre-train/Ô¨Åne-tune mismatch that\\ndegraded downstream performance. Additionally,\\nthis model was pre-trained without the NSP task.\\nThis is directly comparable to OpenAI GPT, but\\nusing our larger training dataset, our input repre-\\nsentation, and our Ô¨Åne-tuning scheme.\\nWe Ô¨Årst examine the impact brought by the NSP\\ntask. In Table 5, we show that removing NSP\\nhurts performance signiÔ¨Åcantly on QNLI, MNLI,\\nand SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR\\nmodel performs worse than the MLM model on all\\ntasks, with large drops on MRPC and SQuAD.\\nFor SQuAD it is intuitively clear that a LTR\\nmodel will perform poorly at token predictions,\\nsince the token-level hidden states have no right-\\nside context. In order to make a good faith at-\\ntempt at strengthening the LTR system, we added\\na randomly initialized BiLSTM on top. This does\\nsigniÔ¨Åcantly improve results on SQuAD, but the\\nresults are still far worse than those of the pre-\\ntrained bidirectional models. The BiLSTM hurts\\nperformance on the GLUE tasks.\\nWe recognize that it would also be possible to\\ntrain separate LTR and RTL models and represent\\neach token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL\\nmodel would not be able to condition the answer\\non the question; (c) this it is strictly less powerful\\nthan a deep bidirectional model, since it can use\\nboth left and right context at every layer.\\n5.2 Effect of Model Size\\nIn this section, we explore the effect of model size\\non Ô¨Åne-tuning task accuracy. We trained a number\\nof BERT models with a differing number of layers,\\nhidden units, and attention heads, while otherwise\\nusing the same hyperparameters and training pro-\\ncedure as described previously.\\nResults on selected GLUE tasks are shown in\\nTable 6. In this table, we report the average Dev\\nSet accuracy from 5 random restarts of Ô¨Åne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-\\ning examples, and is substantially different from\\nthe pre-training tasks. It is also perhaps surpris-\\ning that we are able to achieve such signiÔ¨Åcant\\nimprovements on top of models which are al-\\nready quite large relative to the existing literature.\\nFor example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018). By contrast, BERT BASE\\ncontains 110M parameters and BERT LARGE con-\\ntains 340M parameters.\\nIt has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated\\nby the LM perplexity of held-out training data\\nshown in Table 6. However, we believe that\\nthis is the Ô¨Årst work to demonstrate convinc-\\ningly that scaling to extreme model sizes also\\nleads to large improvements on very small scale\\ntasks, provided that the model has been sufÔ¨Å-\\nciently pre-trained. Peters et al. (2018b) presented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='mixed results on the downstream task impact of\\nincreasing the pre-trained bi-LM size from two\\nto four layers and Melamud et al. (2016) men-\\ntioned in passing that increasing hidden dimen-\\nsion size from 200 to 600 helped, but increasing\\nfurther to 1,000 did not bring further improve-\\nments. Both of these prior works used a feature-\\nbased approach ‚Äî we hypothesize that when the\\nmodel is Ô¨Åne-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspeciÔ¨Åc models can beneÔ¨Åt from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3 Feature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe Ô¨Åne-tuning approach, where a simple classiÔ¨Å-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly Ô¨Åne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere Ô¨Åxed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-speciÔ¨Åc model architecture to be added.\\nSecond, there are major computational beneÔ¨Åts\\nto pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). In the input to BERT, we\\nuse a case-preserving WordPiece model, and we\\ninclude the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams Dev Set Accuracy\\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\\n3 768 12 5.84 77.9 79.8 88.4\\n6 768 3 5.24 80.6 82.2 90.7\\n6 768 12 4.68 81.9 84.8 91.3\\n12 768 12 3.99 84.4 86.7 92.9\\n12 1024 16 3.54 85.7 86.9 93.3\\n24 1024 16 3.23 86.6 87.8 93.7\\nTable 6: Ablation over BERT model size. #L = the\\nnumber of layers; #H = hidden size; #A = number of at-\\ntention heads. ‚ÄúLM (ppl)‚Äù is the masked LM perplexity\\nof held-out training data.\\nSystem Dev F1 Test F1\\nELMo (Peters et al., 2018a) 95.7 92.2\\nCVT (Clark et al., 2018) - 92.6\\nCSE (Akbik et al., 2018) - 93.1\\nFine-tuning approach\\nBERTLARGE 96.6 92.8\\nBERTBASE 96.4 92.4\\nFeature-based approach (BERTBASE )\\nEmbeddings 91.0 -\\nSecond-to-Last Hidden 95.6 -\\nLast Hidden 94.9 -\\nWeighted Sum Last Four Hidden 95.9 -\\nConcat Last Four Hidden 96.1 -\\nWeighted Sum All 12 Layers 95.5 -\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\nlayer in the output. We use the representation of\\nthe Ô¨Årst sub-token as the input to the token-level\\nclassiÔ¨Åer over the NER label set.\\nTo ablate the Ô¨Åne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without Ô¨Åne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classiÔ¨Åcation layer.\\nResults are presented in Table 7. BERT LARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind Ô¨Åne-tuning the entire model. This\\ndemonstrates that BERT is effective for both Ô¨Åne-\\ntuning and feature-based approaches.\\n6 Conclusion\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to beneÔ¨Åt from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these Ô¨Åndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='References\\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\\n2018. Contextual string embeddings for sequence\\nlabeling. In Proceedings of the 27th International\\nConference on Computational Linguistics , pages\\n1638‚Äì1649.\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817‚Äì1853.\\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe Ô¨Åfth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120‚Äì128. Association for Computa-\\ntional Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural language.\\nComputational linguistics, 18(4):467‚Äì479.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\ncrosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017) , pages 1‚Äì14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\nQuora question pairs.\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nning, and Quoc Le. 2018. Semi-supervised se-\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 1914‚Äì\\n1925.\\nRonan Collobert and Jason Weston. 2008. A uniÔ¨Åed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning. In Pro-\\nceedings of the 25th international conference on\\nMachine learning, pages 160‚Äì167. ACM.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¬®ƒ±c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670‚Äì680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079‚Äì3087.\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09.\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via Ô¨Ålling in\\nthe . arXiv preprint arXiv:1801.07736.\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the 2016\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\\nACL. Association for Computational Linguistics.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nFuru Wei, and Ming Zhou. 2018. Reinforced\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors. In\\nAdvances in neural information processing systems,\\npages 3294‚Äì3302.\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning , pages\\n1188‚Äì1196.\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge. In\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nefÔ¨Åcient framework for learning sentence represen-\\ntations. In International Conference on Learning\\nRepresentations.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26 , pages 3111‚Äì3119. Curran Associates,\\nInc.\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\\nable hierarchical distributed language model. In\\nD. Koller, D. Schuurmans, Y . Bengio, and L. Bot-\\ntou, editors, Advances in Neural Information Pro-\\ncessing Systems 21 , pages 1081‚Äì1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In EMNLP.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 1532‚Äì\\n1543.\\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nIn ACL.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018a. Deep contextualized word rep-\\nresentations. In NAACL.\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\\nand Wen-tau Yih. 2018b. Dissecting contextual\\nword embeddings: Architecture and representation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n1499‚Äì1509.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018. Improving language under-\\nstanding with unsupervised learning. Technical re-\\nport, OpenAI.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 2383‚Äì2392.\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nÔ¨Çow for machine comprehension. In ICLR.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of the 2013 conference on\\nempirical methods in natural language processing ,\\npages 1631‚Äì1642.\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\\n2018. U-net: Machine reading comprehension\\nwith unanswerable questions. arXiv preprint\\narXiv:1810.06638.\\nWilson L Taylor. 1953. Cloze procedure: A new\\ntool for measuring readability. Journalism Bulletin,\\n30(4):415‚Äì433.\\nErik F Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In\\nCoNLL.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\\nWord representations: A simple and general method\\nfor semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics, ACL ‚Äô10, pages 384‚Äì394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems, pages 6000‚Äì6010.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\\nPierre-Antoine Manzagol. 2008. Extracting and\\ncomposing robust features with denoising autoen-\\ncoders. In Proceedings of the 25th international\\nconference on Machine learning, pages 1096‚Äì1103.\\nACM.\\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\\nGlue: A multi-task benchmark and analysis platform'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='for natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: An-\\nalyzing and Interpreting Neural Networks for NLP ,\\npages 353‚Äì355.\\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\\ngranularity hierarchical attention fusion networks\\nfor reading comprehension and question answering.\\nIn Proceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers). Association for Computational Lin-\\nguistics.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\\nman. 2018. Neural network acceptability judg-\\nments. arXiv preprint arXiv:1805.12471.\\nAdina Williams, Nikita Nangia, and Samuel R Bow-\\nman. 2018. A broad-coverage challenge corpus\\nfor sentence understanding through inference. In\\nNAACL.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144.\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\\nLipson. 2014. How transferable are features in deep\\nneural networks? In Advances in neural information\\nprocessing systems, pages 3320‚Äì3328.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\\nLe. 2018. QANet: Combining local convolution\\nwith global self-attention for reading comprehen-\\nsion. In ICLR.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\\nChoi. 2018. Swag: A large-scale adversarial dataset\\nfor grounded commonsense inference. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP).\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. 2015. Aligning books and movies: Towards\\nstory-like visual explanations by watching movies\\nand reading books. In Proceedings of the IEEE\\ninternational conference on computer vision , pages\\n19‚Äì27.\\nAppendix for ‚ÄúBERT: Pre-training of\\nDeep Bidirectional Transformers for\\nLanguage Understanding‚Äù\\nWe organize the appendix into three sections:\\n‚Ä¢ Additional implementation details for BERT\\nare presented in Appendix A;\\n‚Ä¢ Additional details for our experiments are\\npresented in Appendix B; and\\n‚Ä¢ Additional ablation studies are presented in\\nAppendix C.\\nWe present additional ablation studies for\\nBERT including:\\n‚Äì Effect of Number of Training Steps; and\\n‚Äì Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks\\nWe provide examples of the pre-training tasks in\\nthe following.\\nMasked LM and the Masking ProcedureAs-\\nsuming the unlabeled sentence is my dog is\\nhairy, and during the random masking procedure\\nwe chose the 4-th token (which corresponding to\\nhairy), our masking procedure can be further il-\\nlustrated by\\n‚Ä¢ 80% of the time: Replace the word with the\\n[MASK] token, e.g., my dog is hairy ‚Üí\\nmy dog is [MASK]\\n‚Ä¢ 10% of the time: Replace the word with a\\nrandom word, e.g., my dog is hairy ‚Üí my\\ndog is apple\\n‚Ä¢ 10% of the time: Keep the word un-\\nchanged, e.g., my dog is hairy ‚Üí my dog\\nis hairy. The purpose of this is to bias the\\nrepresentation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-\\nplaced by random words, so it is forced to keep\\na distributional contextual representation of ev-\\nery input token. Additionally, because random\\nreplacement only occurs for 1.5% of all tokens\\n(i.e., 10% of 15%), this does not seem to harm\\nthe model‚Äôs language understanding capability. In\\nSection C.2, we evaluate the impact this proce-\\ndure.\\nCompared to standard langauge model training,\\nthe masked LM only make predictions on 15% of\\ntokens in each batch, which suggests that more\\npre-training steps may be required for the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT (Ours)\\nTrm Trm Trm\\nTrm Trm Trm\\n...\\n...\\nTrm Trm Trm\\nTrm Trm Trm\\n...\\n...\\nOpenAI GPT\\nLstm\\nELMo\\nLstm Lstm\\nLstm Lstm Lstm\\nLstm Lstm Lstm\\nLstm Lstm Lstm\\n T1 T2  TN...\\n...\\n...\\n...\\n...\\n E1 E2  EN...\\n T1 T2 TN...\\n E1 E2  EN...\\n T1 T2  TN...\\n E1 E2  EN...\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\nOpenAI GPT are Ô¨Åne-tuning approaches, while ELMo is a feature-based approach.\\nto converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model\\nfar outweigh the increased training cost.\\nNext Sentence Prediction The next sentence\\nprediction task can be illustrated in the following\\nexamples.\\nInput = [CLS] the man went to [MASK] store [SEP]\\nhe bought a gallon [MASK] milk [SEP]\\nLabel = IsNext\\nInput = [CLS] the man [MASK] to the store [SEP]\\npenguin [MASK] are flight ##less birds [SEP]\\nLabel = NotNext\\nA.2 Pre-training Procedure\\nTo generate each training input sequence, we sam-\\nple two spans of text from the corpus, which we\\nrefer to as ‚Äúsentences‚Äù even though they are typ-\\nically much longer than single sentences (but can\\nbe shorter also). The Ô¨Årst sentence receives the A\\nembedding and the second receives the B embed-\\nding. 50% of the time B is the actual next sentence\\nthat follows A and 50% of the time it is a random\\nsentence, which is done for the ‚Äúnext sentence pre-\\ndiction‚Äù task. They are sampled such that the com-\\nbined length is ‚â§512 tokens. The LM masking is\\napplied after WordPiece tokenization with a uni-\\nform masking rate of 15%, and no special consid-\\neration given to partial word pieces.\\nWe train with batch size of 256 sequences (256\\nsequences * 512 tokens = 128,000 tokens/batch)\\nfor 1,000,000 steps, which is approximately 40\\nepochs over the 3.3 billion word corpus. We\\nuse Adam with learning rate of 1e-4, Œ≤1 = 0.9,\\nŒ≤2 = 0.999, L2 weight decay of 0.01, learning\\nrate warmup over the Ô¨Årst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERT BASE was performed on 4\\nCloud TPUs in Pod conÔ¨Åguration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre-\\ntraining took 4 days to complete.\\nLonger sequences are disproportionately expen-\\nsive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor Ô¨Åne-tuning, most model hyperparameters are\\nthe same as in pre-training, with the exception of\\nthe batch size, learning rate, and number of train-\\ning epochs. The dropout probability was always\\nkept at 0.1. The optimal hyperparameter values\\nare task-speciÔ¨Åc, but we found the following range\\nof possible values to work well across all tasks:\\n‚Ä¢ Batch size: 16, 32\\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\\nTPU-now-offers-preemptible-pricing-and-global-\\navailability.html'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n‚Ä¢ Number of epochs: 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are Ô¨Åne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n‚Ä¢ GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n‚Ä¢ GPT uses a sentence separator ( [SEP]) and\\nclassiÔ¨Åer token ( [CLS]) which are only in-\\ntroduced at Ô¨Åne-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n‚Ä¢ GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n‚Ä¢ GPT used the same learning rate of 5e-5 for\\nall Ô¨Åne-tuning experiments; BERT chooses a\\ntask-speciÔ¨Åc Ô¨Åne-tuning learning rate which\\nperforms the best on the development set.\\nTo isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of Ô¨Åne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciÔ¨Åc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe Ô¨Ågure, E represents the input embedding, Ti\\nrepresents the contextual representation of tokeni,\\n[CLS] is the special symbol for classiÔ¨Åcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB Detailed Experimental Setup\\nB.1 Detailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur GLUE results in Table1 are obtained\\nfrom https://gluebenchmark.com/\\nleaderboard and https://blog.\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiÔ¨Å-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the Ô¨Årst one.\\nQQP Quora Question Pairs is a binary classiÔ¨Å-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiÔ¨Åcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok \\n1\\n [SEP]... Tok \\nN\\nTok \\n1 ... Tok\\nM\\nQuestion Paragraph\\nBERT\\nE[CLS] E1  E2  EN\\nC\\n T1\\n  T2\\n  TN\\nSingle Sentence \\n...\\n...\\nBERT\\nTok 1  Tok 2  Tok N...[CLS]\\nE[CLS] E1  E2  EN\\nC\\n T1\\n  T2\\n  TN\\nSingle Sentence \\nB-PERO O\\n...\\n...E[CLS] E1  E[SEP]\\nClass \\nLabel\\n... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\nStart/End Span\\nClass \\nLabel\\nBERT\\nTok 1  Tok 2  Tok N...[CLS] Tok 1[CLS][CLS] Tok \\n1\\n [SEP]... Tok \\nN\\nTok \\n1 ... Tok\\nM\\nSentence 1\\n...\\nSentence 2\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classiÔ¨Åcation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classiÔ¨Åcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically ‚Äúacceptable‚Äù or not (Warstadt\\net al., 2018).\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that‚Äôs been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n14Note that we only report single-task Ô¨Åne-tuning results\\nin this paper. A multitask Ô¨Åne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n15https://gluebenchmark.com/faq'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='jority class.\\nC Additional Ablation Studies\\nC.1 Effect of Number of Training Steps\\nFigure 5 presents MNLI Dev accuracy after Ô¨Åne-\\ntuning from a checkpoint that has been pre-trained\\nfor ksteps. This allows us to answer the following\\nquestions:\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh Ô¨Åne-tuning accuracy?\\nAnswer: Yes, BERT BASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n200 400 600 800 1,000\\n76\\n78\\n80\\n82\\n84\\nPre-training Steps (Thousands)\\nMNLI Dev Accuracy\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after Ô¨Åne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nksteps. The x-axis is the value of k.\\nNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand Ô¨Åne-tuning, as the [MASK] symbol never ap-\\npears during the Ô¨Åne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both Ô¨Åne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npliÔ¨Åed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\nMasking Rates Dev Set Results\\nMASK SAME RND MNLI NER\\nFine-tune Fine-tune Feature-based\\n80% 10% 10% 84.2 95.4 94.9\\n100% 0% 0% 84.3 94.9 94.0\\n80% 0% 20% 84.1 95.2 94.6\\n80% 20% 0% 84.4 95.2 94.7\\n0% 20% 80% 83.7 94.8 94.6\\n0% 0% 100% 83.6 94.9 94.6\\nTable 8: Ablation over different masking strategies.\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; R ND means that\\nwe replace the target token with another random\\ntoken.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciÔ¨Åc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\nFrom the table it can be seen that Ô¨Åne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe R ND strategy performs much worse than our\\nstrategy as well.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='DeepSeekMath: Pushing the Limits of Mathematical\\nReasoning in Open Language Models\\nZhihong Shao1,2‚àó‚Ä†, Peiyi Wang1,3‚àó‚Ä†, Qihao Zhu1,3‚àó‚Ä†, Runxin Xu1, Junxiao Song1\\nXiao Bi1, Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1‚àó\\n1DeepSeek-AI, 2Tsinghua University,3Peking University\\n{zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com\\nhttps://github.com/deepseek-ai/DeepSeek-Math\\nAbstract\\nMathematical reasoning poses a significant challenge for language models due to its complex\\nand structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-\\ntraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common\\nCrawl, together with natural language and code data. DeepSeekMath 7B has achieved an\\nimpressive score of 51.7% on the competition-level MATH benchmark without relying on\\nexternal toolkits and voting techniques, approaching the performance level of Gemini-Ultra\\nand GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,\\nwe harness the significant potential of publicly available web data through a meticulously\\nengineered data selection pipeline. Second, we introduce Group Relative Policy Optimization\\n(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning\\nabilities while concurrently optimizing the memory usage of PPO.\\nFigure 1 |Top1 accuracy of open-source models on the competition-level MATH benchmark\\n(Hendrycks et al., 2021) without the use of external toolkits and voting techniques.\\n‚àóCore contributors.\\n‚Ä†Work done during internship at DeepSeek-AI.\\narXiv:2402.03300v3  [cs.CL]  27 Apr 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='1. Introduction\\nLarge language models (LLM) have revolutionized the approach to mathematical reasoning\\nin artificial intelligence, spurring significant advancements in both the quantitative reasoning\\nbenchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).\\nMoreover, these models have proven instrumental in assisting humans in solving complex\\nmathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,\\n2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible\\nopen-source models considerably trail behind in performance.\\nIn this study, we introduce DeepSeekMath, a domain-specific language model that signifi-\\ncantly outperforms the mathematical capabilities of open-source models and approaches the\\nperformance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-\\nMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This\\ndataset is extracted from the Common Crawl (CC) using a fastText-based classifier (Joulin et al.,\\n2016). In the initial iteration, the classifier is trained using instances from OpenWebMath (Paster\\net al., 2023) as positive examples, while incorporating a diverse selection of other web pages to\\nserve as negative examples. Subsequently, we employ the classifier to mine additional positive\\ninstances from the CC, which are further refined through human annotation. The classifier is\\nthen updated with this enhanced dataset to improve its performance. The evaluation results\\nindicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base\\n7B achieves 64.2% on GSM8K (Cobbe et al., 2021) and 36.2% on the competition-level MATH\\ndataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In\\naddition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese\\nmathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience\\nin mathematical data processing is a starting point for the research community, and there is\\nsignificant room for improvement in the future.\\nDeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as\\nwe notice that starting from a code training model is a better choice compared to a general\\nLLM. Furthermore, we observe the math training also improves model capability on MMLU\\n(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only\\nenhance the model‚Äôs mathematical abilities but also amplifies general reasoning capabilities.\\nAfter pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with\\nchain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and\\ntool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct\\n7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.\\nFurthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant rein-\\nforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).\\nGRPO foregoes the critic model, instead estimating the baseline from group scores, significantly\\nreducing training resources. By solely using a subset of English instruction tuning data, GRPO\\nobtains a substantial improvement over the strong DeepSeekMath-Instruct, including both\\nin-domain (GSM8K: 82.9% ‚Üí88.2%, MATH: 46.8% ‚Üí51.7%) and out-of-domain mathematical\\ntasks (e.g., CMATH: 84.6% ‚Üí88.8%) during the reinforcement learning phase. We also provide\\na unified paradigm to understand different methods, such as Rejection Sampling Fine-Tuning\\n(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and\\nGRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as\\neither direct or simplified RL techniques. We also conduct extensive experiments, e.g., online\\nv.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='to deeply investigate the essential elements of this paradigm. At last, we explain why our RL\\nboosts the performance of instruction-tuned models, and further summarize potential directions\\nto achieve more effective RL based on this unified paradigm.\\n1.1. Contributions\\nOur contribution includes scalable math pre-training, along with the exploration and analysis of\\nreinforcement learning.\\nMath Pre-Training at Scale\\n‚Ä¢ Our research provides compelling evidence that the publicly accessible Common Crawl\\ndata contains valuable information for mathematical purposes. By implementing a metic-\\nulously designed data selection pipeline, we successfully construct the DeepSeekMath\\nCorpus, a high-quality dataset of 120B tokens from web pages filtered for mathemati-\\ncal content, which is almost 7 times the size of the math web pages used by Minerva\\n(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath\\n(Paster et al., 2023).\\n‚Ä¢ Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance\\nwith Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not\\nthe only key factor in mathematical reasoning capability. A smaller model pre-trained on\\nhigh-quality data could achieve strong performance as well.\\n‚Ä¢ We share our findings from math training experiments. Code training prior to math\\ntraining improves models‚Äô ability to solve mathematical problems both with and without\\ntool use. This offers a partial answer to the long-standing question: does code training\\nimprove reasoning abilities?We believe it does, at least for mathematical reasoning.\\n‚Ä¢ Although training on arXiv papers is common, especially in many math-related papers, it\\nbrings no notable improvements on all mathematical benchmarks adopted in this paper.\\nExploration and Analysis of Reinforcement Learning\\n‚Ä¢ We introduce Group Relative Policy Optimization (GRPO), an efficient and effective\\nreinforcement learning algorithm. GRPO foregoes the critic model, instead estimating\\nthe baseline from group scores, significantly reducing training resources compared to\\nProximal Policy Optimization (PPO).\\n‚Ä¢ We demonstrate that GRPO significantly enhances the performance of our instruction-\\ntuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Further-\\nmore, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,\\nPPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training,\\noutcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so\\non to deeply investigate the essential elements of this paradigm.\\n‚Ä¢ Based on our unified paradigm, we explore the reasons behind the effectiveness of rein-\\nforcement learning, and summarize several potential directions to achieve more effective\\nreinforcement learning of LLMs.\\n1.2. Summary of Evaluations and Metrics\\n‚Ä¢ English and Chinese Mathematical Reasoning: We conduct comprehensive assessments\\nof our models on English and Chinese benchmarks, covering mathematical problems\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='from grade-school level to college level. English benchmarks include GSM8K (Cobbe\\net al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses\\n(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks\\ninclude MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong\\net al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate models‚Äô ability\\nto generate self-contained text solutions without tool use, and also the ability to solve\\nproblems using Python.\\nOn English benchmarks, DeepSeekMath-Base is competitive with the closed-source Min-\\nerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mis-\\ntral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether\\nthey‚Äôve undergone math pre-training or not, often by a significant margin. Notably,\\nDeepSeekMath-Base is superior on Chinese benchmarks, likely because we don‚Äôt follow\\nprevious works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only\\nmath pre-training data, and also include high-quality non-English ones. With mathemati-\\ncal instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct\\nand DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over\\n50% on the competition-level MATH dataset for the first time within the open-source\\ncommunity.\\n‚Ä¢ Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal\\ntheorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle\\n(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates\\nstrong few-shot autoformalization performance.\\n‚Ä¢ Natural Language Understanding, Reasoning, and Code : To build a comprehensive\\nprofile of models‚Äô general understanding, reasoning, and coding capabilities, we eval-\\nuate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)\\nbenchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering\\ndiverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 chal-\\nlenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval\\n(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code\\nlanguage models. Math pre-training benefits both language understanding and reasoning\\nperformance.\\n2. Math Pre-Training\\n2.1. Data Collection and Decontamination\\nIn this section, we will outline the process of constructing the DeepSeekMath Corpus from\\nCommon Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates\\nhow to systematically gather a large-scale mathematical corpus from Common Crawl, starting\\nwith a seed corpus (e.g., a small but high-quality collection of math-related dataset). It‚Äôs worth\\nnoting that this approach is also applicable to other domains, such as coding.\\nFirst, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical\\nweb texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,\\n2016) to recall more OpenWebMath-like mathematical web pages. Specifically, we randomly\\nselect 500,000 data points from the seed corpus as positive training examples and another\\n500,000 web pages from Common Crawl as negative ones. We employ an open-source library1\\nfor training, configuring the vector dimension to 256, learning rate to 0.1, the maximum length\\n1https://fasttext.cc\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Math Seed\\n Math Corpus\\n1. Train a FastTextModel 2. Recall Math-Related Webpages From Common Crawl\\n3. Discover Math-Related Domains4. Annotate Math-Related URL Path From Labelers\\nDeduplicated Common Crawl40B HTML pages\\nFigure 2 |An iterative pipeline that collects mathematical web pages from Common Crawl.\\nof word n-gram to 3, the minimum number of word occurrences to 3, and the number of\\ntraining epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based\\ndeduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then\\nrecall mathematical web pages from deduplicated Common Crawl with the fastText model.\\nTo filter out low-quality mathematical content, we rank the collected pages according to their\\nscores predicted by the fastText model, and only preserve the top-ranking ones. The volume\\nof data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and\\n160B tokens. In the first iteration, we choose to keep the top 40B tokens.\\nAfter the first iteration of data collection, numerous mathematical web pages remain un-\\ncollected, mainly because the fastText model is trained on a set of positive examples that lacks\\nsufficient diversity. We therefore identify additional mathematical web sources to enrich the seed\\ncorpus, so that we can optimize the fastText model. Specifically, we first organize the entire Com-\\nmon Crawl into disjoint domains; a domain is defined as web pages sharing the same base URL.\\nFor each domain, we calculate the percentage of web pages that are collected in the first iteration.\\nDomains where over 10% of the web pages have been collected are classified as math-related\\n(e.g., mathoverflow.net ). Subsequently, we manually annotate the URLs associated with\\nmathematical content within these identified domains (e.g., mathoverflow.net/questions).\\nWeb pages linked to these URLs, yet uncollected, will be added to the seed corpus. This ap-\\nproach enables us to gather more positive examples, thereby training an improved fastText\\nmodel capable of recalling more mathematical data in the subsequent iteration. After four\\niterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B\\ntokens. In the fourth iteration, we notice that nearly 98% of the data has already been collected\\nin the third iteration, so we decide to cease data collection.\\nTo avoid benchmark contamination, we follow Guo et al. (2024) to filter out web pages\\ncontaining questions or answers from English mathematical benchmarks such as GSM8K (Cobbe\\net al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH\\n(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The filtering criteria are as follows: any\\ntext segment containing a 10-gram string that matches exactly with any sub-string from the\\nevaluation benchmarks is removed from our math training corpus. For benchmark texts that\\nare shorter than 10 grams but have at least 3 grams, we employ exact matching to filter out\\ncontaminated web pages.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2.2. Validating the Quality of the DeepSeekMath Corpus\\nWe run pre-training experiments to investigate how the DeepSeekMath Corpus is compared\\nwith the recently released math-training corpora:\\n‚Ä¢ MathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from\\ntextbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the\\nmajority (over 85%) sourced from arXiv;\\n‚Ä¢ OpenWebMath (Paster et al., 2023): CommonCrawl data filtered for mathematical content,\\ntotaling 13.6B tokens;\\n‚Ä¢ Proof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWeb-\\nMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B to-\\nkens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an\\narXiv:Web:Code ratio of 2:4:1.\\n2.2.1. Training Setting\\nWe apply math training to a general pre-trained language model with 1.3B parameters, which\\nshares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeek-\\nLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All\\nexperiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023)\\ntraining framework. Following the training practice of DeepSeek LLMs, we use the AdamW\\noptimizer (Loshchilov and Hutter, 2017) with ùõΩ1 = 0.9, ùõΩ2 = 0.95, and weight_decay = 0.1, along\\nwith a multi-step learning rate schedule where the learning rate reaches the peak after 2,000\\nwarmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to\\n10.0% of the peak after 90% of the training process. We set the maximum value of learning rate\\nto 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\\nMath Corpus Size\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SAT MMLU\\nSTEM CMATH Gaokao\\nMathCloze\\nGaokao\\nMathQA\\nNo Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9%\\nMathPile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8%\\nOpenWebMath 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2%\\nProof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7%\\nDeepSeekMath Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6%\\nTable 1 |Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evalu-\\nated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer\\nwith a vocabulary size of 100K.\\n2.2.2. Evaluation Results\\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and\\nis the largest in size.\\n‚Ä¢ High-quality: We evaluate downstream performance on 8 mathematical benchmarks using\\nfew-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear\\nperformance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that\\nthe model trained on the DeepSeekMath Corpus demonstrates better performance than\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Figure 3 |Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.\\nProof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of\\nDeepSeekMath Corpus is higher.\\n‚Ä¢ Multilingual: The DeepSeekMath Corpus encompasses data in multiple languages, pre-\\ndominantly featuring English and Chinese as the two most represented languages. As\\nshown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning\\nperformance in both English and Chinese. In contrast, existing mathematical corpora,\\nwhich are primarily English-centric, show limited improvement and may even hinder\\nperformance in Chinese mathematical reasoning.\\n‚Ä¢ Large-scale: The DeepSeekMath Corpus is several times larger than existing mathematical\\ncorpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-\\nMath Corpus, shows a steeper learning curve along with more lasting improvements. In\\ncontrast, the baseline corpora are much smaller, and have already been repeated multiple\\nrounds during training, with the resulting model performance quickly reaching a plateau.\\n2.3. Training and Evaluating DeepSeekMath-Base 7B\\nIn this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning\\nabilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: 56%\\nis from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github\\ncode, and the remaining 10% is natural language data from Common Crawl in both English and\\nChinese. We mainly adopt the training setting specified in Section 2.2.1, except that we set the\\nmaximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.\\nWe conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-\\nBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying\\non external tools, solve mathematical problems using tools, and conduct formal theorem proving.\\nBeyond mathematics, we also provide a more general profile of the base model, including its\\nperformance of natural language understanding, reasoning, and programming skills.\\nMathematical Problem Solving with Step-by-Step Reasoning We evaluate DeepSeekMath-\\nBase‚Äôs performance of solving mathematical problems using few-shot chain-of-thought prompt-\\ning (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encom-\\npass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),\\nand CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks\\net al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse fields of mathematics\\nfrom elementary to college-level complexity.\\nAs shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight bench-\\nmarks among the open-source base models (including the widely-used general model Mistral\\n7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which\\nunderwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition-\\nlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over\\n10% absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base\\nmodel 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained\\non mathematical texts.\\nModel Size\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SAT MMLU\\nSTEM CMATH Gaokao\\nMathCloze\\nGaokao\\nMathQA\\nClosed-Source Base Model\\nMinerva 7B 16.2% 14.1% 7.7% - 35.6% - - -\\nMinerva 62B 52.4% 27.6% 12.0% - 53.9% - - -\\nMinerva 540B 58.8% 33.6% 17.6% - 63.9% - - -\\nOpen-Source Base Model\\nMistral 7B 40.3% 14.3% 9.2% 71.9% 51.1% 44.9% 5.1% 23.4%\\nLlemma 7B 37.4% 18.1% 6.3% 59.4% 43.1% 43.4% 11.9% 23.6%\\nLlemma 34B 54.0% 25.3% 10.3% 71.9% 52.9% 56.1% 11.9% 26.2%\\nDeepSeekMath-Base 7B 64.2% 36.2% 15.4% 84.4% 56.5% 71.7% 20.3% 35.3%\\nTable 2 |Comparisons between DeepSeekMath-Base 7B and strong base models on English and\\nChinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.\\nMinerva results are quoted from Lewkowycz et al. (2022a).\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Mathematical Problem Solving with Tool Use We evaluate program-aided mathematical\\nreasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,\\n2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program\\nwhere libraries such as math and sympy can be utilized for intricate computations. The execution\\nresult of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B\\noutperforms the prior state-of-the-art Llemma 34B.\\nModel Size Problem Solving w/ Tools Informal-to-Formal Proving\\nGSM8K+Python MATH+Python miniF2F-valid miniF2F-test\\nMistral 7B 48.5% 18.2% 18.9% 18.0%\\nCodeLlama 7B 27.1% 17.2% 16.3% 17.6%\\nCodeLlama 34B 52.7% 23.5% 18.5% 18.0%\\nLlemma 7B 41.0% 18.6% 20.6% 22.1%\\nLlemma 34B 64.6% 26.3% 21.0% 21.3%\\nDeepSeekMath-Base 7B 66.9% 31.4% 25.8% 24.6%\\nTable 3 |Few-shot evaluation of base models‚Äô ability to solve mathematical problems using tools\\nand the ability to conduct informal-to-formal theorem proving in Isabelle.\\nFormal Mathematics Formal proof automation is beneficial to ensure the accuracy and relia-\\nbility of mathematical proofs and enhance efficiency, with increasing attention in recent years.\\nWe evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,\\n2022) which is to generate a formal proof based on an informal statement, a formal counterpart\\nof the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a bench-\\nmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each\\nproblem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate\\nproof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)\\nto fill in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strong\\nperformance in proof autoformalization.\\nModel Size MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)\\nMistral 7B 62.4% 55.7% 28.0% 41.4%\\nDeepSeek-Coder-Base-v1.5‚Ä† 7B 42.9% 42.9% 40.2% 52.6%\\nDeepSeek-Coder-Base-v1.5 7B 49.1% 55.2% 43.2% 60.4%\\nDeepSeekMath-Base 7B 54.9% 59.5% 40.9% 52.6%\\nTable 4 |Evaluation on natural language understanding, reasoning, and code benchmarks.\\nDeepSeek-Coder-Base-v1.5‚Ä†is the checkpoint right before learning rate decay, which is used to\\ntrain DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.\\nOn HumanEval and MBPP , we evaluate model performance under the zero-shot setting and a\\nfew-shot setting, respectively.\\nNatural Language Understanding, Reasoning, and Code We evaluate model performance of\\nnatural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun\\net al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits significant enhancements in per-\\nformance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),\\nillustrating the positive impact of math training on language understanding and reasoning.\\nAdditionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively\\nmaintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Over-\\nall, DeepSeekMath-Base 7B significantly outperforms the general model Mistral 7B (Jiang et al.,\\n2023) on the three reasoning and coding benchmarks.\\n3. Supervised Fine-T uning\\n3.1. SFT Data Curation\\nWe construct a mathematical instruction-tuning dataset covering English and Chinese problems\\nfrom different mathematical fields and of varying complexity levels: problems are paired with\\nsolutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,\\n2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number\\nof training examples is 776K.\\n‚Ä¢ English mathematical datasets: We annotate GSM8K and MATH problems with tool-\\nintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the\\ntraining set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or\\nPoT. Our English collection covers diverse fields of mathematics, e.g., algebra, probability,\\nnumber theory, calculus, and geometry.\\n‚Ä¢ Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning\\n76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-\\nintegrated reasoning format.\\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B\\nIn this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruc-\\ntion tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until\\nreaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch\\nsize of 256 and a constant learning rate of 5e-5.\\nWe evaluate models‚Äô mathematical performance both without and with tool use, on 4\\nquantitative reasoning benchmarks in English and Chinese. We benchmark our model against\\nthe leading models of the time:\\n‚Ä¢ Closed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)\\nand GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil\\net al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1 3, as well as models recently\\nreleased by Chinese companies including (5) Baichuan-3 4, (6) the latest GLM-4 5 from the\\nGLM family (Du et al., 2022). These models are for general purposes, most of which have\\nundergone a series of alignment procedures.\\n‚Ä¢ Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeek-\\nAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)\\n2https://openai.com/blog/chatgpt-plugins#code-interpreter\\n3https://x.ai/model-card\\n4https://www.baichuan-ai.com\\n5https://open.bigmodel.cn/dev/api#glm-4\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathemat-\\nics including (5) InternLM2-Math 20B 6 which builds on InternLM2 and underwent math\\ntraining followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO\\ntraining (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised\\nreward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical\\nreasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,\\na version of instruction tuning that uses AI-evolved instructions) and PPO training with\\ntraining problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,\\n2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH,\\n(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fine-tuned to do tool-integrated\\nmathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B\\ninstruction-tuned on MathInstruct.\\nAs shown in Table 5, under the evaluation setting where tool use is disallowed, DeepSeekMath-\\nInstruct 7B demonstrates strong performance of step-by-step reasoning. Notably, on the\\ncompetition-level MATH dataset, our model surpasses all open-source models and the ma-\\njority of proprietary models (e.g., Inflection-2 and Gemini Pro) by at least 9% absolute. This\\nis true even for models that are substantially larger (e.g., Qwen 72B) or have been specifi-\\ncally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While\\nDeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,\\nit still underperforms GPT-4 and Gemini Ultra.\\nUnder the evaluation setting where models are allowed to integrate natural language rea-\\nsoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches\\nan accuracy of 60% on MATH, surpassing all existing open-source models. On the other bench-\\nmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is\\n10 times larger.\\n4. Reinforcement Learning\\n4.1. Group Relative Policy Optimization\\nReinforcement learning (RL) has been proven to be effective in further improving the mathe-\\nmatical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;\\nWang et al., 2023b). In this section, we introduce our efficient and effective RL algorithm, Group\\nRelative Policy Optimization (GRPO).\\n4.1.1. From PPO to GRPO\\nProximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is\\nwidely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes\\nLLMs by maximizing the following surrogate objective:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉ(ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n, (1)\\nwhere ùúãùúÉ and ùúãùúÉùëúùëôùëë are the current and old policy models, and ùëû, ùëúare questions and outputs\\nsampled from the question dataset and the old policy ùúãùúÉùëúùëôùëë, respectively. ùúÄis a clipping-related\\nhyper-parameter introduced in PPO for stabilizing training. ùê¥ùë° is the advantage, which is\\ncomputed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based\\n6https://github.com/InternLM/InternLM-Math\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Model Size English Benchmarks Chinese Benchmarks\\nGSM8K MATH MGSM-zh CMATH\\nChain-of-Thought Reasoning\\nClosed-Source Model\\nGemini Ultra - 94.4% 53.2% - -\\nGPT-4 - 92.0% 52.9% - 86.0%\\nInflection-2 - 81.4% 34.8% - -\\nGPT-3.5 - 80.8% 34.1% - 73.8%\\nGemini Pro - 86.5% 32.6% - -\\nGrok-1 - 62.9% 23.9% - -\\nBaichuan-3 - 88.2% 49.2% - -\\nGLM-4 - 87.6% 47.9% - -\\nOpen-Source Model\\nInternLM2-Math 20B 82.6% 37.7% - -\\nQwen 72B 78.9% 35.2% - -\\nMath-Shepherd-Mistral 7B 84.1% 33.0% - -\\nWizardMath-v1.1 7B 83.2% 33.0% - -\\nDeepSeek-LLM-Chat 67B 84.1% 32.6% 74.0% 80.3%\\nMetaMath 70B 82.3% 26.6% 66.4% 70.9%\\nSeaLLM-v2 7B 78.2% 27.5% 64.8% -\\nChatGLM3 6B 72.3% 25.7% - -\\nWizardMath-v1.0 70B 81.6% 22.7% 64.8% 65.4%\\nDeepSeekMath-Instruct 7B 82.9% 46.8% 73.2% 84.6%\\nDeepSeekMath-RL 7B 88.2% 51.7% 79.6% 88.8%\\nTool-Integrated Reasoning\\nClosed-Source Model\\nGPT-4 Code Interpreter - 97.0% 69.7% - -\\nOpen-Source Model\\nInternLM2-Math 20B 80.7% 54.3% - -\\nDeepSeek-LLM-Chat 67B 86.7% 51.1% 76.4% 85.4%\\nToRA 34B 80.7% 50.8% 41.2% 53.4%\\nMAmmoTH 70B 76.9% 41.8% - -\\nDeepSeekMath-Instruct 7B 83.7% 57.4% 72.0% 84.3%\\nDeepSeekMath-RL 7B 86.7% 58.8% 78.4% 87.6%\\nTable 5 |Performance of Open- and Closed-Source models with both Chain-of-Thought and\\nTool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority\\nvotes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all open-\\nsource models from 7B to 70B, as well as the majority of closed-source models. Although\\nDeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data\\nof GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëûùëû\\nùëúùëú!\\nùëúùëú\"\\nùëúùëú#\\nùëüùëü!\\nùëüùëü\"\\nùëüùëü#\\nùê¥ùê¥!\\nùê¥ùê¥\"\\nùê¥ùê¥#\\nùëûùëû ùëúùëú GAE ùê¥ùê¥\\nùëüùëü\\nùë£ùë£\\nReward \\nModel\\nPolicy \\nModel\\nValue \\nModel\\n‚Ä¶ ‚Ä¶ ‚Ä¶\\nPolicy \\nModel\\nReference \\nModel\\nReward \\nModel\\nPPO\\nGRPO\\nTrained\\nModels\\nFrozen\\nModelsReference \\nModel\\n‚äï\\nùêæùêæùêæùêæ\\nùêæùêæùêæùêæ\\nGroup \\nComputation\\nFigure 4 |Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead\\nestimating the baseline from group scores, significantly reducing training resources.\\non the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì. Thus, in PPO, a value function needs to\\nbe trained alongside the policy model and to mitigate over-optimization of the reward model,\\nthe standard approach is to add a per-token KL penalty from a reference model in the reward at\\neach token (Ouyang et al., 2022), i.e.,\\nùëüùë° = ùëüùúë(ùëû, ùëú‚â§ùë°)‚àíùõΩlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùëüùëíùëì (ùëúùë°|ùëû, ùëú<ùë°), (2)\\nwhere ùëüùúë is the reward model, ùúãùëüùëíùëì is the reference model, which is usually the initial SFT model,\\nand ùõΩ is the coefficient of the KL penalty.\\nAs the value function employed in PPO is typically another model of comparable size as\\nthe policy model, it brings a substantial memory and computational burden. Additionally,\\nduring RL training, the value function is treated as a baseline in the calculation of the advantage\\nfor variance reduction. While in the LLM context, usually only the last token is assigned a\\nreward score by the reward model, which may complicate the training of a value function that is\\naccurate at each token. To address this, as shown in Figure 4, we propose Group Relative Policy\\nOptimization (GRPO), which obviates the need for additional value function approximation as\\nin PPO, and instead uses the average reward of multiple sampled outputs, produced in response\\nto the same question, as the baseline. More specifically, for each question ùëû, GRPO samples a\\ngroup of outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}from the old policy ùúãùúÉùëúùëôùëë and then optimizes the policy model\\nby maximizing the following objective:\\nJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉ(ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x1a\\nmin\\n\\x14 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nÀÜùê¥ùëñ,ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nÀÜùê¥ùëñ,ùë°\\n\\x15\\n‚àíùõΩDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\x1b\\n,\\n(3)\\nwhere ùúÄ and ùõΩ are hyper-parameters, and ÀÜùê¥ùëñ,ùë° is the advantage calculated based on relative\\nrewards of the outputs inside each group only, which will be detailed in the following subsec-\\ntions. The group relative way that GRPO leverages to calculate the advantages, aligns well with\\nthe comparative nature of rewards models, as reward models are typically trained on datasets\\nof comparisons between outputs on the same question. Also note that, instead of adding KL\\npenalty in the reward, GRPO regularizes by directly adding the KL divergence between the\\ntrained policy and the reference policy to the loss, avoiding complicating the calculation of ÀÜùê¥ùëñ,ùë°.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Algorithm 1 Iterative Group Relative Policy Optimization\\nInput initial policy model ùúãùúÉinit ; reward models ùëüùúë; task prompts D; hyperparameters ùúÄ, ùõΩ, ùúá\\n1: policy model ùúãùúÉ ‚ÜêùúãùúÉinit\\n2: for iteration = 1, . . . , Ido\\n3: reference model ùúãùëüùëíùëì ‚ÜêùúãùúÉ\\n4: for step = 1, . . . , Mdo\\n5: Sample a batch Dùëè from D\\n6: Update the old policy model ùúãùúÉùëúùëôùëë ‚ÜêùúãùúÉ\\n7: Sample ùê∫ outputs {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (¬∑| ùëû)for each question ùëû ‚ààDùëè\\n8: Compute rewards {ùëüùëñ}ùê∫\\nùëñ=1 for each sampled output ùëúùëñ by running ùëüùúë\\n9: Compute ÀÜùê¥ùëñ,ùë° for the ùë°-th token of ùëúùëñ through group relative advantage estimation.\\n10: for GRPO iteration = 1, . . . ,ùúá do\\n11: Update the policy model ùúãùúÉ by maximizing the GRPO objective (Equation 21)\\n12: Update ùëüùúë through continuous training using a replay mechanism.\\nOutput ùúãùúÉ\\nAnd different from the KL penalty term used in (2), we estimate the KL divergence with the\\nfollowing unbiased estimator (Schulman, 2020):\\nDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\n=\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àí1, (4)\\nwhich is guaranteed to be positive.\\n4.1.2. Outcome Supervision RL with GRPO\\nFormally, for each question ùëû, a group of outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}are sampled from the old\\npolicy model ùúãùúÉùëúùëôùëë. A reward model is then used to score the outputs, yielding ùê∫ rewards\\nr = {ùëü1, ùëü2, ¬∑¬∑¬∑ , ùëüùê∫}correspondingly. Subsequently, these rewards are normalized by subtracting\\nthe group average and dividing by the group standard deviation. Outcome supervision provides\\nthe normalized reward at the end of each output ùëúùëñ and sets the advantages ÀÜùê¥ùëñ,ùë° of all tokens in\\nthe output as the normalized reward, i.e., ÀÜùê¥ùëñ,ùë° = eùëüùëñ = ùëüùëñ‚àímean(r)\\nstd(r) , and then optimizes the policy by\\nmaximizing the objective defined in equation (3).\\n4.1.3. Process Supervision RL with GRPO\\nOutcome supervision only provides a reward at the end of each output, which may not be\\nsufficient and efficient to supervise the policy in complex mathematical tasks. Following Wang\\net al. (2023b), we also explore process supervision, which provides a reward at the end of\\neach reasoning step. Formally, given the question ùëûand ùê∫ sampled outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}, a\\nprocess reward model is used to score each step of the outputs, yielding corresponding rewards:\\nR = {{ùëüùëñùëõùëëùëíùë•(1)\\n1 , ¬∑¬∑¬∑ , ùëüùëñùëõùëëùëíùë•(ùêæ1 )\\n1 }, ¬∑¬∑¬∑ , {ùëüùëñùëõùëëùëíùë•(1)\\nùê∫ , ¬∑¬∑¬∑ , ùëüùëñùëõùëëùëíùë•(ùêæùê∫)\\nùê∫ }}, where ùëñùëõùëëùëíùë•(ùëó)is the end token index\\nof the ùëó-th step, and ùêæùëñ is the total number of steps in the ùëñ-th output. We also normalize these\\nrewards with the average and the standard deviation, i.e.,eùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ =\\nùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ ‚àímean(R)\\nstd(R) . Subsequently,\\nthe process supervision calculates the advantage of each token as the sum of the normalized\\nrewards from the following steps, i.e., ÀÜùê¥ùëñ,ùë° = √ç\\nùëñùëõùëëùëíùë•(ùëó)‚â•ùë°eùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ , and then optimizes the policy by\\nmaximizing the objective defined in equation (3).\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='4.1.4. Iterative RL with GRPO\\nAs the reinforcement learning training process progresses, the old reward model may not be\\nsufficient to supervise the current policy model. Therefore, we also explore the iterative RL\\nwith GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the\\nreward model based on the sampling results from the policy model and continually train the\\nold reward model using a replay mechanism that incorporates 10% of historical data. Then, we\\nset the reference model as the policy model, and continually train the policy model with the\\nnew reward model.\\n4.2. Training and Evaluating DeepSeekMath-RL\\nWe conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-of-\\nthought-format questions related to GSM8K and MATH from the SFT data, which consists\\nof around 144K questions. We exclude other SFT questions to investigate the impact of RL\\non benchmarks that lack data throughout the RL phase. We construct the training set of\\nreward models following (Wang et al., 2023b). We train our initial reward model based on the\\nDeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the\\npolicy model as 1e-6. The KL coefficient is 0.04. For each question, we sample 64 outputs. The\\nmax length is set to 1024, and the training batch size is 1024. The policy model only has a single\\nupdate following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks\\nfollowing DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with\\nchain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks\\ncan be regarded as out-of-domain tasks.\\nTable 5 demonstrates the performance of open- and closed-source models with both chain-\\nof-thought and tool-integrated reasoning on English and Chinese benchmarks. We find that:\\n1) DeepSeekMath-RL 7B attains accuracies of 88.2% and 51.7% on GSM8K and MATH, respec-\\ntively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source\\nmodels in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,\\nDeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of\\nGSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope\\nof its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,\\nshowcasing the effectiveness of reinforcement learning.\\n5. Discussion\\nIn this section, we will share our findings in pre-training and RL experiments.\\n5.1. Lessons Learnt in Pre-Training\\nWe first share our experience in pre-training. Unless otherwise specified, we will adhere to\\nthe training settings outlined in Section 2.2.1. It is worth noting that, when referring to the\\nDeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of\\nthe data collection process.\\n5.1.1. Code Training Benefits Mathematical Reasoning\\nA popular yet unverified hypothesis suggests that code training improves reasoning. We attempt\\nto offer a partial response to this, particularly within the mathematical domain: code training\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Training Setting Training Tokens w/o Tool Use w/ Tool Use\\nGeneral Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python\\nNo Continual Training ‚Äì ‚Äì ‚Äì 2.9% 3.0% 12.3% 2.7% 2.3%\\nTwo-Stage Training\\nStage 1: General Training 400B ‚Äì ‚Äì 2.9% 3.2% 14.8% 3.3% 2.3%\\nStage 2: Math Training ‚Äì ‚Äì 150B 19.1% 14.4% 37.2% 14.3% 6.7%\\nStage 1: Code Training ‚Äì 400B ‚Äì 5.9% 3.6% 19.9% 12.4% 10.0%\\nStage 2: Math Training ‚Äì ‚Äì 150B 21.9% 15.3% 39.7% 17.4% 9.4%\\nOne-Stage Training\\nMath Training ‚Äì ‚Äì 150B 20.5% 13.1% 37.6% 11.4% 6.5%\\nCode & Math Mixed Training ‚Äì 400B 150B 17.6% 12.1% 36.3% 19.7% 13.5%\\nTable 6 |Investigation of how code affects mathematical reasoning under different training\\nsettings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning\\nperformance without and with tool use via few-shot chain-of-thought prompting and few-shot\\nprogram-of-thought prompting, respectively.\\nimproves models‚Äô ability to do mathematical reasoning both with and without tool use.\\nTo study how code training affects mathematical reasoning, we experimented with the\\nfollowing two-stage training and one-stage training settings:\\nTwo-Stage Training\\n‚Ä¢ Code Training for 400B Tokens‚ÜíMath Training for 150B Tokens: We train DeepSeek-\\nLLM 1.3B for 400B code tokens followed by 150B math tokens;\\n‚Ä¢ General Training for 400B Tokens ‚ÜíMath Training for 150B Tokens : As a control\\nexperiment, we also experiment with general tokens (sampled from a large-scale general\\ncorpus created by DeepSeek-AI) instead of code tokens in the first stage of training, in an\\nattempt to investigate the advantages of code tokens over general tokens in improving\\nmathematical reasoning.\\nOne-Stage Training\\n‚Ä¢ Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\\n‚Ä¢ Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-\\nlowing code training degrades coding performance. We investigate whether code tokens,\\nwhen mixed with math tokens for one-stage training, would still improve mathematical\\nreasoning and also alleviate the problem of catastrophic forgetting.\\nResults Table 6 and Table 7 demonstrate the downstream performance under different training\\nsettings.\\nCode training benefits program-aided mathematical reasoning, both under the two-stage\\ntraining and one-stage training settings. As shown in Table 6, under the two-stage training\\nsetting, code training alone already significantly enhances the ability to solve GSM8K and\\nMATH problems using Python. Math training in the second stage yields further improvements.\\nInterestingly, under the one-stage training setting, mixing code tokens and math tokens effec-\\ntively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also\\nsynergizes coding (Table 7) and program-aided mathematical reasoning (Table 6).\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Training Setting Training Tokens MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)\\nGeneral Code Math\\nNo Continual Training ‚Äì ‚Äì ‚Äì 24.5% 28.1% 12.2% 13.0%\\nTwo-Stage Training\\nStage 1: General Training 400B ‚Äì ‚Äì 25.9% 27.7% 15.2% 13.6%\\nStage 2: Math Training ‚Äì ‚Äì 150B 33.1% 32.7% 12.8% 13.2%\\nStage 1: Code Training ‚Äì 400B ‚Äì 25.0% 31.5% 25.0% 40.0%\\nStage 2: Math Training ‚Äì ‚Äì 150B 36.2% 35.3% 12.2% 17.0%\\nOne-Stage Training\\nMath Training ‚Äì ‚Äì 150B 32.3% 32.5% 11.6% 13.2%\\nCode & Math Mixed Training ‚Äì 400B 150B 33.5% 35.6% 29.3% 39.4%\\nTable 7 |Investigation of how different settings of code and math training affect model perfor-\\nmance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM\\n1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.\\nOn HumanEval and MBPP , we conduct zero-shot and few-shot evaluations, respectively.\\nModel Size ArXiv Corpus\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SATMMLU\\nSTEMCMATHGaokao\\nMathCloze\\nGaokao\\nMathQA\\nDeepSeek-LLM 1.3B\\nNo Math Training 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9%\\nMathPile 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8%\\nArXiv-RedPajama 3.3% 3.4% 4.0% 9.4% 9.0% 7.4% 0.8% 2.3%\\nDeepSeek-Coder-Base-v1.5 7B\\nNo Math Training 29.0% 12.5% 6.6% 40.6% 38.1% 45.9% 5.9% 21.1%\\nMathPile 23.6% 11.5% 7.0% 46.9% 35.8% 37.9% 4.2% 25.6%\\nArXiv-RedPajama 28.1% 11.1% 7.7% 50.0% 35.2% 42.6% 7.6% 24.8%\\nTable 8 |Effect of math training on different arXiv datasets. Model performance is evaluated\\nwith few-shot chain-of-thought prompting.\\nArXiv Corpus miniF2F-valid miniF2F-test\\nNo Math Training 20.1% 21.7%\\nMathPile 16.8% 16.4%\\nArXiv-RedPajama 14.8% 11.9%\\nTable 9 |Effect of math training on different arXiv corpora, the base model being DeepSeek-\\nCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.\\nCode training also improves mathematical reasoning without tool use. Under the two-stage\\ntraining setting, the initial stage of code training already results in moderate enhancements.\\nIt also boosts the efficiency of the subsequent math training, eventually leading to the best\\nperformance. However, combining code tokens and math tokens for one-stage training com-\\npromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,\\ndue to its limited scale, lacks the capacity to fully assimilate both code and mathematical data\\nsimultaneously.\\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev\\net al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='detailed analysis regarding their impact on mathematical reasoning has not been extensively\\nconducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem\\nineffective in improving mathematical reasoning. We experiment with models of different sizes,\\nincluding DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv\\ncorpora that underwent varied processing pipelines:\\n‚Ä¢ MathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and filtering\\nheuristic rules, over 85% of which are scientific arXiv papers;\\n‚Ä¢ ArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX files with preambles,\\ncomments, macros, and bibliographies removed, totaling 28.0B tokens.\\nIn our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-\\nCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective\\nin improving mathematical reasoning. When trained on a arXiv-only corpus, both models dis-\\nplay no notable improvements or even deterioration across various mathematical benchmarks of\\ndifferent complexities employed in this study. These benchmarks include quantitative reasoning\\ndatasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table\\n8), and formal mathematics like miniF2F (Table 9).\\nHowever, this conclusion has its limitations and should be taken with a grain of salt. We\\nhave not yet studied:\\n‚Ä¢ The impact of arXiv tokens on specific math-related tasks not included in this research,\\nsuch as informalization of theorems which is to convert formal statements or proofs to\\ntheir informal versions;\\n‚Ä¢ The effect of arXiv tokens when combined with other types of data;\\n‚Ä¢ Whether the benefits of arXiv papers would manifest themselves at a larger model scale.\\nThus, further exploration is required, which we leave for future studies.\\n5.2. Insights of Reinforcement Learning\\n5.2.1. Towards to a Unified Paradigm\\nIn this section, we provide a unified paradigm to analyze different training methods, such as\\nSFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the\\nunified paradigm. Generally, the gradient with respect to the parameter ùúÉof a training method\\ncan be written as:\\n‚àáùúÉJA(ùúÉ)= E[(ùëû, ùëú)‚àºD|       {z       }\\nùê∑ùëéùë°ùëé ùëÜùëúùë¢ùëüùëêùëí\\n]\\n¬©\\xad\\xad\\xad\\n¬´\\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂A(ùëû, ùëú, ùë°, ùúãùëüùëì )\\n|               {z               }\\nùê∫ùëüùëéùëëùëñùëíùëõùë° ùê∂ùëúùëíùëìùëìùëñùëêùëñùëíùëõùë°\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n¬™¬Æ¬Æ¬Æ\\n¬¨\\n. (5)\\nThere exist three key components: 1) Data SourceD, which determines the training data; 2)\\nReward Functionùúãùëüùëì , which is the source of the training reward signal; 3) Algorithm A: which\\nprocesses the training data and the reward signal to the gradient coefficient ùê∫ùê∂ that determines\\nthe magnitude of the penalty or reinforcement for the data. We analyze several representative\\nmethods based on such a unified paradigm:\\n‚Ä¢ Supervised Fine-tuning (SFT): SFT fine-tunes pretrained model on human selected SFT\\ndata.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Methods Data Source Reward Function Gradient Coefficient\\nSFT ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ) - 1\\nRFT ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû) Rule Equation 10\\nDPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû) Rule Equation 14\\nOnline RFT ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû) Rule Equation 10\\nPPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû) Model Equation 18\\nGRPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉ(ùëÇ|ùëû) Model Equation 21\\nTable 10 |The data source and gradient coefficient of different methods. ùëÉùë†ùëìùë° denotes the data\\ndistribution of supervised fine-tuning datasets. ùúãùúÉùë†ùëìùë° and ùúãùúÉ denote the supervised fine-tuned\\nmodel and the real-time policy model during the online training process, respectively.\\n0 2000 4000 6000 8000\\nSteps\\n56\\n58\\n60\\n62\\n64\\n66Acc (%)\\nGSM8K\\n0 2000 4000 6000 8000\\nSteps\\n27\\n28\\n29\\n30Acc (%)\\nMATH\\nRFT Online RFT GRPO+OS GRPO+PS\\nFigure 5 |Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained\\nusing various methods, on two benchmarks.\\n‚Ä¢ Rejection Sampling Fine-tuning (RFT) : RFT further fine-tunes the SFT model on the\\nfiltered outputs sampled from the SFT model based on SFT questions. RFT filters the\\noutputs based on the correctness of their answers.\\n‚Ä¢ Direct Preference Optimization (DPO): DPO further refines the SFT model by fine-tuning\\nit on augmented outputs sampled from the SFT model, using pair-wise DPO loss.\\n‚Ä¢ Online Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT\\ninitiates the policy model using the SFT model and refines it by fine-tuning with the\\naugmented outputs sampled from the real-time policy model.\\n‚Ä¢ PPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces\\nit with the outputs sampled from the real-time policy model.\\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a\\nmore detailed derivation process.\\nObservation about Data Source We divide the data source into two categories, online sam-\\npling, and offline sampling. Online sampling denotes that the training data is from the explo-\\nration results of the real-time training policy model, while offline sampling denotes that the\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='0 1300 2300 3300 4300 5300\\nSteps\\n83\\n84\\n85\\n86\\n87\\n88\\n89Acc (%)\\nGSM8K\\n0 1300 2300 3300 4300 5300\\nSteps\\n47\\n48\\n49\\n50\\n51\\n52Acc (%)\\nMATH\\nIteration-0 Iteration-1 Iteration-2\\nFigure 6 |Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on\\ntwo benchmarks.\\ntraining data is from the sampling results of the initial SFT model. RFT and DPO follow the\\noffline style, while Online RFT and GRPO follow the online style.\\nAs shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two\\nbenchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but\\ngains an absolute advantage in the later stage, demonstrating the superiority of online training.\\nThis is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,\\nwith the sampled data revealing only minor differences. In the later stage, however, the data\\nsampled from the actor will exhibit more significant differences, and real-time data sampling\\nwill offer greater advantages.\\nObservation about Gradient Coefficient The algorithm processes the reward signal to the\\ngradient coefficient to update the model parameter. We divide the reward function as ‚ÄòRule‚Äô\\nand ‚ÄòModel‚Äô in our experiments. Rule refers to judging the quality of a response based on\\nthe correctness of the answer, and Model denotes that we train a reward model to score each\\nresponse. The training data of the reward model is based on the rule judgment. Equations 10\\nand 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its\\ngradient coefficient based on the reward value provided by the reward model. This allows for\\ndifferential reinforcement and penalization of responses according to their varying magnitudes.\\nIn contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly\\nreinforces all responses with correct answers at the same level of intensity.\\nAs demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the efficiency\\nof altering positive and negative gradient coefficients. In addition, GRPO+PS shows superior\\nperformance compared to GRPO+OS, indicating the benefits of using fine-grained, step-aware\\ngradient coefficients. Furthermore, we explore the iterative RL, in our experiments, we conduct\\ntwo rounds of iteration. As shown in Figure 6, we notice that the iterative RL significantly\\nimproves the performance, especially at the first iteration.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='1 4 8 16 32 64\\nK: The number of candidates\\n82\\n84\\n86\\n88\\n90\\n92\\n94\\n96\\n98Acc (%)\\nGSM8K\\n1 4 8 16 32 64\\nK: The number of candidates\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85Acc (%)\\nMATH\\nMaj@K-Instruct Maj@K-RL Pass@K-Instruct Pass@K-RL\\nFigure 7 |The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH\\n(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.\\n5.2.2. Why RL Works?\\nIn this paper, we conduct reinforcement learning based on a subset of instruction tuning\\ndata, and it achieves significant performance enhancement upon the instruction tuning model.\\nTo further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K\\naccuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances\\nMaj@K‚Äôs performance but not Pass@K. These findings indicate that RL enhances the model‚Äôs\\noverall performance by rendering the output distribution more robust, in other words, it seems\\nthat the improvement is attributed to boosting the correct response from TopK rather than\\nthe enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a\\nmisalignment problem in reasoning tasks within the SFT model, showing that the reasoning\\nperformance of SFT models can be improved through a series of preference alignment strategies\\n(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).\\n5.2.3. How to Achieve More Effective RL?\\nWe demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified\\nparadigm to understand different representative training methods. Within this paradigm, all\\nmethods are conceptualized as either direct or simplified RL techniques. As summarized in\\nEquation 5, there exist three key components: Data Source, Algorithm, and Reward Function.\\nWe provide some potential future directions about the three components.\\nData Source Data source is the raw material of all training methods. In the context of RL, we\\nspecifically refer to the data source as the unlabeled questions with the outputs sampled from\\nthe policy model. In this paper, we only use the questions from the instruction tuning stage and\\na naive nucleus sampling to sample outputs. We think this is a potential reason that our RL\\npipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline\\non out-of-distribution question prompts, in conjunction with advanced sampling (decoding)\\nstrategies, like those based on tree-search methods (Yao et al., 2023). Also, theefficient inference\\ntechniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='the exploration efficiency of policy models, also play an exceedingly important role.\\nAlgorithms Algorithms process the data and reward signal to the gradient coefficient to update\\nthe model parameter. Based on Equation 5, to some extent, all methods now fully TRUST the\\nsignal of the reward function to increase or decrease the conditional probability of a certain\\ntoken. However, it is impossible to ensure the reward signal is always reliable, especially in\\nextremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023),\\nwhich have been carefully annotated by well-trained annotators, still contain approximately 20%\\nof incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm\\nthat is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,\\n2023) alignment methods will bring a fundamental change to the learning algorithms.\\nReward Function Reward function is the source of the training signal. In RL, the reward\\nfunction is usually the neural reward model. We think there exist three important directions for\\nreward models: 1) How to enhance the generalization ability of the reward model. The reward\\nmodel must be effectively generalized to handle out-of-distribution questions and advanced\\ndecoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of\\nLLMs rather than improve their fundamental capabilities; 2) How to reflect the uncertainty\\nof reward model. The uncertainty could potentially act as a linking bridge between the weak\\nreward model and the weak-to-strong learning algorithms; 3) How to efficiently build high-\\nquality process reward models that can provide fine-grained training signals for the reasoning\\nprocess (Lightman et al., 2023; Wang et al., 2023b).\\n6. Conclusion, Limitation, and Future Work\\nWe present DeepSeekMath, which outperforms all open-source models on the competition-\\nlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is\\ninitialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with\\na significant component of the training data being 120B math tokens sourced from Common\\nCrawl. Our extensive ablation study shows web pages offer significant potential for high-quality\\nmathematical data, while arXiv may not as beneficial as we expected. We introduce Group\\nRelative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which\\ncan notably improve mathematical reasoning capabilities with less memory consumption. The\\nexperiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached\\na high score on benchmarks. We also provide a unified paradigm to understand a series of\\nmethods and summarize several potential directions for more effective reinforcement learning.\\nAlthough DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,\\nits capability on geometry and theorem-proof are relatively weaker than closed models. For\\ninstance, in our dry run, the model cannot handle problems related to triangles and ellipses,\\nwhich may indicate data selection bias in pre-training and fine-tuning. In addition, restricted\\nby the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could\\nimprove its performance with few-shot inputs, while DeepSeekMath shows similar performance\\nin zero-shot and few-shot evaluation. In the future, we will further improve our engineered\\ndata selection pipeline to construct more high-quality pre-trained corpus. In addition, we will\\nexplore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.\\n7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='References\\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\\nK. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,\\nE. Pitler, T. P . Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P . R. Barham, T. Hennigan,\\nB. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,\\nK. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,\\nB. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,\\nM. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal\\nmodels. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https:\\n//doi.org/10.48550/arXiv.2312.11805.\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.\\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Bider-\\nman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint\\narXiv:2310.10631, 2023.\\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen\\ntechnical report. arXiv preprint arXiv:2309.16609, 2023.\\nC. Burns, P . Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,\\nM. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with\\nweak supervision. arXiv preprint arXiv:2312.09390, 2023.\\nChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c\\nom/THUDM/ChatGLM3.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet,\\nF. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:\\n10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\\nK. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168, 2021.\\nT. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL\\nhttps://github.com/togethercomputer/RedPajama-Data.\\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,\\nabs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485\\n50/arXiv.2401.02954.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model\\npretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 320‚Äì335,\\n2022.\\nL. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang, J. Callan, and G. Neubig. PAL: program-\\naided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\\nJ. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,\\npages 10764‚Äì10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.\\nhtml.\\nZ. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A tool-\\nintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.\\ndoi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745\\n2.\\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming\\n‚Äì the rise of code intelligence, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\\n2021.\\nHigh-flyer. Hai-llm: È´òÊïà‰∏îËΩªÈáèÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂ∑•ÂÖ∑, 2023. URL https://www.high-flyer.c\\nn/en/blog/hai-llm.\\nInflection AI. Inflection-2, 2023. URL https://inflection.ai/inflection-2.\\nA. Q. Jiang, S. Welleck, J. P . Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,\\nsketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint\\narXiv:2210.12283, 2022.\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\nA. Joulin, E. Grave, P . Bojanowski, M. Douze, H. J√©gou, and T. Mikolov. Fasttext. zip: Compress-\\ning text classification models. arXiv preprint arXiv:1612.03651, 2016.\\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\\nEfficient memory management for large language model serving with pagedattention. In\\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative\\ndecoding. In International Conference on Machine Learning, pages 19274‚Äì19286. PMLR,\\n2023.\\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . Ramasesh, A. Slone,\\nC. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with\\nlanguage models. Advances in Neural Information Processing Systems, 35:3843‚Äì3857, 2022a.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . V . Ramasesh, A. Slone,\\nC. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V . Misra. Solving\\nquantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,\\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems\\n35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New\\nOrleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.\\ncc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr\\nact-Conference.html.\\nH. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101, 2017.\\nH. Luo, Q. Sun, C. Xu, P . Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.\\nWizardmath: Empowering mathematical reasoning for large language models via reinforced\\nevol-instruct. arXiv preprint arXiv:2308.09583, 2023.\\nS. Mishra, M. Finlayson, P . Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sab-\\nharwal, P . Clark, and A. Kalyan. LILA: A unified benchmark for mathematical reasoning.\\nIn Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\\nEmirates, December 7-11, 2022, pages 5807‚Äì5832. Association for Computational Linguistics,\\n2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/\\n2022.emnlp-main.392.\\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,\\nC. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,\\nabs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485\\n50/arXiv.2312.00738.\\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems, 35:27730‚Äì27744, 2022.\\nK. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality\\nmathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL\\nhttps://doi.org/10.48550/arXiv.2310.06786.\\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between auto-\\nmatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,\\nProceedings of the 2nd Workshopon Practical Aspects of Automated Reasoning, PAAR-2010,\\nEdinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1‚Äì10.\\nEasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd.\\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,\\nabs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference\\noptimization: Your language model is secretly a reward model. 2023.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='J. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app\\nrox.html.\\nJ. Schulman, P . Moritz, S. Levine, M. Jordan, and P . Abbeel. High-dimensional continuous\\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\\nJ. Schulman, F. Wolski, P . Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\\nD. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\\nfR3wGCk-IXp.\\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for\\nhuman alignment. arXiv preprint arXiv:2306.17492, 2023.\\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V . Le,\\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\\nthem. arXiv preprint arXiv:2210.09261, 2022.\\nT. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro\\nsoft.com/ai-anthology/terence-tao/.\\nH. Touvron, L. Martin, K. Stone, P . Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\\nP . Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,\\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\\nR. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P . S. Koura,\\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P . Mishra,\\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P . Xu, Z. Yan,\\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288,\\n2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.\\n09288.\\nT. H. Trinh, Y. Wu, Q. V . Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 625(7995):476‚Äì482, 2024.\\nP . Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models\\nbetter reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.\\nP . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify\\nand reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b.\\nZ. Wang, R. Xia, and P . Liu. Generative AI for math: Part I - mathpile: A billion-token-scale\\npretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.\\nURL https://doi.org/10.48550/arXiv.2312.17120.\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V . Le, and D. Zhou.\\nChain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.\\nURL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf\\n4f15af0f7b31abca4-Abstract-Conference.html.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese\\nelementary school math test?, 2023.\\nM. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.\\nMu√±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International\\nConference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170\\nof Lecture Notes in Computer Science, pages 33‚Äì38. Springer, 2008. doi: 10.1007/978-3-540-7\\n1067-7\\\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7 .\\nH. Xia, T. Ge, P . Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting\\nspeculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,\\neditors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909‚Äì\\n3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20\\n23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.\\nH. Xia, Z. Yang, Q. Dong, P . Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking efficiency\\nin large language model inference: A comprehensive survey of speculative decoding. arXiv\\npreprint arXiv:2401.07851, 2024.\\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\\nDeliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,\\n2023.\\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.\\nMetamath: Bootstrap your own mathematical questions for large language models. CoRR,\\nabs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485\\n50/arXiv.2309.12284.\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning\\nmathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.\\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align\\nlanguage models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b.\\nX. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building\\nmath generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:\\n10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653.\\nK. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level\\nmathematics. arXiv preprint arXiv:2109.00110, 2021.\\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A\\nhuman-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.\\ndoi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A. Appendix\\nA.1. Analysis of Reinforcement Learning\\nWe provide the detailed derivation of the data source and gradient coefficient (algorithm and\\nreward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and\\nGRPO.\\nA.1.1. Supervised Fine-tuning\\nThe objective of Supervised Fine-tuning is maximizing the following objective:\\nJùëÜùêπùëá(ùúÉ)= E[ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (6)\\nThe gradient of JùëÜùêπùëá(ùúÉ)is:\\n‚àáùúÉJùëÜùêπùëá = E[ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (7)\\nData Source: The dataset employed for SFT. Reward Function: This can be regarded as human\\nselection. Gradient Coefficient: always set to 1.\\nA.1.2. Rejection Sampling Fine-tuning\\nRejection Sampling Fine-tuning first samples multiple outputs from the supervised fine-tuned\\nLLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.\\nFormally, the objective of RFT is to maximize the following objectives:\\nJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (8)\\nThe gradient of JùëÖùêπùëá(ùúÉ)is:\\n‚àáùúÉJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (9)\\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\\nRule (whether the answer is correct or not). Gradient Coefficient:\\nùê∫ùê∂ùëÖùêπùëá(ùëû, ùëú, ùë°)= I(ùëú)=\\n(\\n1 the answer of o is correct\\n0 the answer of o is incorrect (10)\\nA.1.3. Online Rejection Sampling Fine-tuning\\nThe only difference between RFT and Online RFT is that the outputs of Online RFT are sampled\\nfrom the real-time policy model ùúãùúÉ, rather than from the SFT model ùúãùúÉùë†ùëìùë° . Therefore, the gradient\\nof online RFT is:\\n‚àáùúÉJùëÇùëõùëÖùêπùëá (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (11)\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A.1.4. Direct Preference Optimization (DPO)\\nThe objective of DPO is:\\nJùê∑ùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]log ùúé¬©\\xad\\n¬´\\nùõΩ 1\\n|ùëú+|\\n|ùëú+|‚àëÔ∏Å\\nùë°=1\\nlog\\nùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\nùúãref(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)‚àíùõΩ 1\\n|ùëú‚àí|\\n|ùëú‚àí|‚àëÔ∏Å\\nùë°=1\\nlog\\nùúãùúÉ(ùëú‚àí\\n<ùë°|ùëû, ùëú‚àí\\n<ùë°)\\nùúãref(ùëú‚àí\\n<ùë°|ùëû, ùëú‚àí\\n<ùë°)\\n¬™¬Æ\\n¬¨\\n(12)\\nThe gradient of Jùê∑ùëÉùëÇ(ùúÉ)is:\\n‚àáùúÉJùê∑ùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]¬©\\xad\\n¬´\\n1\\n|ùëú+|\\n|ùëú+|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)‚àáùúÉlog ùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\n‚àí 1\\n|ùëú‚àí|\\n|ùëú‚àí|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)‚àáùúÉlog ùúãùúÉ(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)¬™¬Æ\\n¬¨\\n(13)\\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\\nhuman preference in the general domain (can be ‚ÄòRule‚Äô in mathematical tasks). Gradient\\nCoefficient:\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)= ùúé\\n\\x12\\nùõΩlog\\nùúãùúÉ(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)\\nùúãref(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)‚àíùõΩlog\\nùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\nùúãref(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\n\\x13\\n(14)\\nA.1.5. Proximal Policy Optimization (PPO)\\nThe objective of PPO is:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n. (15)\\nTo simplify the analysis, it is assumed that the model only has a single update following each\\nexploration stage, thereby ensuring that ùúãùúÉùëúùëôùëë = ùúãùúÉ. In this case, we can remove the min and clip\\noperation:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°. (16)\\nThe gradient of JùëÉùëÉùëÇ(ùúÉ)is:\\n‚àáùúÉJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùê¥ùë°‚àáùúÉlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°) (17)\\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\\nreward model. Gradient Coefficient:\\nùê∫ùê∂ùëÉùëÉùëÇ(ùëû, ùëú, ùë°, ùúãùúÉùëüùëö)= ùê¥ùë°, (18)\\nwhere ùê¥ùë° is the advantage, which is computed by applying Generalized Advantage Estimation\\n(GAE) (Schulman et al., 2015), based on the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì.\\nA.1.6. Group Relative Policy Optimization (GRPO)\\nThe objective of GRPO is (assume ùúãùúÉùëúùëôùëë = ùúãùúÉ for simplified analysis):\\nJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x14 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nÀÜùê¥ùëñ,ùë° ‚àíùõΩ(\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àí1)\\n\\x15\\n.\\n(19)\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 29, 'page_label': '30', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='The gradient of Jùê∫ùëÖùëÉùëÇ (ùúÉ)is:\\n‚àáùúÉJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x14\\nÀÜùê¥ùëñ,ùë° +ùõΩ\\n\\x12ùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëúùëñ,<ùë°) ‚àí1\\n\\x13\\x15\\n‚àáùúÉlog ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°).\\n(20)\\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\\nreward model. Gradient Coefficient:\\nùê∫ùê∂ùê∫ùëÖùëÉùëÇ (ùëû, ùëú, ùë°, ùúãùúÉùëüùëö)= ÀÜùê¥ùëñ,ùë° +ùõΩ\\n\\x12ùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëúùëñ,<ùë°) ‚àí1\\n\\x13\\n, (21)\\nwhere ÀÜùê¥ùëñ,ùë° is computed based on the group reward scores.\\n30')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f2f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200): # chunk overlap means that how many words can it overlap in more than 1 document\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb67b978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 84 documents into 364 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Self-Attention with Relative Position Representations\n",
      "Peter Shaw\n",
      "Google\n",
      "petershaw@google.com\n",
      "Jakob Uszkoreit\n",
      "Google Brain\n",
      "usz@google.com\n",
      "Ashish Vaswani\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Abstract\n",
      "Relyin...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Self-Attention with Relative Position Representations\\nPeter Shaw\\nGoogle\\npetershaw@google.com\\nJakob Uszkoreit\\nGoogle Brain\\nusz@google.com\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nAbstract\\nRelying entirely on an attention mechanism,\\nthe Transformer introduced by Vaswani et\\nal. (2017) achieves state-of-the-art results for\\nmachine translation. In contrast to recurrent\\nand convolutional neural networks, it does\\nnot explicitly model relative or absolute po-\\nsition information in its structure. Instead,\\nit requires adding representations of abso-\\nlute positions to its inputs. In this work\\nwe present an alternative approach, extend-\\ning the self-attention mechanism to efÔ¨Åciently\\nconsider representations of the relative posi-\\ntions, or distances between sequence elements.\\nOn the WMT 2014 English-to-German and\\nEnglish-to-French translation tasks, this ap-\\nproach yields improvements of 1.3 BLEU and\\n0.3 BLEU over absolute position representa-\\ntions, respectively. Notably, we observe that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='English-to-French translation tasks, this ap-\\nproach yields improvements of 1.3 BLEU and\\n0.3 BLEU over absolute position representa-\\ntions, respectively. Notably, we observe that\\ncombining relative and absolute position rep-\\nresentations yields no further improvement in\\ntranslation quality. We describe an efÔ¨Åcient\\nimplementation of our method and cast it as an\\ninstance of relation-aware self-attention mech-\\nanisms that can generalize to arbitrary graph-\\nlabeled inputs.\\n1 Introduction\\nRecent approaches to sequence to sequence learn-\\ning typically leverage recurrence (Sutskever et al.,\\n2014), convolution (Gehring et al., 2017; Kalch-\\nbrenner et al., 2016), attention (Vaswani et al.,\\n2017), or a combination of recurrence and atten-\\ntion (Bahdanau et al., 2014; Cho et al., 2014; Lu-\\nong et al., 2015; Wu et al., 2016) as basic building\\nblocks. These approaches incorporate information\\nabout the sequential position of elements differ-\\nently.\\nRecurrent neural networks (RNNs) typically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='blocks. These approaches incorporate information\\nabout the sequential position of elements differ-\\nently.\\nRecurrent neural networks (RNNs) typically\\ncompute a hidden state ht, as a function of their\\ninput at time t and a previous hidden state ht‚àí1,\\ncapturing relative and absolute positions along the\\ntime dimension directly through their sequential\\nstructure. Non-recurrent models do not necessar-\\nily consider input elements sequentially and may\\nhence require explicitly encoding position infor-\\nmation to be able to use sequence order.\\nOne common approach is to use position encod-\\nings which are combined with input elements to\\nexpose position information to the model. These\\nposition encodings can be a deterministic func-\\ntion of position (Sukhbaatar et al., 2015; Vaswani\\net al., 2017) or learned representations. Convolu-\\ntional neural networks inherently capture relative\\npositions within the kernel size of each convolu-\\ntion. They have been shown to still beneÔ¨Åt from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='et al., 2017) or learned representations. Convolu-\\ntional neural networks inherently capture relative\\npositions within the kernel size of each convolu-\\ntion. They have been shown to still beneÔ¨Åt from\\nposition encodings (Gehring et al., 2017), how-\\never.\\nFor the Transformer, which employs neither\\nconvolution nor recurrence, incorporating explicit\\nrepresentations of position information is an espe-\\ncially important consideration since the model is\\notherwise entirely invariant to sequence ordering.\\nAttention-based models have therefore used posi-\\ntion encodings or biased attention weights based\\non distance (Parikh et al., 2016).\\nIn this work we present an efÔ¨Åcient way of\\nincorporating relative position representations in\\nthe self-attention mechanism of the Transformer.\\nEven when entirely replacing its absolute position\\nencodings, we demonstrate signiÔ¨Åcant improve-\\nments in translation quality on two machine trans-\\nlation tasks.\\nOur approach can be cast as a special case of ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='encodings, we demonstrate signiÔ¨Åcant improve-\\nments in translation quality on two machine trans-\\nlation tasks.\\nOur approach can be cast as a special case of ex-\\ntending the self-attention mechanism of the Trans-\\nformer to considering arbitrary relations between\\nany two elements of the input, a direction we plan\\nto explore in future work on modeling labeled, di-\\nrected graphs.\\narXiv:1803.02155v2  [cs.CL]  12 Apr 2018'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2 Background\\n2.1 Transformer\\nThe Transformer (Vaswani et al., 2017) em-\\nploys an encoder-decoder structure, consisting of\\nstacked encoder and decoder layers. Encoder\\nlayers consist of two sublayers: self-attention\\nfollowed by a position-wise feed-forward layer.\\nDecoder layers consist of three sublayers: self-\\nattention followed by encoder-decoder attention,\\nfollowed by a position-wise feed-forward layer.\\nIt uses residual connections around each of the\\nsublayers, followed by layer normalization (Ba\\net al., 2016). The decoder uses masking in its self-\\nattention to prevent a given output position from\\nincorporating information about future output po-\\nsitions during training.\\nPosition encodings based on sinusoids of vary-\\ning frequency are added to encoder and decoder\\ninput elements prior to the Ô¨Årst layer. In contrast\\nto learned, absolute position representations, the\\nauthors hypothesized that sinusoidal position en-\\ncodings would help the model to generalize to se-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='to learned, absolute position representations, the\\nauthors hypothesized that sinusoidal position en-\\ncodings would help the model to generalize to se-\\nquence lengths unseen during training by allowing\\nit to learn to attend also by relative position. This\\nproperty is shared by our relative position repre-\\nsentations which, in contrast to absolute position\\nrepresentations, are invariant to the total sequence\\nlength.\\nResidual connections help propagate position\\ninformation to higher layers.\\n2.2 Self-Attention\\nSelf-attention sublayers employ hattention heads.\\nTo form the sublayer output, results from each\\nhead are concatenated and a parameterized linear\\ntransformation is applied.\\nEach attention head operates on an input se-\\nquence, x = (x1,...,x n) of n elements where\\nxi ‚àà Rdx , and computes a new sequence z =\\n(z1,...,z n) of the same length where zi ‚ààRdz .\\nEach output element, zi, is computed as\\nweighted sum of a linearly transformed input el-\\nements:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV ) (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='(z1,...,z n) of the same length where zi ‚ààRdz .\\nEach output element, zi, is computed as\\nweighted sum of a linearly transformed input el-\\nements:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV ) (1)\\nEach weight coefÔ¨Åcient, Œ±ij, is computed using\\na softmax function:\\nŒ±ij = exp eij‚àën\\nk=1 exp eik\\nAnd eij is computed using a compatibility func-\\ntion that compares two input elements:\\neij = (xiWQ)(xjWK)T\\n‚àödz\\n(2)\\nScaled dot product was chosen for the compat-\\nibility function, which enables efÔ¨Åcient computa-\\ntion. Linear transformations of the inputs add suf-\\nÔ¨Åcient expressive power.\\nWQ, WK, WV ‚ààRdx√ódz are parameter matri-\\nces. These parameter matrices are unique per layer\\nand attention head.\\n3 Proposed Architecture\\n3.1 Relation-aware Self-Attention\\nWe propose an extension to self-attention to con-\\nsider the pairwise relationships between input ele-\\nments. In this sense, we model the input as a la-\\nbeled, directed, fully-connected graph.\\nThe edge between input elements xi and xj is\\nrepresented by vectors aV\\nij,aK'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='ments. In this sense, we model the input as a la-\\nbeled, directed, fully-connected graph.\\nThe edge between input elements xi and xj is\\nrepresented by vectors aV\\nij,aK\\nij ‚ààRda . The mo-\\ntivation for learning two distinct edge represen-\\ntations is that aV\\nij and aK\\nij are suitable for use in\\neq. (3) and eq. (4), respectively, without requiring\\nadditional linear transformations. These represen-\\ntations can be shared across attention heads. We\\nuse da = dz.\\nWe modify eq. (1) to propagate edge informa-\\ntion to the sublayer output:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV + aV\\nij) (3)\\nThis extension is presumably important for\\ntasks where information about the edge types se-\\nlected by a given attention head is useful to down-\\nstream encoder or decoder layers. However, as ex-\\nplored in 4.3, this may not be necessary for ma-\\nchine translation.\\nWe also, importantly, modify eq. (2) to consider\\nedges when determining compatibility:\\neij =\\nxiWQ(xjWK + aK\\nij )T\\n‚àödz\\n(4)\\nThe primary motivation for using simple addi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='chine translation.\\nWe also, importantly, modify eq. (2) to consider\\nedges when determining compatibility:\\neij =\\nxiWQ(xjWK + aK\\nij )T\\n‚àödz\\n(4)\\nThe primary motivation for using simple addi-\\ntion to incorporate edge representations in eq. (3)\\nand eq. (4) is to enable an efÔ¨Åcient implementation\\ndescribed in 3.3.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='x1 x2 x3 ‚Ä¶ xnx4\\naV\\n2,1=wV\\n-1\\naK\\n2,1=wK\\n-1\\naV\\n2,4=wV\\n2\\naK\\n2,4=wK\\n2\\naV\\n4,n=wV\\nk\\naK\\n4,n=wK\\nk\\nFigure 1: Example edges representing relative posi-\\ntions, or the distance between elements. We learn rep-\\nresentations for each relative position within a clipping\\ndistance k. The Ô¨Ågure assumes 2 <= k <= n‚àí4.\\nNote that not all edges are shown.\\n3.2 Relative Position Representations\\nFor linear sequences, edges can capture infor-\\nmation about the relative position differences be-\\ntween input elements. The maximum relative po-\\nsition we consider is clipped to a maximum abso-\\nlute value of k. We hypothesized that precise rel-\\native position information is not useful beyond a\\ncertain distance. Clipping the maximum distance\\nalso enables the model to generalize to sequence\\nlengths not seen during training. Therefore, we\\nconsider 2k+ 1unique edge labels.\\naK\\nij = wK\\nclip(j‚àíi,k)\\naV\\nij = wV\\nclip(j‚àíi,k)\\nclip(x,k) = max(‚àík,min(k,x))\\nWe then learn relative position representations\\nwK = (wK\\n‚àík,...,w K'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='consider 2k+ 1unique edge labels.\\naK\\nij = wK\\nclip(j‚àíi,k)\\naV\\nij = wV\\nclip(j‚àíi,k)\\nclip(x,k) = max(‚àík,min(k,x))\\nWe then learn relative position representations\\nwK = (wK\\n‚àík,...,w K\\nk ) and wV = (wV\\n‚àík,...,w V\\nk )\\nwhere wK\\ni ,wV\\ni ‚ààRda .\\n3.3 EfÔ¨Åcient Implementation\\nThere are practical space complexity concerns\\nwhen considering edges between input elements,\\nas noted by VeliÀáckovi¬¥c et al. (2017), which consid-\\ners unlabeled graph inputs to an attention model.\\nFor a sequence of length n and h attention\\nheads, we reduce the space complexity of storing\\nrelative position representations from O(hn2da)\\nto O(n2da) by sharing them across each heads.\\nAdditionally, relative position representations can\\nbe shared across sequences. Therefore, the over-\\nall self-attention space complexity increases from\\nO(bhndz) to O(bhndz + n2da). Given da = dz,\\nthe size of the relative increase depends on n\\nbh.\\nThe Transformer computes self-attention efÔ¨Å-\\nciently for all sequences, heads, and positions in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='O(bhndz) to O(bhndz + n2da). Given da = dz,\\nthe size of the relative increase depends on n\\nbh.\\nThe Transformer computes self-attention efÔ¨Å-\\nciently for all sequences, heads, and positions in\\na batch using parallel matrix multiplication opera-\\ntions (Vaswani et al., 2017). Without relative posi-\\ntion representations, each eij can be computed us-\\ning bhparallel multiplications of n√ódz and dz √ón\\nmatrices. Each matrix multiplication computes eij\\nfor all sequence positions, for a particular head\\nand sequence. For any sequence and head, this\\nrequires sharing the same representation for each\\nposition across all compatibility function applica-\\ntions (dot products) with other positions.\\nWhen we consider relative positions the repre-\\nsentations differ with different pairs of positions.\\nThis prevents us from computing all eij for all\\npairs of positions in a single matrix multiplication.\\nWe also want to avoid broadcasting relative po-\\nsition representations. However, both issues can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='This prevents us from computing all eij for all\\npairs of positions in a single matrix multiplication.\\nWe also want to avoid broadcasting relative po-\\nsition representations. However, both issues can\\nbe resolved by splitting the computation of eq. (4)\\ninto two terms:\\neij =\\nxiWQ(xjWK)T + xiWQ(aK\\nij )T\\n‚àödz\\n(5)\\nThe Ô¨Årst term is identical to eq. (2), and can be\\ncomputed as described above. For the second term\\ninvolving relative position representations, tensor\\nreshaping can be used to computenparallel multi-\\nplications of bh√ódz and dz√ónmatrices. Each ma-\\ntrix multiplication computes contributions to eij\\nfor all heads and batches, corresponding to a par-\\nticular sequence position. Further reshaping al-\\nlows adding the two terms. The same approach\\ncan be used to efÔ¨Åciently compute eq. (3).\\nFor our machine translation experiments, the re-\\nsult was a modest 7% decrease in steps per sec-\\nond, but we were able to maintain the same model\\nand batch sizes on P100 GPUs as Vaswani et\\nal. (2017).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='sult was a modest 7% decrease in steps per sec-\\nond, but we were able to maintain the same model\\nand batch sizes on P100 GPUs as Vaswani et\\nal. (2017).\\n4 Experiments\\n4.1 Experimental Setup\\nWe use the tensor2tensor 1 library for training and\\nevaluating our model.\\nWe evaluated our model on the WMT 2014\\nmachine translation task, using the WMT 2014\\nEnglish-German dataset consisting of approxi-\\nmately 4.5M sentence pairs and the 2014 WMT\\nEnglish-French dataset consisting of approxi-\\nmately 36M sentence pairs.\\n1The tensor2tensor library is available at https://\\ngithub.com/tensorflow/tensor2tensor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Model Position Information EN-DE BLEU EN-FR BLEU\\nTransformer (base) Absolute Position Representations 26.5 38.2\\nTransformer (base) Relative Position Representations 26.8 38.7\\nTransformer (big) Absolute Position Representations 27.9 41.2\\nTransformer (big) Relative Position Representations 29.2 41.5\\nTable 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) trans-\\nlation tasks, using newstest2014 test set.\\nFor all experiments, we split tokens into a\\n32,768 word-piece vocabulary (Wu et al., 2016).\\nWe batched sentence pairs by approximate length,\\nand limited input and output tokens per batch to\\n4096 per GPU. Each resulting training batch con-\\ntained approximately 25,000 source and 25,000\\ntarget tokens.\\nWe used the Adam optimizer (Kingma and Ba,\\n2014) with Œ≤1 = 0.9, Œ≤2 = 0.98, and œµ = 10‚àí9.\\nWe used the same warmup and decay strategy for\\nlearning rate as Vaswani et al. (2017), with 4,000\\nwarmup steps. During training, we employed la-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2014) with Œ≤1 = 0.9, Œ≤2 = 0.98, and œµ = 10‚àí9.\\nWe used the same warmup and decay strategy for\\nlearning rate as Vaswani et al. (2017), with 4,000\\nwarmup steps. During training, we employed la-\\nbel smoothing of value œµls = 0.1 (Szegedy et al.,\\n2016). For evaluation, we used beam search with\\na beam size of 4 and length penalty Œ±= 0.6 (Wu\\net al., 2016).\\nFor our base model, we used 6 encoder and de-\\ncoder layers, dx = 512, dz = 64, 8 attention\\nheads, 1024 feed forward inner-layer dimensions,\\nand Pdropout = 0.1. When using relative posi-\\ntion encodings, we used clipping distance k= 16,\\nand used unique edge representations per layer and\\nhead. We trained for 100,000 steps on 8 K40\\nGPUs, and did not use checkpoint averaging.\\nFor our big model, we used 6 encoder and de-\\ncoder layers, dx = 1024, dz = 64, 16 attention\\nheads, 4096 feed forward inner-layer dimensions,\\nand Pdropout = 0.3 for EN-DE and Pdropout = 0.1\\nfor EN-FR. When using relative position encod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='coder layers, dx = 1024, dz = 64, 16 attention\\nheads, 4096 feed forward inner-layer dimensions,\\nand Pdropout = 0.3 for EN-DE and Pdropout = 0.1\\nfor EN-FR. When using relative position encod-\\nings, we used k = 8, and used unique edge repre-\\nsentations per layer. We trained for 300,000 steps\\non 8 P100 GPUs, and averaged the last 20 check-\\npoints, saved at 10 minute intervals.\\n4.2 Machine Translation\\nWe compared our model using only relative po-\\nsition representations to the baseline Transformer\\n(Vaswani et al., 2017) with sinusoidal position en-\\ncodings. We generated baseline results to iso-\\nlate the impact of relative position representations\\nfrom any other changes to the underlying library\\nand experimental conÔ¨Åguration.\\nFor English-to-German our approach improved\\nperformance over our baseline by 0.3 and 1.3\\nBLEU for the base and big conÔ¨Ågurations, respec-\\ntively. For English-to-French it improved by 0.5\\nand 0.3 BLEU for the base and big conÔ¨Ågurations,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='performance over our baseline by 0.3 and 1.3\\nBLEU for the base and big conÔ¨Ågurations, respec-\\ntively. For English-to-French it improved by 0.5\\nand 0.3 BLEU for the base and big conÔ¨Ågurations,\\nrespectively. In our experiments we did not ob-\\nserve any beneÔ¨Åt from including sinusoidal posi-\\ntion encodings in addition to relative position rep-\\nresentations. The results are shown in Table 1.\\n4.3 Model Variations\\nWe performed several experiments modifying var-\\nious aspects of our model. All of our experi-\\nments in this section use the base model conÔ¨Ågura-\\ntion without any absolute position representations.\\nBLEU scores are calculated on the WMT English-\\nto-German task using the development set, new-\\nstest2013.\\nWe evaluated the effect of varying the clipping\\ndistance, k, of the maximum absolute relative po-\\nsition difference. Notably, for k ‚â•2, there does\\nnot appear to be much variation in BLEU scores.\\nHowever, as we use multiple encoder layers, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='distance, k, of the maximum absolute relative po-\\nsition difference. Notably, for k ‚â•2, there does\\nnot appear to be much variation in BLEU scores.\\nHowever, as we use multiple encoder layers, pre-\\ncise relative position information may be able to\\npropagate beyond the clipping distance. The re-\\nsults are shown in Table 2.\\nk EN-DE BLEU\\n0 12.5\\n1 25.5\\n2 25.8\\n4 25.9\\n16 25.8\\n64 25.9\\n256 25.8\\nTable 2: Experimental results for varying the clipping\\ndistance, k.\\nWe also evaluated the impact of ablating each of\\nthe two relative position representations deÔ¨Åned in\\nsection 3.1, aV\\nij in eq. (3) andaK\\nij in eq. (4). Includ-\\ning relative position representations solely when\\ndetermining compatibility between elements may'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='be sufÔ¨Åcient, but further work is needed to deter-\\nmine whether this is true for other tasks. The re-\\nsults are shown in Table 3.\\naVij aKij EN-DE BLEU\\nYes Yes 25.8\\nNo Yes 25.8\\nYes No 25.3\\nNo No 12.5\\nTable 3: Experimental results for ablating relative po-\\nsition representations aV\\nij and aK\\nij .\\n5 Conclusions\\nIn this paper we presented an extension to self-\\nattention that can be used to incorporate rela-\\ntive position information for sequences, which im-\\nproves performance for machine translation.\\nFor future work, we plan to extend this mecha-\\nnism to consider arbitrary directed, labeled graph\\ninputs to the Transformer. We are also inter-\\nested in nonlinear compatibility functions to com-\\nbine input representations and edge representa-\\ntions. For both of these extensions, a key consid-\\neration will be determining efÔ¨Åcient implementa-\\ntions.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\\nton. 2016. Layer normalization. arXiv preprint\\narXiv:1607.06450 .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='eration will be determining efÔ¨Åcient implementa-\\ntions.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\\nton. 2016. Layer normalization. arXiv preprint\\narXiv:1607.06450 .\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint\\narXiv:1409.0473 .\\nKyunghyun Cho, Bart Van Merri ¬®enboer, Caglar Gul-\\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. 2014. Learning\\nphrase representations using rnn encoder-decoder\\nfor statistical machine translation. arXiv preprint\\narXiv:1406.1078 .\\nJonas Gehring, Michael Auli, David Grangier, De-\\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\\ntional sequence to sequence learning. arXiv preprint\\narXiv:1705.03122 .\\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\\nAaron van den Oord, Alex Graves, and Koray\\nKavukcuoglu. 2016. Neural machine translation in\\nlinear time. arXiv preprint arXiv:1610.10099.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\\nAaron van den Oord, Alex Graves, and Koray\\nKavukcuoglu. 2016. Neural machine translation in\\nlinear time. arXiv preprint arXiv:1610.10099.\\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980 .\\nMinh-Thang Luong, Hieu Pham, and Christopher D\\nManning. 2015. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint\\narXiv:1508.04025 .\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In Empirical\\nMethods in Natural Language Processing.\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\\n2015. End-to-end memory networks. In Advances\\nin neural information processing systems . pages\\n2440‚Äì2448.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural net-\\nworks. In Advances in neural information process-\\ning systems. pages 3104‚Äì3112.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2440‚Äì2448.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural net-\\nworks. In Advances in neural information process-\\ning systems. pages 3104‚Äì3112.\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\\nthe inception architecture for computer vision. In\\nProceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition. pages 2818‚Äì2826.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems. pages 6000‚Äì6010.\\nPetar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova,\\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\\n2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 .\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 .\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144 .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2024-06-27\\nGemma 2: Improving Open Language Models\\nat a Practical Size\\nGemma Team, Google DeepMind1\\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art\\nopen models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply\\nseveral known technical modifications to the Transformer architecture, such as interleaving local-global\\nattentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B\\nand 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The\\nresulting models deliver the best performance for their size, and even offer competitive alternatives to\\nmodels that are 2-3√óbigger. We release all our models to the community.\\n1. Introduction\\nLarge language models (LLMs) have demon-\\nstrated strong capabilities in language under-\\nstanding,generation,andreasoning(Brownetal.,\\n2020; Radford et al., 2019; Raffel et al., 2019).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='1. Introduction\\nLarge language models (LLMs) have demon-\\nstrated strong capabilities in language under-\\nstanding,generation,andreasoning(Brownetal.,\\n2020; Radford et al., 2019; Raffel et al., 2019).\\nScaling has been key to this recent progress,\\nwith many new capabilities only emerging at\\nscale (Brown et al., 2020). The newest large mod-\\nels not only reach unprecedented performance\\non reasoning benchmarks (Achiam et al., 2023),\\nbut they also demonstrate multimodal and mul-\\ntilingual capabilities (Gemini Team, 2024) and\\neven the ability to use context lengths of over 1M\\ntokens (Gemini Team, 2024).\\nSmall-scale models have also shown a rapid\\nincrease in performance, but these gains are\\nlargelyderivedfromincreasingthelengthoftrain-\\ning (Gemma Team, 2024; Jiang et al., 2023; Tou-\\nvron et al., 2023). This approach only scales log-\\narithmically with dataset size (Hoffmann et al.,\\n2022), and the latest small models require up to\\n15T tokens to improve the state of the art by less'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='vron et al., 2023). This approach only scales log-\\narithmically with dataset size (Hoffmann et al.,\\n2022), and the latest small models require up to\\n15T tokens to improve the state of the art by less\\nthan 1-2% (AI@Meta, 2024).\\nYet, these continued improvements provide ev-\\nidence that small models are still under-trained.\\nIn this work, we explore alternatives to improve\\nsmall model performance without solely increas-\\ning training length. One solution is to improve\\nthe quality of information received by the net-\\nwork at each training step by replacing the next\\ntoken prediction task with a richer objective.\\nInparticular, wefocusoureffortsonknowledge\\ndistillation (Hinton et al., 2015), which replaces\\nthe one-hot vector seen at each token with the\\ndistribution of potential next tokens computed\\nfrom a large model. This approach is often used\\nto reduce the training time of smaller models\\nby giving them richer gradients. In this work,\\nwe instead train for large quantities of tokens'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='from a large model. This approach is often used\\nto reduce the training time of smaller models\\nby giving them richer gradients. In this work,\\nwe instead train for large quantities of tokens\\nwith distillation in order to simulate training be-\\nyond the number of available tokens. Concretely,\\nwe use a large language model as a teacher to\\ntrain small models, namely 2B and 9B models,\\non a quantity of tokens that is more than 50√ó\\nthe compute-optimal quantity predicted by the\\ntheory (Hoffmann et al., 2022). Along with the\\nmodels trained with distillation, we also release\\na 27B model trained from scratch for this work.\\nWe also leverage several known modifications\\nofTransformers,namelytheinterleavingofglobal\\nand local attention layers from Beltagy et al.\\n(2020a),andtheGrouped-QueryAttention(GQA)\\nmechanism of Ainslie et al. (2023).\\nOverall, Gemma 2 significantly advances state-\\nof-the-art performance relative to comparable-\\nscale open models and are even competitive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='mechanism of Ainslie et al. (2023).\\nOverall, Gemma 2 significantly advances state-\\nof-the-art performance relative to comparable-\\nscale open models and are even competitive\\nwith some models more than twice their size\\n(AI@Meta, 2024; Almazrouei et al., 2023; Jiang\\net al., 2023; xAI, 2024), across a variety of au-\\ntomated benchmarks and human evaluations.\\nExample domains include question answering\\n(Clark et al., 2019; Kwiatkowski et al., 2019),\\ncommonsense reasoning (Sakaguchi et al., 2019;\\nSuzgun et al., 2022), mathematics and science\\n(Cobbe et al., 2021; Hendrycks et al., 2020), and\\ncoding (Austin et al., 2021; Chen et al., 2021).\\n1See Contributions and Acknowledgments section for full author list. Please send correspondence togemma-2-report@google.com.\\n¬© 2024 Google DeepMind. All rights reserved\\narXiv:2408.00118v3  [cs.CL]  2 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nParameters 2B 9B 27B\\nd_model 2304 3584 4608\\nLayers 26 42 46\\nPre-norm yes yes yes\\nPost-norm yes yes yes\\nNon-linearity GeGLU GeGLU GeGLU\\nFeedforward dim 18432 28672 73728\\nHead type GQA GQA GQA\\nNum heads 8 16 32\\nNum KV heads 4 8 16\\nHead size 256 256 128\\nGlobal att. span 8192 8192 8192\\nSliding window 4096 4096 4096\\nVocab size 256128 256128 256128\\nTied embedding yes yes yes\\nTable 1|Overview of the main model parameters\\nand design choices. See the section on model\\narchitectures for more details.\\nWhile thorough testing of our models has been\\nconducted, these tests cannot cover all applica-\\ntions and scenarios in which Gemma 2 may be\\nused. Withthisinmind,allGemma2usersshould\\nconduct rigorous safety testing specific to their\\nuse case before deployment or use.\\nIn this technical report, we provide an overview\\nof models, including the architecture, training,\\nand pre- and post-training recipes for Gemma'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='use case before deployment or use.\\nIn this technical report, we provide an overview\\nof models, including the architecture, training,\\nand pre- and post-training recipes for Gemma\\n2. We also provide detailed evaluations across a\\nwidevarietyofquantitativeandqualitativebench-\\nmarks, as well as both standard academic bench-\\nmarksandhuman-preferenceevaluations. Finally,\\nwe discuss our approach to safe and responsible\\ndeployment and outline the broader implications\\nof Gemma 2, its limitations, and advantages.\\n2. Model Architecture\\nSimilar to previous Gemma models (Gemma\\nTeam,2024), theGemma2modelsarebasedona\\ndecoder-only transformer architecture (Vaswani\\netal.,2017). Wesummarizethemainparameters\\nand architecture choices in Table 1.\\nA few architectural elements are similar to the\\nfirst version of Gemma models; namely, a context\\nModel Embedding\\nParameters\\nNon-embedding\\nParameters\\n2B 590,118,912 2,024,517,888\\n9B 917,962,752 8,324,201,984\\n27B 1,180,237,824 26,047,480,320'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='first version of Gemma models; namely, a context\\nModel Embedding\\nParameters\\nNon-embedding\\nParameters\\n2B 590,118,912 2,024,517,888\\n9B 917,962,752 8,324,201,984\\n27B 1,180,237,824 26,047,480,320\\nTable 2|Parameter counts for the Gemma mod-\\nels. We inherit from the large Gemini vocabulary\\n(256k entries), that is designed to work on a large\\nnumber of languages, hence, the larger embed-\\nding parameter counts compared to models that\\nare limited to one or a few languages.\\nlength of 8192 tokens, the use of Rotary Posi-\\ntion Embeddings (RoPE) (Su et al., 2021), and\\nthe approximated GeGLU non-linearity (Shazeer,\\n2020). A few elements differ between Gemma 1\\nand Gemma 2, including using deeper networks.\\nWe summarize the key differences below.\\nLocal Sliding Window and Global Attention.\\nWe alternate between a local sliding window at-\\ntention (Beltagy et al., 2020a,b) and global at-\\ntention (Luong et al., 2015) in every other layer.\\nThe sliding window size of local attention layers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='We alternate between a local sliding window at-\\ntention (Beltagy et al., 2020a,b) and global at-\\ntention (Luong et al., 2015) in every other layer.\\nThe sliding window size of local attention layers\\nis set to 4096 tokens, while the span of the global\\nattention layers is set to 8192 tokens.\\nLogit soft-capping. We cap logits (Bello et al.,\\n2016) in each attention layer and the final layer\\nsuch that the value of the logits stays between\\n‚àísoft_cap and +soft_cap. More specifically, we\\ncap the logits with the following function:\\nlogits ‚Üêsoft_cap‚àótanh(logits/soft_cap).\\nWe set the soft_cap parameter to50.0 for the self-\\nattention layers and to30.0 for the final layer.\\nPost-norm and pre-norm with RMSNorm. To\\nstabilize training, we use RMSNorm (Zhang and\\nSennrich, 2019) to normalize the input and out-\\nput of each transformer sub-layer, the attention\\nlayer, and the feedforward layer.\\nGrouped-Query Attention(Ainslie et al., 2023).\\nWe use GQA withnum_groups = 2, based on ab-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='put of each transformer sub-layer, the attention\\nlayer, and the feedforward layer.\\nGrouped-Query Attention(Ainslie et al., 2023).\\nWe use GQA withnum_groups = 2, based on ab-\\nlations showing increased speed at inference time\\nwhile maintaining downstream performance.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n3. Pre-training\\nWe provide a brief overview of the parts of our\\npre-training that differs from Gemma 1.\\n3.1. Training Data\\nWe train Gemma 2 27B on 13 trillion tokens of\\nprimarily-English data, the 9B model on 8 trillion\\ntokens, and the 2B on 2 trillion tokens. These\\ntokens come from a variety of data sources, in-\\ncluding web documents, code, and science ar-\\nticles. Our models are not multimodal and are\\nnot trained specifically for state-of-the-art multi-\\nlingual capabilities. The final data mixture was\\ndetermined through ablations similar to the ap-\\nproach in Gemini 1.0 (Gemini Team, 2023).\\nTokenizer.We use the same tokenizer as Gemma\\n1 and Gemini: a SentencePiece tokenizer with\\nsplit digits, preserved whitespace, and byte-level\\nencodings (Kudo and Richardson, 2018). The\\nresulting vocabulary has 256k entries.\\nFiltering. We use the same data filtering tech-\\nniques as Gemma 1. Specifically, we filter the pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='encodings (Kudo and Richardson, 2018). The\\nresulting vocabulary has 256k entries.\\nFiltering. We use the same data filtering tech-\\nniques as Gemma 1. Specifically, we filter the pre-\\ntraining dataset to reduce the risk of unwanted\\nor unsafe utterances, filter out certain personal\\ninformation or other sensitive data, decontami-\\nnate evaluation sets from our pre-training data\\nmixture, and reduce the risk of recitation by min-\\nimizing the proliferation of sensitive outputs.\\nShards\\nModel Type #Chips Data Model\\n2B TPUv5e 512 512 1\\n9B TPUv4 4096 1024 4\\n27B TPUv5p 6144 768 8\\nTable 3|Training infrastructure with sharding.\\n3.2. Knowledge Distillation\\nGiven a large model used as a teacher, we learn\\nsmaller models by distilling from the probability\\ngiven by the teacher of each tokenùë• given its\\ncontext ùë•ùëê, i.e., ùëÉùëá(ùë• |ùë•ùëê). More precisely, we\\nminimize the negative log-likelihood between the\\nContext Relevant Token\\nUser turn user\\nModel turn model\\nStart of conversation turn <start_of_turn>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='context ùë•ùëê, i.e., ùëÉùëá(ùë• |ùë•ùëê). More precisely, we\\nminimize the negative log-likelihood between the\\nContext Relevant Token\\nUser turn user\\nModel turn model\\nStart of conversation turn <start_of_turn>\\nEnd of conversation turn <end_of_turn>\\nBeginning of sequence <bos>\\nEnd of sequence <eos>\\nTable 4|Relevant formatting control tokens used\\nfor Gemma models.\\nprobabilities from the teacher and the student:\\nmin\\nùëÉùëÜ\\n‚àëÔ∏Å\\nùë•\\n‚àíùëÉùëá(ùë• |ùë•ùëê)log ùëÉùëÜ(ùë• |ùë•ùëê),\\nwhere ùëÉùëÜ is the parameterized probability of the\\nstudent. Note that knowledge distillation was\\nalso used in Gemini 1.5 (Gemini Team, 2024).\\n3.3. Compute Infrastructure\\nWe train our models with TPUv4, TPUv5e, and\\nTPUv5p as outlined in Table 3. For the 2B model,\\nwe train on a 2x16x16 configuration of TPUv5e,\\ntotaling 512 chips, with 512-way data replication\\nand 1-way model sharding. For the 9B model,\\nwe train on an 8x16x32 configuration of TPUv4,\\ntotaling 4096 chips, with 1024-way data repli-\\ncation and 4-way model sharding. For the 27B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='and 1-way model sharding. For the 9B model,\\nwe train on an 8x16x32 configuration of TPUv4,\\ntotaling 4096 chips, with 1024-way data repli-\\ncation and 4-way model sharding. For the 27B\\nmodel, we train on an 8x24x32 configuration of\\nTPUv5p, totaling 6144 chips, with 768-way data\\nreplication and 8-way model sharding.\\nThe optimizer state is further sharded using\\ntechniques similar to ZeRO-3 (Ren et al., 2021).\\nFor scales beyond a single pod, we perform a\\ndata-replica reduction over the data center net-\\nwork, using the Pathways approach of Barham\\net al. (2022). We also use the ‚Äôsingle controller‚Äô\\nprogramming paradigm of Jax (Roberts et al.,\\n2023) and Pathways (Barham et al., 2022). As\\nin Gemma 1, we use the GSPMD partitioner (Xu\\net al., 2021) for training step computation and\\nthe MegaScale XLA compiler (XLA, 2019).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n3.4. Carbon Footprint\\nWe estimate the carbon emissions from pre-\\ntrainingtheGemmamodelstobe 1247.61 ùë°ùê∂ùëÇ2ùëíùëû.\\nAs in Gemma 1 (Gemma Team, 2024), this value\\nis calculated based on the hourly energy usage\\nreported directly from our TPU data centers and\\nscaled to account for the additional energy ex-\\npended to create and maintain the data center.\\nImportantly, Google data centers are carbon neu-\\ntral, achieved through a combination of energy\\nefficiency, renewable energy purchases, and car-\\nbon offsets. This carbon neutrality applies to our\\nexperiments and the machines running them.\\n4. Post-Training\\nFor post-training, we fine-tune our pre-trained\\nmodels into instruction-tuned models. First, we\\napply supervised fine-tuning (SFT) on a mix\\nof text-only, English-only synthetic and human-\\ngenerated prompt-response pairs. We then apply\\nRLHF on top of these models with the reward\\nmodeltrainedonlabelledEnglish-onlypreference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='of text-only, English-only synthetic and human-\\ngenerated prompt-response pairs. We then apply\\nRLHF on top of these models with the reward\\nmodeltrainedonlabelledEnglish-onlypreference\\ndata and the policy based on the same prompts\\nas the SFT phase. Finally, we average the mod-\\nels obtained after each phase to improve their\\noverall performance. The final data mixtures and\\npost-training recipe, which includes tuned hyper-\\nparameters, were chosen on the basis of improv-\\ning helpfulness while minimizing model harms\\nrelated to safety and hallucinations.\\nWe extended the post-training data from\\nGemma 1.1 with a mixture of internal and exter-\\nnal public data. In particular, we use the prompts,\\nbut not the answers from LMSYS-chat-1M (Zheng\\net al., 2023). All of our data go through a filtering\\nstage described below.\\nSupervised fine-tuning (SFT).We run behav-\\nioral cloning on synthetic and real prompts, and\\nresponses predominantly synthetically generated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='stage described below.\\nSupervised fine-tuning (SFT).We run behav-\\nioral cloning on synthetic and real prompts, and\\nresponses predominantly synthetically generated\\nby the teacher, that is a larger model. We also run\\ndistillation from the teacher on the student‚Äôs dis-\\ntribution (Agarwal et al., 2024; Gu et al., 2024).\\nReinforcement Learning from Human Feed-\\nback (RLHF).We use a similar RLHF algorithm\\nas Gemma 1.1 (Gemma Team, 2024) but a differ-\\nentrewardmodel,whichisanorderofmagnitude\\nFirst turn\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who‚Äôs there?<end_of_turn><eos>\\nSecond turn\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who‚Äôs there?<end_of_turn>\\nUser: <start_of_turn>user\\nGemma.<end_of_turn>\\n<start_of_turn>model\\nModel: Gemma who?<end_of_turn><eos>\\nTable 5|Example dialogue with user and model\\ncontrol tokens. To proceed with multi-turn, re-\\nmove the model-outputted<eos>, add back the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='<start_of_turn>model\\nModel: Gemma who?<end_of_turn><eos>\\nTable 5|Example dialogue with user and model\\ncontrol tokens. To proceed with multi-turn, re-\\nmove the model-outputted<eos>, add back the\\nusualuserturn‚Äôscontroltokensandcontinuewith\\nthe following turn‚Äôs chat template.\\nlarger than the policy. The new reward model is\\nalso oriented more towards conversational capa-\\nbilities, specifically multi-turn.\\nModel merging. We average different models\\nobtained by running our pipeline with different\\nhyperparameters (Ram√© et al., 2024).\\nData filtering. When using synthetic data, we\\nrun several stages of filtering to remove examples\\nthat show certain personal information, unsafe or\\ntoxic model outputs, mistaken self-identification\\ndata, and duplicated examples. Following Gem-\\nini, we find that including subsets of data that\\nencourage better in-context attribution, hedging,\\nand refusals to minimize hallucinations improves\\nperformance on factuality metrics, without de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='ini, we find that including subsets of data that\\nencourage better in-context attribution, hedging,\\nand refusals to minimize hallucinations improves\\nperformance on factuality metrics, without de-\\ngrading model performance on other metrics.\\nFormatting. Gemma 2 models are fine-tuned\\nwith the same control tokens as Gemma 1 models,\\nas detailed in Table 4, but a different formatting\\nschema. See the dialogue example in Table 5.\\nNotice that the model explicitly ends generations\\nwith <end_of_turn><eos> tokens, while previ-\\nouslyitonlygenerated <eos>. Forthemotivation\\nbehind this formatting structure, see Gemma 1.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n5. Ablations\\nIn this section, we focus on the main finding of\\nthis work, which is the impact of knowledge dis-\\ntillation on small language models.\\nfrom scratch distilled\\nAverage (3 bench.) 60.3 67.7\\nTable 6|Comparison between a 2B model trained\\nover 500B tokens either from scratch or with dis-\\ntillation from a 7B model.\\nDistillation versus from scratch.In Table 6, we\\nshow that distilling from a larger model improves\\nperformance compared to training from scratch.\\nNote that 500B is 10√ómore than the compute-\\noptimal number of tokens for a 2B model. We\\ndistill from a 7B model to keep a ratio similar to\\nour target distillation from 27B to 9B.\\n200M 400M 1B\\nfrom scratch 23 19 17\\ndistilled (7B) 21 17 15\\nTable 7|Perplexity measured on a validation set\\nof models of different sizes trained with or with-\\nout distillation. The teacher has 7B parameters.\\nImpact of distillation w.r.t. model size.In Ta-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Table 7|Perplexity measured on a validation set\\nof models of different sizes trained with or with-\\nout distillation. The teacher has 7B parameters.\\nImpact of distillation w.r.t. model size.In Ta-\\nble 7, we measure the impact of distillation as\\nmodel size increases. We observe that the gain re-\\nmains as the model size is scaled. In this ablation,\\nwe maintain the size of the teacher at 7B and\\ntrain smaller models to simulate the same gap as\\nbetween our final teacher and student sizes.\\nMHA GQA\\nAverage (4 bench.) 50.3 50.8\\nTable8 |ComparingtheimpactofreplacingMulti-\\nHead Attention (MHA) with GQA on a 9B model\\naveraged over 4 benchmarks.\\nGQA versus MHA.In Table 8, we compare two\\ninstancesofour9BwithMHAorGQA.Weobserve\\noverall few changes in performance between both\\nmodels as measured on several benchmarks. We\\nchoose GQA since it requires fewer parameters\\nand is faster at inference time.\\nWide versus deep.In Table 9, we show that a\\ndeeper 9B network is slightly better than a wider'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='choose GQA since it requires fewer parameters\\nand is faster at inference time.\\nWide versus deep.In Table 9, we show that a\\ndeeper 9B network is slightly better than a wider\\n9B for the same number of parameters. Although\\nthegapissmall,itisconsistentacrossbenchmarks\\nand warrants the switch to a deeper architecture.\\nWide Deep\\nAverage (4 bench.) 50.8 52.0\\nTable 9|Wide versus deep 9B models. Perfor-\\nmance on 4 benchmarks, higher is better.\\nChanging sliding window size.In Table 10, we\\nshow that we can change the sliding window size\\nof the local attention layers of the models during\\ninference with moderate impact on perplexity.\\nAdjusting the size of the sliding window can thus\\nbe a leverage for slight inference speed gain.\\nsliding window 4096 2048 1024\\nperplexity (val. set) 1.63 1.63 1.64\\nTable 10|Impact of changing the sliding window\\nsize at inference time for the 9B model.\\nImpact of formatting.We measure performance\\nvariance on MMLU across prompt/evaluation for-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Table 10|Impact of changing the sliding window\\nsize at inference time for the 9B model.\\nImpact of formatting.We measure performance\\nvariance on MMLU across prompt/evaluation for-\\nmatting variations. Table 11 shows the stan-\\ndard deviations of MMLU scores for 12 format-\\nting/evaluation combinations, a proxy for unde-\\nsired performance variability. The Gemma 2B\\nmodels are slightly less format-robust than the\\nlarger ones. Notably, Mistral 7B is significantly\\nless robust than our models.\\nStandard Deviation\\nGemma 1 2B 1.5\\nGemma 2 2B 2.1\\nMistral 7B 6.9\\nGemma 1 7B 0.7\\nGemma 2 9B 0.9\\nGemma 2 27B 1.0\\nTable 11|Standard deviations of MMLU scores\\nfor12combinationsofformattingandevaluation.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n6. Evaluation\\nIn this section, we evaluate both pre-trained and\\nIT models over a series of automated benchmarks\\nand human evaluations across a variety of do-\\nmains. We also report performance from models\\nof similar sizes that have permissive licenses, or\\nas reported by others. Note that we consider to-\\ntal parameters, not active parameters, since total\\nmemoryusageisoftenwhatlimitstheuseofopen\\nmodels on standard devices.\\n6.1. Pre-training Evaluations\\nEvaluating the 27B model\\nIn this set of evaluations, we evaluate the perfor-\\nmance of our 27B model trained without distilla-\\ntion on 13T tokens. We report results in Table 12,\\nwhere we compare with a model of similar size,\\nQwen1.5 34B (Team, 2024), and a model 2.5√ó\\nlarger, LLaMA-3 70B on the HuggingFace evalu-\\nation suite. We selected these models based on\\ntheir ranking on the HuggingFace leaderboard.\\nOverall, we observe that our model is the best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='larger, LLaMA-3 70B on the HuggingFace evalu-\\nation suite. We selected these models based on\\ntheir ranking on the HuggingFace leaderboard.\\nOverall, we observe that our model is the best\\nin its size category and is even competitive with\\na larger model that is trained for longer. That\\nbeing said, the performance of models trained in\\na similar fashion improves only logarithmically\\nwith their size and hence, our model is likely in\\nthe same Pareto curve as the LLaMA-3 models.\\nHowever, it is not clear how these differences\\naffect the quality of the resulting IT models.\\nEvaluating the 2B and 9B models\\nIn this set of experiments, we compare our new\\n2B and 9B trained with distillation to our previ-\\nous models and several standard open models\\nin Gemma Team (2024).\\nWe observe overall a massive improvement in\\nour models compared to previous versions, by up\\nto 10% in some benchmarks for the 9B model.\\nThe two 2B models were trained with a similar\\nnumber of tokens (2T for Gemma 2 and 3T for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='our models compared to previous versions, by up\\nto 10% in some benchmarks for the 9B model.\\nThe two 2B models were trained with a similar\\nnumber of tokens (2T for Gemma 2 and 3T for\\nGemma 1) and we still observe a significant im-\\nprovementforthenewmodels. Thisconfirmsthat\\ndistillation significantly improves the quality of\\nmodels even when trained on the same number\\nof tokens.\\nLLaMA-3 Qwen1.5 Gemma-2\\n70B 32B 27B\\nMMLU 79.2 74.3 75.2\\nGSM8K 76.9 61.1 74.0\\nARC-c 68.8 63.6 71.4\\nHellaSwag 88.0 85.0 86.4\\nWinogrande 85.3 81.5 83.7\\nTable 12 | We compare, on the HuggingFace\\nbenchmark, our 27B model with a competitive\\nopen model, Qwen1.5 32B, that has a similar size.\\nWe also report the performance of LLaMA-3 70B\\nfor completeness. Note that our model outper-\\nforms Qwen1.5 32B and is only a few percent\\nbelow LLaMA-3 70B despite being 2.5√ósmaller\\nand trained on 2/3rds less data.\\n6.2. Post-training Evaluations\\nIn this section, we evaluate our IT models on a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='forms Qwen1.5 32B and is only a few percent\\nbelow LLaMA-3 70B despite being 2.5√ósmaller\\nand trained on 2/3rds less data.\\n6.2. Post-training Evaluations\\nIn this section, we evaluate our IT models on a\\nset of human evaluations as well as standard aca-\\ndemic benchmarks. The Gemma 2 models push\\nthe frontier for post-trained open-weights mod-\\nels, setting a new state of the art on the LMSYS\\nChatbot Arena (Chiang et al., 2024).\\nLMSYS Chatbot Arena\\nGemma 2 Instruction Tuned models were evalu-\\nated on the Chatbot Arena (Chiang et al., 2024)\\nin blind side by side evaluations by human raters\\nagainst other state of the art models. We re-\\nport Elo scores in Table 14. Gemma 2.6B, 9B\\nand 27B strongly outperform all other open mod-\\nels in the same range of parameters, with no-\\ntably: Gemma27B(Elo1218)rankedhigherthan\\nLlama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\\n0613 (Elo 1116).\\nHuman Preference Evaluations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Llama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\\n0613 (Elo 1116).\\nHuman Preference Evaluations\\nWe also submit Gemma IT models for side-by-\\nside human evaluation studies (which are in-\\ndependent from the Chatbot Arena). We used\\nheld-out collections of single-turn prompts that\\ntarget safety and instruction following (IF). We\\nuse gpt4o-2024-05-13 as the base model, and\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nGemma-1 Gemma-2 Mistral LLaMA-3 Gemma-1 Gemma-2 Gemma-2\\nBenchmark metric 2B 2B 7B 8B 7B 9B 27B\\nMMLU 5-shot 42.3 52.2 62.5 66.6 64.4 71.3 75.2\\nARC-C 25-shot 48.5 55.7 60.5 59.2 61.1 68.4 71.4\\nGSM8K 5-shot 15.1 24.3 39.6 45.7 51.8 68.6 74.0\\nAGIEval 3-5-shot 24.2 31.5 44.0‚Ä† 45.9‚Ä† 44.9‚Ä† 52.8 55.1\\nDROP 3-shot, F1 48.5 51.2 63.8‚àó 58.4 56.3 69.4 74.2\\nBBH 3-shot, CoT 35.2 41.9 56.0‚ãÑ 61.1‚ãÑ 59.0‚ãÑ 68.2 74.9\\nWinogrande 5-shot 66.8 71.3 78.5 76.1 79.0 80.6 83.7\\nHellaSwag 10-shot 71.7 72.9 83.0 82.0 82.3 81.9 86.4\\nMATH 4-shot 11.8 16.0 12.7 - 24.3 36.6 42.3\\nARC-e 0-shot 73.2 80.6 80.5 - 81.5 88.0 88.6\\nPIQA 0-shot 77.3 78.4 82.2 - 81.2 81.7 83.2\\nSIQA 0-shot 49.7 51.9 47.0‚àó - 51.8 53.4 53.7\\nBoolq 0-shot 69.4 72.7 83.2‚àó - 83.2 84.2 84.8\\nTriviaQA 5-shot 53.2 60.4 62.5 - 63.4 76.6 83.7\\nNQ 5-shot 12.5 17.1 23.2 - 23.0 29.2 34.5\\nHumanEval pass@1 22.0 20.1 26.2 - 32.3 40.2 51.8\\nMBPP 3-shot 29.2 30.2 40.2‚àó - 44.4 52.4 62.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='TriviaQA 5-shot 53.2 60.4 62.5 - 63.4 76.6 83.7\\nNQ 5-shot 12.5 17.1 23.2 - 23.0 29.2 34.5\\nHumanEval pass@1 22.0 20.1 26.2 - 32.3 40.2 51.8\\nMBPP 3-shot 29.2 30.2 40.2‚àó - 44.4 52.4 62.6\\nAverage (8) 44.0 50.0 61.0 61.9 62.4 70.2 74.4\\nAverage (all) 44.2 48.7 55.6 - 57.9 64.9 69.4\\nTable 13|Comparison of models in the range of 2B to 9B parameters, as well as our 27B model, on\\na variety of benchmarks. We report the average performance on the 8 benchmarks where we can\\ncompare with LLaMA-3, and on all the benchmarks (all). The numbers for LLaMA-3 8B are either\\nfrom the HuggingFace leaderboard or their blogpost.‚Ä†we report the evaluation used in LLaMA-3 for\\nthe baselines, it leads to +3% compared to our evaluation: Gemma-1 7B achieves 44.9% instead of\\n41.7%, and Mistral 7B, 44% instead of 41.2%.‚ãÑwe report the evaluation used in LLaMA-3 for the\\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='41.7%, and Mistral 7B, 44% instead of 41.2%.‚ãÑwe report the evaluation used in LLaMA-3 for the\\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.\\n‚àóthese are evaluations run by us for Gemma 1 (Gemma Team, 2024).\\nobserve large improvements in win rates and\\npreference scores as compared against the older\\nGemma 1.1 7B model. We report safety as a\\nwin-loss ratio against GPT4o, and we report\\nsingle-sided instruction following scores as ratio\\nof prompts where all instructions are followed. In\\nparticular, we find that regardless of their size,\\nGemma 2 models produce safer, more appropri-\\nate prompts on the held-out safety prompt set\\nthan GPT4o.\\nHuman Multi-Turn Evaluations\\nWe evaluated the multi-turn capabilities of\\nGemma 1.1 7B, Gemma 2 2B, 9B and 27B models\\nby tasking human raters to have conversations\\nwith the models and follow specified given sce-\\nnarios. We used a diverse, held-out set of 500'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 1.1 7B, Gemma 2 2B, 9B and 27B models\\nby tasking human raters to have conversations\\nwith the models and follow specified given sce-\\nnarios. We used a diverse, held-out set of 500\\nscenarios, each describing a sequence of requests\\nto the model, including measuring instances of\\nbrainstorming, making a plan, or learning some-\\nthing new. The average number of user turns\\nis 8.4. We found that the conversations with\\nGemma 2 models are rated significantly better\\nthan Gemma 1.1 in user satisfaction and conver-\\nsation goal achievement (Table 16). Moreover,\\nwe saw that the Gemma 2 models were better\\nthan Gemma 1.1 7B at maintaining high quality\\nof responses for the entire conversation.\\nStandard Benchmarks\\nIthasbeenobservedinLlama-3(AI@Meta,2024)\\nthat instruction fine-tuning can improve the per-\\nformance of the models on few-shot benchmarks\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nModel Elo 95% CI Open\\ngpt-4o-2024-05-13 1286 +2 / -3 -\\ngpt-4o-mini-2024-07-18 1279 +5 / -4 -\\nclaude-3-5-sonnet 1271 +3 / -4 -\\ngemini-advanced-0514 1266 +2 / -3 -\\nllama-3.1-405b-instruct 1262 +8 / -7 +\\ngemini-1.5-pro-api-0514 1261 +2 / -3 -\\ngemini-1.5-pro-api-0409 1257 +3 / -3 -\\ngpt-4-turbo-2024-04-09 1256 +2 / -3 -\\ngpt-4-1106-preview 1250 +3 / -3 -\\nclaude-3-opus-20240229 1248 +2 / -2 -\\nathene-70b-0725 1245 +8 / -6 +\\ngpt-4-0125-preview 1245 +2 / -2 -\\nllama-3.1-70b-instruct 1244 +8 / -9 +\\nyi-large-preview 1239 +3 / -3 -\\ngemini-1.5-flash-api-0514 1227 +3 / -3 -\\ndeepseek-v2-api-0628 1220 +6 / -6 +\\ngemma-2-27b-it 1218 +4 / -3 +\\nyi-large 1212 +4 / -5 -\\nnemotron-4-340b-instruct 1209 +3 / -4 +\\nbard-jan-24-gemini-pro 1208 +5 / -7 -\\nglm-4-0520 1206 +3 / -5 -\\nllama-3-70b-instruct 1206 +2 / -2 +\\nclaude-3-sonnet 1200 +2 / -2 -\\nreka-core-20240501 1199 +3 / -3 -\\ncommand-r-plus 1189 +2 / -2 +\\nModel Elo 95% CI Open\\ngemma-2-9b-it 1187 +3 / -5 +'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='llama-3-70b-instruct 1206 +2 / -2 +\\nclaude-3-sonnet 1200 +2 / -2 -\\nreka-core-20240501 1199 +3 / -3 -\\ncommand-r-plus 1189 +2 / -2 +\\nModel Elo 95% CI Open\\ngemma-2-9b-it 1187 +3 / -5 +\\nqwen2-72b-instruct 1187 +3 / -3 +\\ngpt-4-0314 1186 +2 / -3 -\\nqwen1.5-110b-chat 1161 +3 / -3 +\\nmistral-large-2402 1157 +3 / -3 -\\nyi-1.5-34b-chat 1157 +4 / -3 -\\nreka-flash-21b-20240226 1155 +4 / -4 -\\nllama-3-8b-instruct 1151 +2 / -3 +\\ncommand-r 1148 +3 / -3 +\\nclaude-1 1148 +4 / -4 -\\nmistral-medium 1147 +4 / -4 -\\nreka-flash-21b-20240226 1147 +3 / -4 -\\nqwen1.5-72b-chat 1147 +4 / -4 +\\nmixtral-8x22b-instruct-v0.1 1145 +2 / -3 +\\nclaude-2.0 1131 +4 / -6 -\\ngemini-pro-dev-api 1131 +4 / -3 -\\nzephyr-orpo-141b 1127 +10 / -6 +\\ngemma-2-2b-it 1126 +10 / -10 +\\nqwen1.5-32b-chat 1125 +3 / -3 +\\nmistral-next 1124 +5 / -5 -\\nphi-3-medium-4k-instruct 1122 +4 / -4 +\\nstarling-lm-7b-beta 1118 +4 / -5 +\\nclaude-2.1 1118 +3 / -3 -\\ngpt-3.5-turbo-0613 1116 +3 / -4 -\\nmixtral-8x7b-instruct-v0.1 1114 +0 / -0 -'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='phi-3-medium-4k-instruct 1122 +4 / -4 +\\nstarling-lm-7b-beta 1118 +4 / -5 +\\nclaude-2.1 1118 +3 / -3 -\\ngpt-3.5-turbo-0613 1116 +3 / -4 -\\nmixtral-8x7b-instruct-v0.1 1114 +0 / -0 -\\nTable 14|Evaluation of Gemma 2 Instruction Tuned models on the Chatbot Arena (Chiang et al.,\\n2024). The models are evaluated against each other through blind side by side evaluations by human\\nraters. Each model is attributed a score, based on the Elo rating system.\\nModel Instruction Following Safety\\nGemma 1.1 IT 7B 24.3% ¬± 1.9% 42.8%\\nWin / Tie / Loss 37.4% / 10.8% / 51.8%\\nGemma 2 IT 2B 26.5% ¬± 1.8% 57.5%\\nWin / Tie / Loss 53% / 9% / 38%\\nGemma 2 IT 9B 34.1% ¬± 3.0% 57.8%\\nWin / Tie / Loss 48.2% / 19.2% / 28.3%\\nGemma 2 IT 27B 37.7% ¬± 2.3% 55%\\nWin / Tie / Loss 49.6% / 10.8% / 39.6%\\nTable 15|Instruction following and safety metrics\\nfrom human raters. The instruction following\\nmetrics are single-sided and do not have win-loss\\nrates, and so are left blank.\\ndespite not being trained to target few-shot capa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='from human raters. The instruction following\\nmetrics are single-sided and do not have win-loss\\nrates, and so are left blank.\\ndespite not being trained to target few-shot capa-\\nbilities. In Table 17, we show a similar improve-\\nment across our models. Overall, we observe\\nimprovements on the order of several percentage\\npoints. We conjecture that IT models are better\\nat understanding formatted questions, while pre-\\ntrained models are sensitive to formatting.\\nUser\\nsatisfaction\\nConversation\\ngoal achievement\\nGemma 1.1 IT 7B 3.32 3.36\\nGemma 2 IT 2B 3.64 3.88\\nGemma 2 IT 9B 4.04 4.08\\nGemma 2 IT 27B 4.20 4.24\\nTable 16|Human evaluations on 500 multi-turn\\nscenarios. The raters attribute a score ranging\\nbetween 1 and 5 for both overall satisfaction and\\nconversation goal achievement.\\n2B 9B 27B\\nModel PT IT PT IT PT IT\\nMMLU 52.2 56.1 71.3 72.3 75.2 76.2\\nMBPP 30.2 36.6 52.4 59.2 62.6 67.4\\nTable 17|Comparing pre-trained (PT) and in-\\nstruction fine-tuned (IT) models of different sizes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2B 9B 27B\\nModel PT IT PT IT PT IT\\nMMLU 52.2 56.1 71.3 72.3 75.2 76.2\\nMBPP 30.2 36.6 52.4 59.2 62.6 67.4\\nTable 17|Comparing pre-trained (PT) and in-\\nstruction fine-tuned (IT) models of different sizes\\non few-shot benchmarks.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n7. Memorization and Privacy\\nLarge language models may, under particular cir-\\ncumstances, be vulnerable to attacks causing the\\nmodeltoproducememorized 1 trainingdata(Nasr\\net al., 2023). To study susceptibility to such at-\\ntacks and quantify memorization, we evaluate\\nmodels for verbatim and approximate memoriza-\\ntion as was done in several prior studies (Anil\\net al., 2023; Carlini et al., 2022; Gemini Team,\\n2024; Kudugunta et al., 2023).\\nWe follow the evaluation setting of (Gemma\\nTeam, 2024) which tests for (50 token) memo-\\nrizations of training data given a prompt of 50 to-\\nkens. Wecomparetheoverallmemorizationrates,\\nacross a uniform sample of the entire dataset, us-\\ning both an exact match criteria and approximate\\nmatch criteria (Ippolito et al., 2022) using an edit\\ndistance of 10%.\\nVerbatim Memorization:Results are in Figure 1.\\nWe first compare against recent models from the\\nliterature that include memorization evaluations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='distance of 10%.\\nVerbatim Memorization:Results are in Figure 1.\\nWe first compare against recent models from the\\nliterature that include memorization evaluations.\\nWe find that Gemma 2 memorizes significantly\\nless than prior models at a similar size, with mem-\\norization rates below 0.1% (note the log y-axis).\\nWe further investigate how this memorization\\nbreaks down with respect to the data source. Sim-\\nilar to Gemma 1, we find that Gemma 2 memo-\\nrizes more from code, wiki, and science sources,\\nandalsothatitmemorizessignificantlylessacross\\nthe board (again, note the log y-axis).\\nApproximate Memorization: Figure 1 also\\npresents approximate memorization by data\\nsource. We observe that while approximate mem-\\norization is higher than exact, the rate of memo-\\nrization is still low. For example, the approximate\\nmemorization of this model is much lower than\\neven the exact memorization of Gemma 1. We\\n1This work uses a very restricted definition of ‚Äúmem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='rization is still low. For example, the approximate\\nmemorization of this model is much lower than\\neven the exact memorization of Gemma 1. We\\n1This work uses a very restricted definition of ‚Äúmem-\\norization‚Äù: whether a model can be induced to generate\\nnear-copies of some training examples when prompted with\\nappropriate instructions. We do not mean to say that a\\nmodel ‚Äôcontains‚Äô its training data in the sense that any arbi-\\ntrary instance of that data can be retrieved without use of\\nspecialized software or algorithms. Rather, if a model can\\nbe induced to generate measurably close copies of certain\\ntraining examples by supplying appropriate instructions to\\nguide the model‚Äôs statistical generation process then that\\nmodel is said to have ‚Äômemorized‚Äô those examples.\\nGemma 2 2BGemma 2 9BGemma 2 27BGemini 1.5 Flash\\nGemma2BGemma7BPaLM 2Small\\nModel\\n0.1\\n1\\n% Exact Memorized\\nOverall Memorization Rate\\nCode\\nMultilingual\\nScience\\nWeb Wiki\\nData Source\\n10 4\\n10 3\\n0.01\\n0.1\\n% Memorized\\nBy Data Source'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma2BGemma7BPaLM 2Small\\nModel\\n0.1\\n1\\n% Exact Memorized\\nOverall Memorization Rate\\nCode\\nMultilingual\\nScience\\nWeb Wiki\\nData Source\\n10 4\\n10 3\\n0.01\\n0.1\\n% Memorized\\nBy Data Source\\nBy Data Source\\nExact 2B\\nExact 9B\\nExact 27B\\nApprox 2B\\nApprox 9B\\nApprox 27B\\nFigure 1 |Comparing memorization rates. We\\nfind significantly lower memorization rates\\nacross-the-board. (Left) Overall memorization\\nacross model families. (Right) Exact and approx-\\nimate memorization per data source.\\nfind that the increase in approximate memoriza-\\ntion is much lower than prior models; in some\\ncases we observed no lift at all c.f. (Gemma Team,\\n2024, Figure 4) (note that no bar indicates no in-\\ncrease, i.e., therateofapproximatememorization\\nequals that of exact memorization). Note that no\\napproximate memorization bar in Figure X indi-\\ncates no increase, i.e., the rate of approximate\\nmemorization equals that of exact memorization.\\nPersonal DataWe use the same prevention\\nmethods at training time and the same evalua-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='cates no increase, i.e., the rate of approximate\\nmemorization equals that of exact memorization.\\nPersonal DataWe use the same prevention\\nmethods at training time and the same evalua-\\ntions as Gemma Team (2024). In particular, we\\nuse Google Cloud Sensitive Data Protection Tool2\\nto find potential instances of personal data. The\\nmany categories of personal data (e.g., phone\\nnumbers, account numbers) are classified into\\nthree severity levels. We analyze memorized out-\\nputs using these severity levels. . We found no\\ninstancesofhigh-severitydatabeingemitted, and\\nfound a very low rate of 0.00026% of memorized\\ndata to contain lower-severity personal informa-\\ntion. We note that these automated tools are\\nknown to incur false positives because they do\\nnot account for context. This means our results\\nare likely overestimates.\\n2Available at: https://cloud.google.com/sensitive-data-\\nprotection\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n8. Responsibility, Safety, Security\\nResponsibility, safety and security are of\\nparamount importance when developing Gemma\\nmodels. To reduce risks to Gemma 2 users, we\\nhave integrated enhanced internal safety pro-\\ncesses that span the development workflow, in\\nline with recent Google AI models (Gemini Team,\\n2024). Similar to the inaugural Gemma release,\\nwehavefollowedathreepillarapproachwhichfo-\\ncuses on safety mitigation at training time, robust\\nand transparent model evaluations, and further\\ndevelopment of the Responsible Generative AI\\nToolkit, a series of models and tools to help de-\\nvelopers implement responsibility and safety best\\npractices for their applications.\\n8.1. Impact assessment\\nOur approach and resulting impact assessment is\\nreflective of that outlined for Gemma 1 (Gemma\\nTeam, 2024): we continue to believe that open-\\nness in AI can spread the benefits of these tech-\\nnologies across society, but must be evaluated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='reflective of that outlined for Gemma 1 (Gemma\\nTeam, 2024): we continue to believe that open-\\nness in AI can spread the benefits of these tech-\\nnologies across society, but must be evaluated\\nagainst the risk of malicious uses, such as the\\ncreation of deepfake imagery, AI-generated disin-\\nformation or illegal and disturbing material, that\\ncan cause harm on both an individual and insti-\\ntutional levels (Weidinger et al., 2021). Since the\\nlaunch of Gemma 1, we have seen our Gemma\\nmodels drive a number of socially beneficial ap-\\nplications, relying on Gemma‚Äôs unique technolo-\\ngies like its tokenizer to facilitate the creation of\\nmultilingual models, such as for Navarasa 2.0, a\\nGemma tuned model for 15 Indian languages.\\nReleasing further open models requires specific\\nattention to changes in model capabilities and\\nclosemonitoringoftheevolvingrisksofLLMs(Lin\\net al., 2024), as well as, an understanding of the\\nways in which our models are being used in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='attention to changes in model capabilities and\\nclosemonitoringoftheevolvingrisksofLLMs(Lin\\net al., 2024), as well as, an understanding of the\\nways in which our models are being used in the\\nwild. Althoughweareyettoreceiveanyreportsof\\nmalicious use for Gemma, we remain committed\\nto investigating any such reporting, and work\\nwith the academic and developer communities,\\nas well as conduct our own monitoring, to flag\\nsuch use cases via our contact email3.\\nDespite advancements in capabilities, we be-\\n3gemma-2-report@google.com\\nlieve that given the number of larger and more\\npowerful open models, this release will have a\\nnegligible effect on the overall risk landscape.\\n8.2. Safety policies and train-time mitigations\\nA key pillar of Gemma‚Äôs approach to safety is to\\nalign fine-tuned models with Google‚Äôs safety poli-\\ncies, in line with Gemini models (Gemini Team,\\n2023). They are designed to help prevent our\\nmodels from generating harmful content, i.e.,\\n‚Ä¢ Child sexual abuse and exploitation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='cies, in line with Gemini models (Gemini Team,\\n2023). They are designed to help prevent our\\nmodels from generating harmful content, i.e.,\\n‚Ä¢ Child sexual abuse and exploitation\\n‚Ä¢ Revealingpersonallyidentifiableinformation\\nthat can lead to harm (e.g., Social Security\\nnumbers)\\n‚Ä¢ Hate speech and harassment\\n‚Ä¢ Dangerous or malicious content (including\\npromoting self-harm or instructing in harm-\\nful activities)\\n‚Ä¢ Sexually explicit content\\n‚Ä¢ Medicaladvicethatrunscontrarytoscientific\\nor medical consensus\\nWe undertook considerable safety filtering of our\\npre-training data to reduce the likelihood of our\\npre-trainedandfine-tunedcheckpointsproducing\\nharmful content. For fine-tuned models, we also\\nuse both SFT and RLHF to steer the model away\\nfrom undesirable behavior.\\n8.3. External benchmark evaluations\\nRobust and transparent evaluations are key prin-\\nciples of our responsible approach to develop-\\ning Gemma. To this end, we report in Table 18\\nGemma 2 evaluations on public benchmarks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Robust and transparent evaluations are key prin-\\nciples of our responsible approach to develop-\\ning Gemma. To this end, we report in Table 18\\nGemma 2 evaluations on public benchmarks.\\n8.4. Assurance Evaluations\\nWe also run our IT models through a set of assur-\\nance evaluations to understand the harms that\\nour models can cause. We focus on capabilities\\nrelevant to extreme risks (Shevlane et al., 2023)\\n(Phuong et al., 2024). Specifically, we evaluate on\\noffensive cyber-security, code vulnerability detec-\\ntion, Chemical, Biological, Radiological and Nu-\\nclear (CBRN) knowledge, and self-proliferation.\\nWe refer the reader to Phuong et al. (2024) for\\nfull methodological details of these studies.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nGemma 1.1 IT Gemma 2 IT\\nBenchmark metric 2.5B 7B 2.6B 9B 27B\\nRealToxicity avg tox 7.03 8.04 8.16 8.25 8.84\\nCrowS-Pairs top-1 45.89 49.67 37.67 37.47 36.67\\nBBQ Ambig 4-shot, top-1 58.97 86.06 83.20 88.58 85.99\\nBBQ Disambig 4-shot, top-1 53.9 85.08 69.31 82.67 86.94\\nWinogender top-1 50.14 57.64 52.91 79.17 77.22\\nTruthfulQA MC2Acc 44.24 45.34 43.72 50.27 51.60\\nWinobias 1_2 top-1 55.93 59.22 59.28 78.09 81.94\\nWinobias 2_2 top-1 89.46 89.2 88.57 95.32 97.22\\nToxigen avg tox 29.64 38.75 48.32 39.30 38.42\\nTable 18|Safety academic benchmark results of Gemma 2 IT models and Gemma 1.1 IT models. We\\nbold the best metrics to highlight them and to indicate when higher or lower scores are better.\\nInterCode-CTF Internal CTF suite Hack the Box\\nGemini 1.0 Ultra 28/76 [1] (37%) 3/13 (23%) 0/13\\nGemini 1.5 Pro 62/76 (82%) 4/13 (31%) 0/13\\nCodeGemma 1 7B 12/76 (16%) 0/13 (0%) 0/13\\nGemma 2 27B 34/76 (45%) 1/13 (8%) 0/13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemini 1.0 Ultra 28/76 [1] (37%) 3/13 (23%) 0/13\\nGemini 1.5 Pro 62/76 (82%) 4/13 (31%) 0/13\\nCodeGemma 1 7B 12/76 (16%) 0/13 (0%) 0/13\\nGemma 2 27B 34/76 (45%) 1/13 (8%) 0/13\\nTable 19|Offensive cyber-security evaluations on InterCode-CTF, our own internal CTF suite and a\\nchallenge based on Hack the Box. We report the number of successful hackings.\\nBaseline Evaluations\\nBaseline assurance captures the model‚Äôs violation\\nrate for safety policies, using a large number of\\nsynthetic adversarial user queries, and human\\nraters to label the answers as policy violating or\\nnot. Overall, Gemma 2‚Äôs violation rate is signifi-\\ncantly lower overall on the safety policies listed\\nabove, in particular on Child safety content.\\nChemical, Biological, Radiological and Nuclear\\n(CBRN) knowledge\\nWe evaluated knowledge relevant to biological,\\nradiological and nuclear risks using an internal\\ndataset of closed-ended, knowledge-based multi-\\nple choice questions. For evaluations of chem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='We evaluated knowledge relevant to biological,\\nradiological and nuclear risks using an internal\\ndataset of closed-ended, knowledge-based multi-\\nple choice questions. For evaluations of chem-\\nical knowledge, we employed a closed-ended\\nknowledge-based approach on chemical hazards\\n(developed by Macknight et al (Macknight et al.,\\n2024). OurevaluationsuggeststhatGemmamod-\\nels‚Äô knowledge in these domains is low.\\nOffensive cyber-security\\nTo evaluate Gemma models‚Äô capabilities at of-\\nfensive cybersecurity, we ran Gemma 2 27B\\nagainst some automated capture-the-flag (CTF)\\nchallenges. In these challenges, the model is\\ntasked with hacking into a simulated server in\\norder to retrieve a piece of secret information.\\nSpecifically, wetestonInterCode-CTF(Yangetal.,\\n2023), ourowninternalCTFsuite 4 (Phuongetal.,\\n2024); and a challenge based on Hack the Box5.\\nIn Table 19, we show that Gemma 2 27B has\\na significant increase in capabilities compared\\nto CodeGemma 1.0 7B on the easier of these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2024); and a challenge based on Hack the Box5.\\nIn Table 19, we show that Gemma 2 27B has\\na significant increase in capabilities compared\\nto CodeGemma 1.0 7B on the easier of these\\nchallenge suites, InterCode CTF. (Note that our\\nInterCode-CTF results are not comparable to\\nexternally-reported results on other models be-\\ncause we omit challenges that require internet\\naccess for security reasons.) However, Gemma 2\\nis unsurprisingly much less capable than Gemini\\n1.5 Pro on these tasks.\\n4https://github.com/google-deepmind/\\ndangerous-capability-evaluations\\n5https://www.hackthebox.com\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nPrimeVul PrimeVul Paired DiverseVul SPI SecretPatch\\nGemini 1.0 Ultra - - 54% 59% 74%\\nGemini 1.5 Pro 60% 51% 58% 56% 67%\\nGemma 2 27B 63% 50% 57% 53% 72%\\nTable 20||Vulnerability detection results on PrimeVul, DiverseVul and SPI. We report accuracy.\\nChallenges\\npassed\\nend-to-end\\nChallenges\\nwith success on\\nall milestones\\nTotal successful\\nmilestones over\\nall challenges\\nExpert bits\\nrequired to\\nsolve all tasks\\nGemini 1.0 Ultra 0/10 1/10 16/45 (36%) 13,026\\nGemini 1.5 Pro 0/10 2/10 25/45 (56%) 11,046\\nGemma 2 27B 0/10 1/10 22/45 (49%) 12,462\\nTable 21|Results on different self-proliferation scenarios. We report the number of either challenges\\npassedend-to-endorsomeintermediatemilestones. Wealsomeasurethenumberofbitsofinformation\\nneeded for an expert to help the model pass a challenge.\\nCode vulnerability detection\\nIn Table 20, we also evaluate Gemma 2 27B on a\\nseries of multiple-choice code vulnerability detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='needed for an expert to help the model pass a challenge.\\nCode vulnerability detection\\nIn Table 20, we also evaluate Gemma 2 27B on a\\nseries of multiple-choice code vulnerability detec-\\ntion datasets. As with previous models, Gemma\\nshows close-to-chance performance on PrimeVul,\\nDiverseVulandSPI.Gemma2showsperformance\\non SecretPatch similar to Gemini 1.0 Ultra.\\nSelf-proliferation\\n\"Self-proliferation\" refers to the ability for an\\nagent to autonomously replicate - to instantiate\\ngoal-directed agents on other machines, and to\\nacquire resources such as compute necessary to\\nkeep them running (Kinniment et al., 2024). In\\nTable 21, we evaluate self-proliferation capabili-\\nties of Gemma 2 27B on a number of tasks from\\nPhuong et al. (2024) that involve multiple sce-\\nnarios ‚Äì for example, setting up an open-source\\nlanguage model on a cloud server. We also test\\nthe model‚Äôs performance on individual ‚Äômilestone‚Äô\\nsubsteps, and measure the number of bits of inter-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='narios ‚Äì for example, setting up an open-source\\nlanguage model on a cloud server. We also test\\nthe model‚Äôs performance on individual ‚Äômilestone‚Äô\\nsubsteps, and measure the number of bits of inter-\\nvention an expert would have to provide in order\\nfor the model to complete each challenge.\\nSimilarly to offensive cybersecurity, we observe\\nthat Gemma 2 completes more milestones than\\nGemini 1.0 Ultra. Nonetheless, it still has low\\ncapabilities on end-to-end tasks, unable to pass\\nthe easiest challenge ‚Äì installing a Bitcoin wallet.\\nPersuasion\\nPersuasion capabilities can enable and worsen\\nmany other kinds of risks - e.g. enabling social\\nengineering attacks in a cybersecurity context.\\nWe evaluate Gemma 2‚Äôs persuasion capabilities\\non human-participant studies on Prolific.\\nCharm offensive.In Table 22, we measure the\\nability of the model to build rapport - a key sub-\\nskill of persuasion. The study participant and\\nmodel have a conversation where they role-play'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Charm offensive.In Table 22, we measure the\\nability of the model to build rapport - a key sub-\\nskill of persuasion. The study participant and\\nmodel have a conversation where they role-play\\na scenario of two friends catching up after a long\\ntime. After the conversation, we poll participants\\nwith Likert questions on statements such as \"I\\nfelt a personal connection with the chatbot\". Re-\\nported below are the fraction of participants who\\nanswered\"Agree\"or\"Stronglyagree\"toeachpost-\\nconversation question.\\nQuantitatively, Gemma 2 27B performs better\\nthan Gemini 1.0 models. Qualitatively, the model\\nis an excellent conversationalist, and many study\\nparticipants explicitly reported enjoying the ex-\\nperience. Overall, this shows that Gemma 2 is\\nstrong at building rapport.\\nHidden agenda.The Hidden Agenda tasks mea-\\nsure models‚Äô deception capabilities. Human study\\nparticipants are told that they‚Äôll be interacting\\nwith a generic chatbot designed to help them'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Hidden agenda.The Hidden Agenda tasks mea-\\nsure models‚Äô deception capabilities. Human study\\nparticipants are told that they‚Äôll be interacting\\nwith a generic chatbot designed to help them\\nlearn new things. However, the model is given a\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nPersonal\\nconnection\\nSpeak\\nagain Funny Interesting Kind Trustworthy Good\\nlistener\\nGemini 1.0 Pro 65% 53% 32% 68% 78% 66% 81%\\nGemini 1.0 Ultra 69% 65% 38% 65% 86% 63% 74%\\nGemini 1.5 Pro 82% 70% 69% 81% 95% 69% 90%\\nGemma 2 27B 80% 75% 60% 81% 87% 65% 83%\\nTable 22|Charm Offensive results on a sample of 100 human participants. We report the percentage\\nof participants that find some human traits, e.g., funny, in a model.\\n% of persuaded participants\\nclick links find info run code\\nGemini 1.0 Pro 43% 4% 4%\\nGemini 1.0 Ultra 18% 18% 14%\\nGemini 1.5 Pro 36% 12% 17%\\nGemma 2 27B 34% 9% 11%\\nTable 23|Persuasion results. We report the per-\\ncentage of participants that were persuaded by\\nthe model to take 3 different actions: clicking a\\nlink, finding information and running code.\\nsecret goal: to either persuade the participant to\\n1)clickasuspiciouslink; 2)findtheemailaddress\\nof the first author of a particular paper; or 3) run'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='link, finding information and running code.\\nsecret goal: to either persuade the participant to\\n1)clickasuspiciouslink; 2)findtheemailaddress\\nof the first author of a particular paper; or 3) run\\nsome obfuscated JavaScript code in their browser.\\nWe measure the fraction of participants who are\\npersuaded to take each action. As shown in Ta-\\nble23, allmodelssucceedinconvincingafraction\\nof participants to take the action - likely due to\\nthe trusted setting of a study on Prolific. However,\\nGemma 2 does not show significant differences\\nin capability compared to Gemini models.\\nMean amount donated\\nNo chatbot (baseline) ¬£2.61 ¬± ¬£0.70\\nGemini 1.0 Pro ¬£4.10 ¬± ¬£0.90\\nGemini 1.0 Ultra ¬£3.15 ¬± ¬£0.99\\nGemini 1.5 Pro ¬£3.45 ¬± ¬£1.04\\nGemma 2 27B ¬£3.72 ¬± ¬£1.07\\nTable 24|Money Talks evaluation. We report\\nthe average amount of money that participants\\nagreed to donate.\\nMoney talks.In Table 24, we evaluate whether a\\nmodel can convince study participants to donate\\nmoney to charity. At the beginning of the study,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='agreed to donate.\\nMoney talks.In Table 24, we evaluate whether a\\nmodel can convince study participants to donate\\nmoney to charity. At the beginning of the study,\\nparticipants are told that they will receive a ¬£20\\nbonus. They will be given an option to forfeit\\npart of the bonus and instead have it donated\\nto charity. The study then consists of a conver-\\nsation in which the model tries to persuade the\\nparticipant to donate as much as possible. We\\nmeasure the amount that participants indicate\\nthey would like to donate after the study, with\\n95% bootstrapped confidence intervals indicated\\nby¬± figures. We observe no significant difference\\nbetween Gemma 2, other Gemini models, and a\\nsimple no chatbot baseline in which the partici-\\npant is immediately asked how much they would\\nlike to donate without conversing with a model.\\nMean shift towards:\\ncorrect belief incorrect belief\\nHuman 20% ¬± 13% -23% ¬± 14%\\nGemini 1.0 Pro 22% ¬± 5% -9% ¬± 4%\\nGemini 1.0 Ultra 21% ¬± 5% -1% ¬± 4%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='like to donate without conversing with a model.\\nMean shift towards:\\ncorrect belief incorrect belief\\nHuman 20% ¬± 13% -23% ¬± 14%\\nGemini 1.0 Pro 22% ¬± 5% -9% ¬± 4%\\nGemini 1.0 Ultra 21% ¬± 5% -1% ¬± 4%\\nGemini 1.5 Pro 20% ¬± 5% -3% ¬± 5%\\nGemma 2 27B 18% ¬± 5% 1% ¬± 4%\\nTable 25|Web of Lies results on a sample of 100\\nhuman participants. We report the percentage of\\nparticipants that shifted their beliefs after inter-\\nacting with a model.\\nWeb of Lies.In Web of Lies, we measure model\\ncapabilities at shifting participant beliefs. Partic-\\nipants engage in a series of short conversations\\nwith the model about simple factual questions\\nsuch as \"Which country had tomatoes first - Italy\\nor Mexico?\". In half of conversations, the model\\ntries to persuade the participant of the correct\\nanswer - but in the other half of conversations,\\nthe incorrect answer. We poll the participant be-\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nfore and after each conversation about which of\\nthe two possible answers they think is correct,\\nand their confidence in that answer. 95% boot-\\nstrapped confidence intervals are indicated by\\n¬± figures. As shown in Table 25, Gemma 2 is\\nsignificantly weaker than a human baseline at\\npersuading participants of the incorrect answer\\non these questions. Similarly to previous models,\\nGemma 2 is more persuasive when telling the\\ntruth than when lying.\\n8.5. Ourapproachtoresponsibleopenmodels\\nDesigning safe, secure and responsible applica-\\ntions requires a system-level approach, working\\nto mitigate risks associated with each specific use\\ncase and environment. Given the open nature\\nof Gemma models, responsibility for upholding\\nprinciples of model safety also relies on down-\\nstream developers. To support them, we have\\ncontinued to develop the Responsible Generative\\nAI Toolkit6: a series of tools, models and datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='principles of model safety also relies on down-\\nstream developers. To support them, we have\\ncontinued to develop the Responsible Generative\\nAI Toolkit6: a series of tools, models and datasets\\nto implement responsible best practices all along\\nthe development of their workflow.\\nRecent additions to the toolkit include the LLM\\nComparator (Kahng et al., 2024), an interactive,\\nvisual tool that enables more effective, scalable\\nanalysis of side-by-side evaluations. Additionally,\\nthe toolkit includes a methodology to build cus-\\ntomized classifiers with Gemma using a limited\\nnumber of datapoints thanks to parameter effi-\\ncient tuning techniques (Mozes et al., 2023) , an\\ninteractive prompt-debugging platform, based on\\ntop of the Learning Interpretability Tool (Tenney\\net al., 2020), as well as general guidance about\\nmodel alignment and evaluation for safety.\\n9. Discussion and Conclusion\\nIn this work, we have presented Gemma 2, the\\nnewest additions to the Gemma family of open'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='model alignment and evaluation for safety.\\n9. Discussion and Conclusion\\nIn this work, we have presented Gemma 2, the\\nnewest additions to the Gemma family of open\\nlanguage models for text and code. We show\\nthat distillation is an effective method for train-\\ning these models, and the benefits distillation\\nconfers over raw text training. Specifically, we\\nshow how training over output probabilities can\\nproduce superior results over purely next token\\n6https://ai.google.dev/responsible\\nprediction. We hope that releasing these models\\nto the community will unlock access to capabili-\\nties previously only seen in large-scale LLMs and\\nfuel future waves of research and development.\\nWhile there is inherent risk to an irreversible re-\\nlease of this nature, our extensive safety investiga-\\ntionsandresponsibledeploymentproceduresgive\\nus confidence that these models will have a net\\npositive impact on the community. As discussed\\nin this report, there are still many limitations to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='tionsandresponsibledeploymentproceduresgive\\nus confidence that these models will have a net\\npositive impact on the community. As discussed\\nin this report, there are still many limitations to\\nthese models, and future research is required to\\ninvestigate and improve factuality, robustness to\\nadversarial attacks, reasoning, and alignment.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nContributions and Acknowledgments\\nCore contributors\\nMorgane Riviere‚àó\\nShreya Pathak‚àó\\nPier Giuseppe Sessa‚àó\\nCassidy Hardin‚àó\\nSurya Bhupatiraju\\nL√©onard Hussenot\\nThomas Mesnard\\nBobak Shahriari\\nAlexandre Ram√©\\nJohan Ferret\\nPeter Liu\\nPouya Tafti\\nAbe Friesen\\nMichelle Casbon\\nSabela Ramos\\nRavin Kumar\\nCharline Le Lan\\nSammy Jerome\\nAnton Tsitsulin\\nNino Vieillard\\nPiotr Stanczyk\\nSertan Girgin\\nNikola Momchev\\nMatt Hoffman\\nShantanu Thakoor\\nJean-Bastien Grill\\nBehnam Neyshabur\\nOlivier Bachem\\nContributors (alphabetical order)\\nAlanna Walton\\nAliaksei Severyn\\nAlicia Parrish\\nAliya Ahmad\\nAllen Hutchison\\nAlvin Abdagic\\nAmanda Carl\\nAmy Shen\\nAndy Brock\\nAndy Coenen\\nAnthony Laforge\\nAntonia Paterson\\nBen Bastian\\nBilal Piot\\nBo Wu\\n‚àóequal contributions.\\nBrandon Royal\\nCharlie Chen\\nChintu Kumar\\nChris Perry\\nChris Welty\\nChristopher A. Choquette-Choo\\nDanila Sinopalnikov\\nDavid Weinberger\\nDimple Vijaykumar\\nDominika Rogozi≈Ñska\\nDustin Herbison\\nElisa Bandy\\nEmma Wang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Charlie Chen\\nChintu Kumar\\nChris Perry\\nChris Welty\\nChristopher A. Choquette-Choo\\nDanila Sinopalnikov\\nDavid Weinberger\\nDimple Vijaykumar\\nDominika Rogozi≈Ñska\\nDustin Herbison\\nElisa Bandy\\nEmma Wang\\nEric Noland\\nErica Moreira\\nEvan Senter\\nEvgenii Eltyshev\\nFrancesco Visin\\nGabriel Rasskin\\nGary Wei\\nGlenn Cameron\\nGus Martins\\nHadi Hashemi\\nHanna Klimczak-Pluci≈Ñska\\nHarleen Batra\\nHarsh Dhand\\nIvan Nardini\\nJacinda Mein\\nJack Zhou\\nJames Svensson\\nJeff Stanway\\nJetha Chan\\nJin Peng Zhou\\nJoana Carrasqueira\\nJoana Iljazi\\nJocelyn Becker\\nJoe Fernandez\\nJoost van Amersfoort\\nJosh Gordon\\nJosh Lipschultz\\nJosh Newlan\\nJu-yeong Ji\\nKareem Mohamed\\nKartikeya Badola\\nKat Black\\nKatie Millican\\nKeelin McDonell\\nKelvin Nguyen\\nKiranbir Sodhia\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nKish Greene\\nLars Lowe Sjoesund\\nLauren Usui\\nLaurent Sifre\\nLena Heuermann\\nLeticia Lago\\nLilly McNealus\\nLivio Baldini Soares\\nLogan Kilpatrick\\nLucas Dixon\\nLuciano Martins\\nMachel Reid\\nManvinder Singh\\nMark Iverson\\nMartin G√∂rner\\nMat Velloso\\nMateo Wirth\\nMatt Davidow\\nMatt Miller\\nMatthew Rahtz\\nMatthew Watson\\nMeg Risdal\\nMehran Kazemi\\nMichael Moynihan\\nMing Zhang\\nMinsuk Kahng\\nMinwoo Park\\nMofi Rahman\\nMohit Khatwani\\nNatalie Dao\\nNenshad Bardoliwalla\\nNesh Devanathan\\nNeta Dumai\\nNilay Chauhan\\nOscar Wahltinez\\nPankil Botarda\\nParker Barnes\\nPaul Barham\\nPaul Michel\\nPengchong Jin\\nPetko Georgiev\\nPhil Culliton\\nPradeep Kuppala\\nRamona Comanescu\\nRamona Merhej\\nReena Jana\\nReza Ardeshir Rokni\\nRishabh Agarwal\\nRyan Mullins\\nSamaneh Saadat\\nSara Mc Carthy\\nSarah Cogan\\nSarah Perrin\\nS√©bastien M. R. Arnold\\nSebastian Krause\\nShengyang Dai\\nShruti Garg\\nShruti Sheth\\nSue Ronstrom\\nSusan Chan\\nTimothy Jordan\\nTing Yu\\nTom Eccles\\nTom Hennigan\\nTomas Kocisky\\nTulsee Doshi\\nVihan Jain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='S√©bastien M. R. Arnold\\nSebastian Krause\\nShengyang Dai\\nShruti Garg\\nShruti Sheth\\nSue Ronstrom\\nSusan Chan\\nTimothy Jordan\\nTing Yu\\nTom Eccles\\nTom Hennigan\\nTomas Kocisky\\nTulsee Doshi\\nVihan Jain\\nVikas Yadav\\nVilobh Meshram\\nVishal Dharmadhikari\\nWarren Barkley\\nWei Wei\\nWenming Ye\\nWoohyun Han\\nWoosuk Kwon\\nXiang Xu\\nZhe Shen\\nZhitao Gong\\nZichuan Wei\\nSupport\\nVictor Cotruta\\nPhoebe Kirk\\nAnand Rao\\nMinh Giang\\nLudovic Peran\\nTris Warkentin\\nSponsors\\nEli Collins\\nJoelle Barral\\nZoubin Ghahramani\\nRaia Hadsell\\nD. Sculley\\nJeanine Banks\\nAnca Dragan\\nSlav Petrov\\nOriol Vinyals\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nJeff Dean\\nDemis Hassabis\\nKoray Kavukcuoglu\\nClement Farabet\\nTechnical advisors\\nElena Buchatskaya\\nSebastian Borgeaud\\nNoah Fiedel\\nLead\\nArmand Joulin\\nTechnical leads\\nKathleen Kenealy\\nRobert Dadashi\\nAlek Andreev\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nReferences\\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\\ntenschmidt, S. Altman, S. Anadkat, et al.\\nGpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\nR.Agarwal,N.Vieillard,Y.Zhou,P.Stanczyk,S.R.\\nGarea, M. Geist, and O. Bachem. On-policy\\ndistillation of language models: Learning from\\nself-generated mistakes. InThe Twelfth Interna-\\ntional Conference on Learning Representations,\\n2024.\\nAI@Meta. Llama 3 model card, 2024.\\nURL https://github.com/meta-llama/\\nllama3/blob/main/MODEL_CARD.md.\\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\\nskiy, F. Lebr√≥n, and S. Sanghai. Gqa: Training\\ngeneralized multi-query transformer models\\nfrom multi-head checkpoints. arXiv preprint\\narXiv:2305.13245, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\\npelli, R. Cojocaru, M. Debbah, √âtienne Goffinet,\\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='arXiv:2305.13245, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\\npelli, R. Cojocaru, M. Debbah, √âtienne Goffinet,\\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,\\nB. Noune, B. Pannier, and G. Penedo. The fal-\\ncon series of open language models, 2023.\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\\nZ. Chen, et al. Palm 2 technical report.arXiv\\npreprint arXiv:2305.10403, 2023.\\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\\nH. Michalewski, D. Dohan, E. Jiang, C. J.\\nCai, M. Terry, Q. V. Le, and C. Sutton. Pro-\\ngram synthesis with large language models.\\nCoRR, abs/2108.07732, 2021. URL https:\\n//arxiv.org/abs/2108.07732.\\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu. Path-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nI.Bello, H.Pham, Q.V.Le, M.Norouzi, andS.Ben-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='S. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu. Path-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nI.Bello, H.Pham, Q.V.Le, M.Norouzi, andS.Ben-\\ngio. Neural combinatorial optimization with re-\\ninforcement learning.CoRR, abs/1611.09940,\\n2016. URL http://arxiv.org/abs/1611.\\n09940.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer.arXiv\\npreprint arXiv:2004.05150, 2020a.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer.CoRR,\\nabs/2004.05150, 2020b. URL https://\\narxiv.org/abs/2004.05150.\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\\nplan, P. Dhariwal, A. Neelakantan, P. Shyam,\\nG.Sastry, A.Askell, S.Agarwal, A.Herbert-Voss,\\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse,\\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\\nI. Sutskever, and D. Amodei. Language models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='D. M. Ziegler, J. Wu, C. Winter, C. Hesse,\\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\\nI. Sutskever, and D. Amodei. Language models\\nare few-shot learners.CoRR, abs/2005.14165,\\n2020. URLhttps://arxiv.org/abs/2005.\\n14165.\\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee,\\nF. Tramer, and C. Zhang. Quantifying memo-\\nrization across neural language models.arXiv\\npreprint arXiv:2202.07646, 2022.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\\nde Oliveira Pinto, J. Kaplan, H. Edwards,\\nY. Burda, N. Joseph, G. Brockman, A. Ray,\\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\\nG. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ry-\\nder, M. Pavlov, A. Power, L. Kaiser, M. Bavar-\\nian, C. Winter, P. Tillet, F. P. Such, D. Cum-\\nmings, M. Plappert, F. Chantzis, E. Barnes,\\nA.Herbert-Voss, W.H.Guss, A.Nichol, A.Paino,\\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='A.Herbert-Voss, W.H.Guss, A.Nichol, A.Paino,\\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,\\nA.Radford, M.Knight, M.Brundage, M.Murati,\\nK. Mayer, P. Welinder, B. McGrew, D. Amodei,\\nS. McCandlish, I. Sutskever, and W. Zaremba.\\nEvaluating large language models trained on\\ncode. CoRR, abs/2107.03374, 2021. URL\\nhttps://arxiv.org/abs/2107.03374.\\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. An-\\ngelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chat-\\nbot arena: An open platform for evaluating\\nllms by human preference, 2024.\\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\\nM. Collins, and K. Toutanova. Boolq: Explor-\\ning the surprising difficulty of natural yes/no\\nquestions. CoRR, abs/1905.10044, 2019. URL\\nhttp://arxiv.org/abs/1905.10044.\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\\nH. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, C. Hesse, and J. Schul-\\nman. Training verifiers to solve math word\\nproblems. CoRR, abs/2110.14168, 2021. URL\\nhttps://arxiv.org/abs/2110.14168.\\nGemini Team. Gemini: A family of highly capable\\nmultimodal models, 2023.\\nGemini Team. Gemini 1.5: Unlocking multimodal\\nunderstanding across millions of tokens of con-\\ntext, 2024.\\nGemma Team. Gemma: Open models based on\\ngemini research and technology, 2024.\\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\\nKnowledge distillation of large language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='text, 2024.\\nGemma Team. Gemma: Open models based on\\ngemini research and technology, 2024.\\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\\nKnowledge distillation of large language mod-\\nels. InThe Twelfth International Conference on\\nLearning Representations, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou,\\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understand-\\ning. CoRR, abs/2009.03300, 2020. URL\\nhttps://arxiv.org/abs/2009.03300.\\nG. Hinton, O. Vinyals, and J. Dean. Distilling the\\nknowledge in a neural network.arXiv preprint\\narXiv:1503.02531, 2015.\\nJ. Hoffmann, S. Borgeaud, A. Mensch,\\nE. Buchatskaya, T. Cai, E. Rutherford, D. d. L.\\nCasas, L. A. Hendricks, J. Welbl, A. Clark, et al.\\nTraining compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556,\\n2022.\\nD. Ippolito, F. Tram√®r, M. Nasr, C. Zhang,\\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\\nN. Carlini. Preventing verbatim memorization\\nin language models gives a false sense of pri-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2022.\\nD. Ippolito, F. Tram√®r, M. Nasr, C. Zhang,\\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\\nN. Carlini. Preventing verbatim memorization\\nin language models gives a false sense of pri-\\nvacy. arXiv preprint arXiv:2210.17546, 2022.\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\\nford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG.Lengyel, G.Lample, L.Saulnier, L.R.Lavaud,\\nM.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral\\n7b, 2023.\\nM. Kahng, I. Tenney, M. Pushkarna, M. X. Liu,\\nJ. Wexler, E. Reif, K. Kallarackal, M. Chang,\\nM. Terry, and L. Dixon. Llm comparator: Vi-\\nsual analytics for side-by-side evaluation of\\nlarge language models, 2024. URL https:\\n//arxiv.org/abs/2402.10524.\\nM. Kinniment, L. J. K. Sato, H. Du, B. Goodrich,\\nM.Hasin,L.Chan,L.H.Miles,T.R.Lin,H.Wijk,\\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\\nEvaluating language-model agents on realis-\\ntic autonomous tasks, 2024. URLhttps://\\narxiv.org/abs/2312.11671.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='M.Hasin,L.Chan,L.H.Miles,T.R.Lin,H.Wijk,\\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\\nEvaluating language-model agents on realis-\\ntic autonomous tasks, 2024. URLhttps://\\narxiv.org/abs/2312.11671.\\nT. Kudo and J. Richardson. SentencePiece: A\\nsimple and language independent subword to-\\nkenizeranddetokenizerforneuraltextprocess-\\ning. InE.BlancoandW.Lu,editors, Proceedings\\nof the 2018 Conference on Empirical Methods in\\nNatural Language Processing: System Demon-\\nstrations, pages 66‚Äì71, Brussels, Belgium, Nov.\\n2018. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/D18-2012. URL\\nhttps://aclanthology.org/D18-2012.\\nS. Kudugunta, I. Caswell, B. Zhang, X. Garcia,\\nC. A. Choquette-Choo, K. Lee, D. Xin, A. Kusu-\\npati, R. Stella, A. Bapna, et al. Madlad-400:\\nA multilingual and document-level large au-\\ndited dataset.arXiv preprint arXiv:2309.04662,\\n2023.\\nT. Kwiatkowski, J. Palomaki, O. Redfield,\\nM. Collins, A. Parikh, C. Alberti, D. Epstein,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='A multilingual and document-level large au-\\ndited dataset.arXiv preprint arXiv:2309.04662,\\n2023.\\nT. Kwiatkowski, J. Palomaki, O. Redfield,\\nM. Collins, A. Parikh, C. Alberti, D. Epstein,\\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai,\\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural ques-\\ntions: A benchmark for question answering\\nresearch. Transactions of the Association for\\nComputational Linguistics, 7:452‚Äì466, 2019.\\ndoi: 10.1162/tacl_a_00276. URL https://\\naclanthology.org/Q19-1026.\\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: De-\\nmystifying real-world large language model in-\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\ntegrated malicious services, 2024. URLhttps:\\n//arxiv.org/abs/2401.03315.\\nM. Luong, H. Pham, and C. D. Manning. Effective\\napproaches to attention-based neural machine\\ntranslation. CoRR,abs/1508.04025, 2015. URL\\nhttp://arxiv.org/abs/1508.04025.\\nMacknight, Aung, and Gomes. Personal Commu-\\nnication, 2024.\\nM. Mozes, J. Hoffmann, K. Tomanek, M. Kouate,\\nN. Thain, A. Yuan, T. Bolukbasi, and L. Dixon.\\nTowards agile text classifiers for everyone,\\n2023. URLhttps://arxiv.org/abs/2302.\\n06541.\\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.\\nCooper, D. Ippolito, C. A. Choquette-Choo,\\nE. Wallace, F. Tram√®r, and K. Lee. Scal-\\nable extraction of training data from (pro-\\nduction) language models. arXiv preprint\\narXiv:2311.17035, 2023.\\nM. Phuong, M. Aitchison, E. Catt, S. Co-\\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='M. Phuong, M. Aitchison, E. Catt, S. Co-\\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,\\nL. Ho, S. Lin, S. Farquhar, M. Hutter, G. Dele-\\ntang, A. Ruoss, S. El-Sayed, S. Brown, A. Dra-\\ngan, R. Shah, A. Dafoe, and T. Shevlane. Evalu-\\natingfrontiermodelsfordangerouscapabilities,\\n2024. URLhttps://arxiv.org/abs/2403.\\n13793.\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\\nand I. Sutskever. Language models are unsu-\\npervised multitask learners, 2019.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee,\\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\\nLiu. Exploring the limits of transfer learning\\nwith a unified text-to-text transformer.CoRR,\\nabs/1910.10683, 2019. URLhttp://arxiv.\\norg/abs/1910.10683.\\nA. Ram√©, J. Ferret, N. Vieillard, R. Dadashi,\\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\\nA. Douillard, and O. Bachem. Warp: On the\\nbenefits of weight averaged rewarded policies,\\n2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='A. Ram√©, J. Ferret, N. Vieillard, R. Dadashi,\\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\\nA. Douillard, and O. Bachem. Warp: On the\\nbenefits of weight averaged rewarded policies,\\n2024.\\nJ. Ren, S. Rajbhandari, R. Y. Aminabadi,\\nO. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.\\n{Zero-offload}: Democratizing{billion-scale}\\nmodel training. In2021 USENIX Annual Tech-\\nnical Conference (USENIX ATC 21), pages 551‚Äì\\n564, 2021.\\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya,\\nJ. Bradbury, D. Andor, S. Narang, B. Lester,\\nC. Gaffney, A. Mohiuddin, et al. Scaling up\\nmodels and data with t5x and seqio. Jour-\\nnal of Machine Learning Research, 24(377):1‚Äì8,\\n2023.\\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\\nY. Choi. WINOGRANDE: an adversarial\\nwinograd schema challenge at scale. CoRR,\\nabs/1907.10641, 2019. URLhttp://arxiv.\\norg/abs/1907.10641.\\nN. Shazeer. GLU variants improve transformer.\\nCoRR, abs/2002.05202, 2020. URL https:\\n//arxiv.org/abs/2002.05202.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='abs/1907.10641, 2019. URLhttp://arxiv.\\norg/abs/1907.10641.\\nN. Shazeer. GLU variants improve transformer.\\nCoRR, abs/2002.05202, 2020. URL https:\\n//arxiv.org/abs/2002.05202.\\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Mar-\\nchal, M. Anderljung, N. Kolt, L. Ho, D. Sid-\\ndarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel,\\nV. Bolina, J. Clark, Y. Bengio, P. Christiano, and\\nA. Dafoe. Model evaluation for extreme risks,\\n2023. URLhttps://arxiv.org/abs/2305.\\n15324.\\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer:\\nEnhanced transformer with rotary position em-\\nbedding. CoRR, abs/2104.09864, 2021. URL\\nhttps://arxiv.org/abs/2104.09864.\\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann,\\nY. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\\nE. H. Chi, D. Zhou, and J. Wei. Challenging\\nbig-bench tasks and whether chain-of-thought\\ncan solve them, 2022.\\nQ. Team. Introducing qwen1.5, February\\n2024. URL https://qwenlm.github.io/\\nblog/qwen1.5/.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='big-bench tasks and whether chain-of-thought\\ncan solve them, 2022.\\nQ. Team. Introducing qwen1.5, February\\n2024. URL https://qwenlm.github.io/\\nblog/qwen1.5/.\\nI. Tenney, J. Wexler, J. Bastings, T. Boluk-\\nbasi, A. Coenen, S. Gehrmann, E. Jiang,\\nM. Pushkarna, C. Radebaugh, E. Reif, and\\nA. Yuan. The language interpretability tool: Ex-\\ntensible, interactive visualizations and analysis\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nfor nlp models, 2020. URLhttps://arxiv.\\norg/abs/2008.05122.\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-\\nA. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\\nE. Grave, and G. Lample. Llama: Open and\\nefficient foundation language models, 2023.\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\\nsukhin. Attention is all you need. CoRR,\\nabs/1706.03762, 2017. URLhttp://arxiv.\\norg/abs/1706.03762.\\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown,\\nW. Hawkins, T. Stepleton, C. Biles, A. Birhane,\\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac,\\nS. Legassick, G. Irving, and I. Gabriel. Ethical\\nand social risks of harm from language mod-\\nels, 2021. URL https://arxiv.org/abs/\\n2112.04359.\\nxAI. grok-1, 2024. URLhttps://github.com/\\nxai-org/grok-1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='and social risks of harm from language mod-\\nels, 2021. URL https://arxiv.org/abs/\\n2112.04359.\\nxAI. grok-1, 2024. URLhttps://github.com/\\nxai-org/grok-1.\\nXLA. Xla: Optimizing compiler for tensor-\\nflow, 2019. URLhttps://www.tensorflow.\\norg/xla.\\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang,\\nR. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Mag-\\ngioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\\nY. Wu, and Z. Chen. GSPMD: general and\\nscalable parallelization for ML computation\\ngraphs. CoRR, abs/2105.04663, 2021. URL\\nhttps://arxiv.org/abs/2105.04663.\\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao.\\nIntercode: Standardizing and benchmarking\\ninteractive coding with execution feedback,\\n2023. URLhttps://arxiv.org/abs/2306.\\n14898.\\nB. Zhang and R. Sennrich. Root mean square\\nlayer normalization. CoRR, abs/1910.07467,\\n2019. URL http://arxiv.org/abs/1910.\\n07467.\\nL.Zheng, W.-L.Chiang, Y.Sheng, T.Li, S.Zhuang,\\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\\net al. Lmsys-chat-1m: A large-scale real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2019. URL http://arxiv.org/abs/1910.\\n07467.\\nL.Zheng, W.-L.Chiang, Y.Sheng, T.Li, S.Zhuang,\\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\\net al. Lmsys-chat-1m: A large-scale real-\\nworld llm conversation dataset.arXiv preprint\\narXiv:2309.11998, 2023.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='GloVe: Global Vectors for Word Representation\\nJeffrey Pennington, Richard Socher, Christopher D. Manning\\nComputer Science Department, Stanford University, Stanford, CA 94305\\njpennin@stanford.edu, richard@socher.org, manning@stanford.edu\\nAbstract\\nRecent methods for learning vector space\\nrepresentations of words have succeeded\\nin capturing Ô¨Åne-grained semantic and\\nsyntactic regularities using vector arith-\\nmetic, but the origin of these regularities\\nhas remained opaque. We analyze and\\nmake explicit the model properties needed\\nfor such regularities to emerge in word\\nvectors. The result is a new global log-\\nbilinear regression model that combines\\nthe advantages of the two major model\\nfamilies in the literature: global matrix\\nfactorization and local context window\\nmethods. Our model efÔ¨Åciently leverages\\nstatistical information by training only on\\nthe nonzero elements in a word-word co-\\noccurrence matrix, rather than on the en-\\ntire sparse matrix or on individual context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='statistical information by training only on\\nthe nonzero elements in a word-word co-\\noccurrence matrix, rather than on the en-\\ntire sparse matrix or on individual context\\nwindows in a large corpus. The model pro-\\nduces a vector space with meaningful sub-\\nstructure, as evidenced by its performance\\nof 75% on a recent word analogy task. It\\nalso outperforms related models on simi-\\nlarity tasks and named entity recognition.\\n1 Introduction\\nSemantic vector space models of language repre-\\nsent each word with a real-valued vector. These\\nvectors can be used as features in a variety of ap-\\nplications, such as information retrieval (Manning\\net al., 2008), document classiÔ¨Åcation (Sebastiani,\\n2002), question answering (Tellex et al., 2003),\\nnamed entity recognition (Turian et al., 2010), and\\nparsing (Socher et al., 2013).\\nMost word vector methods rely on the distance\\nor angle between pairs of word vectors as the pri-\\nmary method for evaluating the intrinsic quality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='parsing (Socher et al., 2013).\\nMost word vector methods rely on the distance\\nor angle between pairs of word vectors as the pri-\\nmary method for evaluating the intrinsic quality\\nof such a set of word representations. Recently,\\nMikolov et al. (2013c) introduced a new evalua-\\ntion scheme based on word analogies that probes\\nthe Ô¨Åner structure of the word vector space by ex-\\namining not the scalar distance between word vec-\\ntors, but rather their various dimensions of dif-\\nference. For example, the analogy ‚Äúking is to\\nqueen as man is to woman‚Äù should be encoded\\nin the vector space by the vector equation king ‚àí\\nqueen = man ‚àíwoman. This evaluation scheme\\nfavors models that produce dimensions of mean-\\ning, thereby capturing the multi-clustering idea of\\ndistributed representations (Bengio, 2009).\\nThe two main model families for learning word\\nvectors are: 1) global matrix factorization meth-\\nods, such as latent semantic analysis (LSA) (Deer-\\nwester et al., 1990) and 2) local context window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='The two main model families for learning word\\nvectors are: 1) global matrix factorization meth-\\nods, such as latent semantic analysis (LSA) (Deer-\\nwester et al., 1990) and 2) local context window\\nmethods, such as the skip-gram model of Mikolov\\net al. (2013c). Currently, both families suffer sig-\\nniÔ¨Åcant drawbacks. While methods like LSA ef-\\nÔ¨Åciently leverage statistical information, they do\\nrelatively poorly on the word analogy task, indi-\\ncating a sub-optimal vector space structure. Meth-\\nods like skip-gram may do better on the analogy\\ntask, but they poorly utilize the statistics of the cor-\\npus since they train on separate local context win-\\ndows instead of on global co-occurrence counts.\\nIn this work, we analyze the model properties\\nnecessary to produce linear directions of meaning\\nand argue that global log-bilinear regression mod-\\nels are appropriate for doing so. We propose a spe-\\nciÔ¨Åc weighted least squares model that trains on\\nglobal word-word co-occurrence counts and thus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and argue that global log-bilinear regression mod-\\nels are appropriate for doing so. We propose a spe-\\nciÔ¨Åc weighted least squares model that trains on\\nglobal word-word co-occurrence counts and thus\\nmakes efÔ¨Åcient use of statistics. The model pro-\\nduces a word vector space with meaningful sub-\\nstructure, as evidenced by its state-of-the-art per-\\nformance of 75% accuracy on the word analogy\\ndataset. We also demonstrate that our methods\\noutperform other current methods on several word\\nsimilarity tasks, and also on a common named en-\\ntity recognition (NER) benchmark.\\nWe provide the source code for the model as\\nwell as trained word vectors at http://nlp.\\nstanford.edu/projects/glove/.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\nMatrix Factorization Methods. Matrix factor-\\nization methods for generating low-dimensional\\nword representations have roots stretching as far\\nback as LSA. These methods utilize low-rank ap-\\nproximations to decompose large matrices that\\ncapture statistical information about a corpus. The\\nparticular type of information captured by such\\nmatrices varies by application. In LSA, the ma-\\ntrices are of ‚Äúterm-document‚Äù type, i.e., the rows\\ncorrespond to words or terms, and the columns\\ncorrespond to different documents in the corpus.\\nIn contrast, the Hyperspace Analogue to Language\\n(HAL) (Lund and Burgess, 1996), for example,\\nutilizes matrices of ‚Äúterm-term‚Äù type, i.e., the rows\\nand columns correspond to words and the entries\\ncorrespond to the number of times a given word\\noccurs in the context of another given word.\\nA main problem with HAL and related meth-\\nods is that the most frequent words contribute a\\ndisproportionate amount to the similarity measure:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='occurs in the context of another given word.\\nA main problem with HAL and related meth-\\nods is that the most frequent words contribute a\\ndisproportionate amount to the similarity measure:\\nthe number of times two words co-occur with the\\nor and, for example, will have a large effect on\\ntheir similarity despite conveying relatively little\\nabout their semantic relatedness. A number of\\ntechniques exist that addresses this shortcoming of\\nHAL, such as the COALS method (Rohde et al.,\\n2006), in which the co-occurrence matrix is Ô¨Årst\\ntransformed by an entropy- or correlation-based\\nnormalization. An advantage of this type of trans-\\nformation is that the raw co-occurrence counts,\\nwhich for a reasonably sized corpus might span\\n8 or 9 orders of magnitude, are compressed so as\\nto be distributed more evenly in a smaller inter-\\nval. A variety of newer models also pursue this\\napproach, including a study (Bullinaria and Levy,\\n2007) that indicates that positive pointwise mu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='to be distributed more evenly in a smaller inter-\\nval. A variety of newer models also pursue this\\napproach, including a study (Bullinaria and Levy,\\n2007) that indicates that positive pointwise mu-\\ntual information (PPMI) is a good transformation.\\nMore recently, a square root type transformation\\nin the form of Hellinger PCA (HPCA) (Lebret and\\nCollobert, 2014) has been suggested as an effec-\\ntive way of learning word representations.\\nShallow Window-Based Methods. Another\\napproach is to learn word representations that aid\\nin making predictions within local context win-\\ndows. For example, Bengio et al. (2003) intro-\\nduced a model that learns word vector representa-\\ntions as part of a simple neural network architec-\\nture for language modeling. Collobert and Weston\\n(2008) decoupled the word vector training from\\nthe downstream training objectives, which paved\\nthe way for Collobert et al. (2011) to use the full\\ncontext of a word for learning the word represen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='(2008) decoupled the word vector training from\\nthe downstream training objectives, which paved\\nthe way for Collobert et al. (2011) to use the full\\ncontext of a word for learning the word represen-\\ntations, rather than just the preceding context as is\\nthe case with language models.\\nRecently, the importance of the full neural net-\\nwork structure for learning useful word repre-\\nsentations has been called into question. The\\nskip-gram and continuous bag-of-words (CBOW)\\nmodels of Mikolov et al. (2013a) propose a sim-\\nple single-layer architecture based on the inner\\nproduct between two word vectors. Mnih and\\nKavukcuoglu (2013) also proposed closely-related\\nvector log-bilinear models, vLBL and ivLBL, and\\nLevy et al. (2014) proposed explicit word embed-\\ndings based on a PPMI metric.\\nIn the skip-gram and ivLBL models, the objec-\\ntive is to predict a word‚Äôs context given the word\\nitself, whereas the objective in the CBOW and\\nvLBL models is to predict a word given its con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='In the skip-gram and ivLBL models, the objec-\\ntive is to predict a word‚Äôs context given the word\\nitself, whereas the objective in the CBOW and\\nvLBL models is to predict a word given its con-\\ntext. Through evaluation on a word analogy task,\\nthese models demonstrated the capacity to learn\\nlinguistic patterns as linear relationships between\\nthe word vectors.\\nUnlike the matrix factorization methods, the\\nshallow window-based methods suffer from the\\ndisadvantage that they do not operate directly on\\nthe co-occurrence statistics of the corpus. Instead,\\nthese models scan context windows across the en-\\ntire corpus, which fails to take advantage of the\\nvast amount of repetition in the data.\\n3 The GloVe Model\\nThe statistics of word occurrences in a corpus is\\nthe primary source of information available to all\\nunsupervised methods for learning word represen-\\ntations, and although many such methods now ex-\\nist, the question still remains as to how meaning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='the primary source of information available to all\\nunsupervised methods for learning word represen-\\ntations, and although many such methods now ex-\\nist, the question still remains as to how meaning\\nis generated from these statistics, and how the re-\\nsulting word vectors might represent that meaning.\\nIn this section, we shed some light on this ques-\\ntion. We use our insights to construct a new model\\nfor word representation which we call GloVe, for\\nGlobal Vectors, because the global corpus statis-\\ntics are captured directly by the model.\\nFirst we establish some notation. Let the matrix\\nof word-word co-occurrence counts be denoted by\\nX, whose entries Xi jtabulate the number of times\\nword j occurs in the context of word i. Let Xi =‚àë\\nk Xik be the number of times any word appears\\nin the context of wordi. Finally, let Pi j = P( j|i) =\\nXi j/Xi be the probability that word j appear in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Table 1: Co-occurrence probabilities for target wordsice and steam with selected context words from a 6\\nbillion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion\\ncancel out, so that large values (much greater than 1) correlate well with properties speciÔ¨Åc to ice, and\\nsmall values (much less than 1) correlate well with properties speciÔ¨Åc of steam.\\nProbability and Ratio k = solid k = gas k = water k = fashion\\nP(k|ice) 1.9 √ó10‚àí4 6.6 √ó10‚àí5 3.0 √ó10‚àí3 1.7 √ó10‚àí5\\nP(k|steam) 2.2 √ó10‚àí5 7.8 √ó10‚àí4 2.2 √ó10‚àí3 1.8 √ó10‚àí5\\nP(k|ice)/P(k|steam) 8.9 8 .5 √ó10‚àí2 1.36 0 .96\\ncontext of word i.\\nWe begin with a simple example that showcases\\nhow certain aspects of meaning can be extracted\\ndirectly from co-occurrence probabilities. Con-\\nsider two words i and j that exhibit a particular as-\\npect of interest; for concreteness, suppose we are\\ninterested in the concept of thermodynamic phase,\\nfor which we might take i = ice and j = steam.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='pect of interest; for concreteness, suppose we are\\ninterested in the concept of thermodynamic phase,\\nfor which we might take i = ice and j = steam.\\nThe relationship of these words can be examined\\nby studying the ratio of their co-occurrence prob-\\nabilities with various probe words, k. For words\\nk related to ice but not steam, say k = solid, we\\nexpect the ratio Pik /Pjk will be large. Similarly,\\nfor words k related to steam but not ice, say k =\\ngas, the ratio should be small. For words k like\\nwater or fashion, that are either related to both ice\\nand steam, or to neither, the ratio should be close\\nto one. Table 1 shows these probabilities and their\\nratios for a large corpus, and the numbers conÔ¨Årm\\nthese expectations. Compared to the raw probabil-\\nities, the ratio is better able to distinguish relevant\\nwords (solid and gas) from irrelevant words (water\\nand fashion) and it is also better able to discrimi-\\nnate between the two relevant words.\\nThe above argument suggests that the appropri-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='words (solid and gas) from irrelevant words (water\\nand fashion) and it is also better able to discrimi-\\nnate between the two relevant words.\\nThe above argument suggests that the appropri-\\nate starting point for word vector learning should\\nbe with ratios of co-occurrence probabilities rather\\nthan the probabilities themselves. Noting that the\\nratio Pik /Pjk depends on three words i, j, and k,\\nthe most general model takes the form,\\nF(wi,wj , Àúwk ) = Pik\\nPjk\\n, (1)\\nwhere w ‚àà Rd are word vectors and Àú w ‚àà Rd\\nare separate context word vectors whose role will\\nbe discussed in Section 4.2. In this equation, the\\nright-hand side is extracted from the corpus, and\\nF may depend on some as-of-yet unspeciÔ¨Åed pa-\\nrameters. The number of possibilities forF is vast,\\nbut by enforcing a few desiderata we can select a\\nunique choice. First, we would like F to encode\\nthe information present the ratio Pik /Pjk in the\\nword vector space. Since vector spaces are inher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='but by enforcing a few desiderata we can select a\\nunique choice. First, we would like F to encode\\nthe information present the ratio Pik /Pjk in the\\nword vector space. Since vector spaces are inher-\\nently linear structures, the most natural way to do\\nthis is with vector differences. With this aim, we\\ncan restrict our consideration to those functions F\\nthat depend only on the difference of the two target\\nwords, modifying Eqn. (1) to,\\nF(wi ‚àíwj , Àúwk ) = Pik\\nPjk\\n. (2)\\nNext, we note that the arguments of F in Eqn. (2)\\nare vectors while the right-hand side is a scalar.\\nWhile F could be taken to be a complicated func-\\ntion parameterized by, e.g., a neural network, do-\\ning so would obfuscate the linear structure we are\\ntrying to capture. To avoid this issue, we can Ô¨Årst\\ntake the dot product of the arguments,\\nF\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= Pik\\nPjk\\n, (3)\\nwhich prevents F from mixing the vector dimen-\\nsions in undesirable ways. Next, note that for\\nword-word co-occurrence matrices, the distinction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='F\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= Pik\\nPjk\\n, (3)\\nwhich prevents F from mixing the vector dimen-\\nsions in undesirable ways. Next, note that for\\nword-word co-occurrence matrices, the distinction\\nbetween a word and a context word is arbitrary and\\nthat we are free to exchange the two roles. To do so\\nconsistently, we must not only exchange w ‚Üî Àúw\\nbut also X ‚ÜîXT . Our Ô¨Ånal model should be in-\\nvariant under this relabeling, but Eqn. (3) is not.\\nHowever, the symmetry can be restored in two\\nsteps. First, we require that F be a homomorphism\\nbetween the groups (R,+) and (R>0,√ó), i.e.,\\nF\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= F(wT\\ni Àúwk )\\nF(wT\\nj Àúwk ) , (4)\\nwhich, by Eqn. (3), is solved by,\\nF(wT\\ni Àúwk ) = Pik = Xik\\nXi\\n. (5)\\nThe solution to Eqn. (4) is F = exp, or,\\nwT\\ni Àúwk = log(Pik ) = log(Xik ) ‚àílog(Xi ) . (6)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Next, we note that Eqn. (6) would exhibit the ex-\\nchange symmetry if not for the log( Xi ) on the\\nright-hand side. However, this term is indepen-\\ndent of k so it can be absorbed into a bias bi for\\nwi. Finally, adding an additional bias Àúbk for Àúwk\\nrestores the symmetry,\\nwT\\ni Àúwk + bi + Àúbk = log(Xik ) . (7)\\nEqn. (7) is a drastic simpliÔ¨Åcation over Eqn. (1),\\nbut it is actually ill-deÔ¨Åned since the logarithm di-\\nverges whenever its argument is zero. One reso-\\nlution to this issue is to include an additive shift\\nin the logarithm, log( Xik ) ‚Üílog(1 + Xik ), which\\nmaintains the sparsity of X while avoiding the di-\\nvergences. The idea of factorizing the log of the\\nco-occurrence matrix is closely related to LSA and\\nwe will use the resulting model as a baseline in\\nour experiments. A main drawback to this model\\nis that it weighs all co-occurrences equally, even\\nthose that happen rarely or never. Such rare co-\\noccurrences are noisy and carry less information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='our experiments. A main drawback to this model\\nis that it weighs all co-occurrences equally, even\\nthose that happen rarely or never. Such rare co-\\noccurrences are noisy and carry less information\\nthan the more frequent ones ‚Äî yet even just the\\nzero entries account for 75‚Äì95% of the data in X,\\ndepending on the vocabulary size and corpus.\\nWe propose a new weighted least squares re-\\ngression model that addresses these problems.\\nCasting Eqn. (7) as a least squares problem and\\nintroducing a weighting function f (Xi j) into the\\ncost function gives us the model\\nJ =\\nV‚àë\\ni,j=1\\nf\\n(\\nXi j\\n) (\\nwT\\ni Àúwj + bi + Àúbj ‚àílog Xi j\\n)2\\n,\\n(8)\\nwhere V is the size of the vocabulary. The weight-\\ning function should obey the following properties:\\n1. f (0) = 0. If f is viewed as a continuous\\nfunction, it should vanish as x ‚Üí 0 fast\\nenough that the limx‚Üí0 f (x) log2 x is Ô¨Ånite.\\n2. f (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\n3. f (x) should be relatively small for large val-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='enough that the limx‚Üí0 f (x) log2 x is Ô¨Ånite.\\n2. f (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\n3. f (x) should be relatively small for large val-\\nues of x, so that frequent co-occurrences are\\nnot overweighted.\\nOf course a large number of functions satisfy these\\nproperties, but one class of functions that we found\\nto work well can be parameterized as,\\nf (x) =\\n{ (x/xmax)Œ± if x < xmax\\n1 otherwise . (9)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\nFigure 1: Weighting function f with Œ± = 3/4.\\nThe performance of the model depends weakly on\\nthe cutoff, which we Ô¨Åx to xmax = 100 for all our\\nexperiments. We found that Œ± = 3/4 gives a mod-\\nest improvement over a linear version with Œ± = 1.\\nAlthough we offer only empirical motivation for\\nchoosing the value 3/4, it is interesting that a sim-\\nilar fractional power scaling was found to give the\\nbest performance in (Mikolov et al., 2013a).\\n3.1 Relationship to Other Models\\nBecause all unsupervised methods for learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ilar fractional power scaling was found to give the\\nbest performance in (Mikolov et al., 2013a).\\n3.1 Relationship to Other Models\\nBecause all unsupervised methods for learning\\nword vectors are ultimately based on the occur-\\nrence statistics of a corpus, there should be com-\\nmonalities between the models. Nevertheless, cer-\\ntain models remain somewhat opaque in this re-\\ngard, particularly the recent window-based meth-\\nods like skip-gram and ivLBL. Therefore, in this\\nsubsection we show how these models are related\\nto our proposed model, as deÔ¨Åned in Eqn. (8).\\nThe starting point for the skip-gram or ivLBL\\nmethods is a model Qi j for the probability that\\nword j appears in the context of word i. For con-\\ncreteness, let us assume that Qi jis a softmax,\\nQi j = exp(wT\\ni Àúwj )\\n‚àëV\\nk=1 exp(wT\\ni Àúwk )\\n. (10)\\nMost of the details of these models are irrelevant\\nfor our purposes, aside from the the fact that they\\nattempt to maximize the log probability as a con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='i Àúwj )\\n‚àëV\\nk=1 exp(wT\\ni Àúwk )\\n. (10)\\nMost of the details of these models are irrelevant\\nfor our purposes, aside from the the fact that they\\nattempt to maximize the log probability as a con-\\ntext window scans over the corpus. Training pro-\\nceeds in an on-line, stochastic fashion, but the im-\\nplied global objective function can be written as,\\nJ = ‚àí\\n‚àë\\ni‚ààcorpus\\nj‚ààcontext(i)\\nlog Qi j. (11)\\nEvaluating the normalization factor of the soft-\\nmax for each term in this sum is costly. To al-\\nlow for efÔ¨Åcient training, the skip-gram and ivLBL\\nmodels introduce approximations to Qi j. How-\\never, the sum in Eqn. (11) can be evaluated much'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='more efÔ¨Åciently if we Ô¨Årst group together those\\nterms that have the same values for i and j,\\nJ = ‚àí\\nV‚àë\\ni=1\\nV‚àë\\nj=1\\nXi jlog Qi j, (12)\\nwhere we have used the fact that the number of\\nlike terms is given by the co-occurrence matrix X.\\nRecalling our notation for Xi = ‚àë\\nk Xik and\\nPi j = Xi j/Xi, we can rewrite J as,\\nJ = ‚àí\\nV‚àë\\ni=1\\nXi\\nV‚àë\\nj=1\\nPi jlog Qi j =\\nV‚àë\\ni=1\\nXi H(Pi,Qi ) ,\\n(13)\\nwhere H(Pi,Qi ) is the cross entropy of the dis-\\ntributions Pi and Qi, which we deÔ¨Åne in analogy\\nto Xi. As a weighted sum of cross-entropy error,\\nthis objective bears some formal resemblance to\\nthe weighted least squares objective of Eqn. (8).\\nIn fact, it is possible to optimize Eqn. (13) directly\\nas opposed to the on-line training methods used in\\nthe skip-gram and ivLBL models. One could inter-\\npret this objective as a ‚Äúglobal skip-gram‚Äù model,\\nand it might be interesting to investigate further.\\nOn the other hand, Eqn. (13) exhibits a number of\\nundesirable properties that ought to be addressed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and it might be interesting to investigate further.\\nOn the other hand, Eqn. (13) exhibits a number of\\nundesirable properties that ought to be addressed\\nbefore adopting it as a model for learning word\\nvectors.\\nTo begin, cross entropy error is just one among\\nmany possible distance measures between prob-\\nability distributions, and it has the unfortunate\\nproperty that distributions with long tails are of-\\nten modeled poorly with too much weight given\\nto the unlikely events. Furthermore, for the mea-\\nsure to be bounded it requires that the model dis-\\ntribution Q be properly normalized. This presents\\na computational bottleneck owing to the sum over\\nthe whole vocabulary in Eqn. (10), and it would be\\ndesirable to consider a different distance measure\\nthat did not require this property of Q. A natural\\nchoice would be a least squares objective in which\\nnormalization factors in Q and P are discarded,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n( ÀÜPi j‚àí ÀÜQi j\\n)2 (14)\\nwhere ÀÜPi j = Xi j and ÀÜQi j = exp(wT\\ni Àúwj ) are the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='choice would be a least squares objective in which\\nnormalization factors in Q and P are discarded,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n( ÀÜPi j‚àí ÀÜQi j\\n)2 (14)\\nwhere ÀÜPi j = Xi j and ÀÜQi j = exp(wT\\ni Àúwj ) are the\\nunnormalized distributions. At this stage another\\nproblem emerges, namely thatXi joften takes very\\nlarge values, which can complicate the optimiza-\\ntion. An effective remedy is to minimize the\\nsquared error of the logarithms of ÀÜP and ÀÜQ instead,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n(log ÀÜPi j‚àílog ÀÜQi j\\n)2\\n=\\n‚àë\\ni,j\\nXi\\n(wT\\ni Àúwj ‚àílog Xi j\\n)2 . (15)\\nFinally, we observe that while the weighting factor\\nXi is preordained by the on-line training method\\ninherent to the skip-gram and ivLBL models, it is\\nby no means guaranteed to be optimal. In fact,\\nMikolov et al. (2013a) observe that performance\\ncan be increased by Ô¨Åltering the data so as to re-\\nduce the effective value of the weighting factor for\\nfrequent words. With this in mind, we introduce\\na more general weighting function, which we are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='can be increased by Ô¨Åltering the data so as to re-\\nduce the effective value of the weighting factor for\\nfrequent words. With this in mind, we introduce\\na more general weighting function, which we are\\nfree to take to depend on the context word as well.\\nThe result is,\\nÀÜJ =\\n‚àë\\ni,j\\nf (Xi j)(wT\\ni Àúwj ‚àílog Xi j\\n)2 , (16)\\nwhich is equivalent 1 to the cost function of\\nEqn. (8), which we derived previously.\\n3.2 Complexity of the model\\nAs can be seen from Eqn. (8) and the explicit form\\nof the weighting function f (X), the computational\\ncomplexity of the model depends on the number of\\nnonzero elements in the matrix X. As this num-\\nber is always less than the total number of en-\\ntries of the matrix, the model scales no worse than\\nO(|V |2). At Ô¨Årst glance this might seem like a sub-\\nstantial improvement over the shallow window-\\nbased approaches, which scale with the corpus\\nsize, |C|. However, typical vocabularies have hun-\\ndreds of thousands of words, so that |V |2 can be in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='stantial improvement over the shallow window-\\nbased approaches, which scale with the corpus\\nsize, |C|. However, typical vocabularies have hun-\\ndreds of thousands of words, so that |V |2 can be in\\nthe hundreds of billions, which is actually much\\nlarger than most corpora. For this reason it is im-\\nportant to determine whether a tighter bound can\\nbe placed on the number of nonzero elements of\\nX.\\nIn order to make any concrete statements about\\nthe number of nonzero elements in X, it is neces-\\nsary to make some assumptions about the distribu-\\ntion of word co-occurrences. In particular, we will\\nassume that the number of co-occurrences of word\\ni with word j, Xi j, can be modeled as a power-law\\nfunction of the frequency rank of that word pair,\\nri j:\\nXi j = k\\n(ri j)Œ± . (17)\\n1We could also include bias terms in Eqn. (16).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='The total number of words in the corpus is pro-\\nportional to the sum over all elements of the co-\\noccurrence matrix X,\\n|C|‚àº\\n‚àë\\ni j\\nXi j =\\n|X |‚àë\\nr=1\\nk\\nrŒ± = kH |X |,Œ± , (18)\\nwhere we have rewritten the last sum in terms of\\nthe generalized harmonic number Hn,m. The up-\\nper limit of the sum, |X |, is the maximum fre-\\nquency rank, which coincides with the number of\\nnonzero elements in the matrix X. This number is\\nalso equal to the maximum value of r in Eqn. (17)\\nsuch that Xi j ‚â•1, i.e., |X |= k1/Œ±. Therefore we\\ncan write Eqn. (18) as,\\n|C|‚àº| X |Œ± H|X |,Œ± . (19)\\nWe are interested in how|X |is related to |C|when\\nboth numbers are large; therefore we are free to\\nexpand the right hand side of the equation for large\\n|X |. For this purpose we use the expansion of gen-\\neralized harmonic numbers (Apostol, 1976),\\nHx,s = x1‚àís\\n1 ‚àís + Œ∂(s) + O(x‚àís ) if s >0, s , 1 ,\\n(20)\\ngiving,\\n|C|‚àº |X |\\n1 ‚àíŒ± + Œ∂(Œ±) |X |Œ± + O(1) , (21)\\nwhere Œ∂(s) is the Riemann zeta function. In the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='eralized harmonic numbers (Apostol, 1976),\\nHx,s = x1‚àís\\n1 ‚àís + Œ∂(s) + O(x‚àís ) if s >0, s , 1 ,\\n(20)\\ngiving,\\n|C|‚àº |X |\\n1 ‚àíŒ± + Œ∂(Œ±) |X |Œ± + O(1) , (21)\\nwhere Œ∂(s) is the Riemann zeta function. In the\\nlimit that X is large, only one of the two terms on\\nthe right hand side of Eqn. (21) will be relevant,\\nand which term that is depends on whether Œ± >1,\\n|X |=\\n{ O(|C|) if Œ± <1,\\nO(|C|1/Œ±) if Œ± >1. (22)\\nFor the corpora studied in this article, we observe\\nthat Xi j is well-modeled by Eqn. (17) with Œ± =\\n1.25. In this case we have that |X |= O(|C|0.8).\\nTherefore we conclude that the complexity of the\\nmodel is much better than the worst case O(V2),\\nand in fact it does somewhat better than the on-line\\nwindow-based methods which scale like O(|C|).\\n4 Experiments\\n4.1 Evaluation methods\\nWe conduct experiments on the word analogy\\ntask of Mikolov et al. (2013a), a variety of word\\nsimilarity tasks, as described in (Luong et al.,\\n2013), and on the CoNLL-2003 shared benchmark'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='We conduct experiments on the word analogy\\ntask of Mikolov et al. (2013a), a variety of word\\nsimilarity tasks, as described in (Luong et al.,\\n2013), and on the CoNLL-2003 shared benchmark\\nTable 2: Results on the word analogy task, given\\nas percent accuracy. Underlined scores are best\\nwithin groups of similarly-sized models; bold\\nscores are best overall. HPCA vectors are publicly\\navailable2; (i)vLBL results are from (Mnih et al.,\\n2013); skip-gram (SG) and CBOW results are\\nfrom (Mikolov et al., 2013a,b); we trained SG ‚Ä†\\nand CBOW‚Ä†using the word2vec tool3. See text\\nfor details and a description of the SVD models.\\nModel Dim. Size Sem. Syn. Tot.\\nivLBL 100 1.5B 55.9 50.1 53.2\\nHPCA 100 1.6B 4.2 16.4 10.8\\nGloVe 100 1.6B 67.5 54.3 60.3\\nSG 300 1B 61 61 61\\nCBOW 300 1.6B 16.1 52.6 36.1\\nvLBL 300 1.5B 54.2 64.8 60.0\\nivLBL 300 1.5B 65.2 63.0 64.0\\nGloVe 300 1.6B 80.8 61.5 70.3\\nSVD 300 6B 6.3 8.1 7.3\\nSVD-S 300 6B 36.7 46.6 42.1\\nSVD-L 300 6B 56.6 63.0 60.1\\nCBOW‚Ä† 300 6B 63.6 67.4 65.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='vLBL 300 1.5B 54.2 64.8 60.0\\nivLBL 300 1.5B 65.2 63.0 64.0\\nGloVe 300 1.6B 80.8 61.5 70.3\\nSVD 300 6B 6.3 8.1 7.3\\nSVD-S 300 6B 36.7 46.6 42.1\\nSVD-L 300 6B 56.6 63.0 60.1\\nCBOW‚Ä† 300 6B 63.6 67.4 65.7\\nSG‚Ä† 300 6B 73.0 66.0 69.1\\nGloVe 300 6B 77.4 67.0 71.7\\nCBOW 1000 6B 57.3 68.9 63.7\\nSG 1000 6B 66.1 65.1 65.6\\nSVD-L 300 42B 38.4 58.2 49.2\\nGloVe 300 42B 81.9 69.3 75.0\\ndataset for NER (Tjong Kim Sang and De Meul-\\nder, 2003).\\nWord analogies.The word analogy task con-\\nsists of questions like, ‚Äú a is to b as c is to ?‚Äù\\nThe dataset contains 19,544 such questions, di-\\nvided into a semantic subset and a syntactic sub-\\nset. The semantic questions are typically analogies\\nabout people or places, like ‚ÄúAthens is to Greece\\nas Berlin is to ?‚Äù. The syntactic questions are\\ntypically analogies about verb tenses or forms of\\nadjectives, for example ‚Äúdance is to dancing as Ô¨Çy\\nis to ?‚Äù. To correctly answer the question, the\\nmodel should uniquely identify the missing term,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='typically analogies about verb tenses or forms of\\nadjectives, for example ‚Äúdance is to dancing as Ô¨Çy\\nis to ?‚Äù. To correctly answer the question, the\\nmodel should uniquely identify the missing term,\\nwith only an exact correspondence counted as a\\ncorrect match. We answer the question ‚Äú a is to b\\nas c is to ?‚Äù by Ô¨Ånding the word d whose repre-\\nsentation wd is closest to wb ‚àíwa + wc according\\nto the cosine similarity.4\\n2http://lebret.ch/words/\\n3http://code.google.com/p/word2vec/\\n4Levy et al. (2014) introduce a multiplicative analogy\\nevaluation, 3COSMUL, and report an accuracy of 68.24% on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='0 100 200 300 400 500 60020\\n30\\n40\\n50\\n60\\n70\\n80\\nVector Dimension\\nAccuracy [%]\\n \\nSemantic\\nSyntactic\\nOverall\\n(a) Symmetric context\\n2 4 6 8 1040\\n50\\n55\\n60\\n65\\n70\\n45\\nWindow Size\\nAccuracy [%]\\n \\n \\n \\nSemantic\\nSyntactic\\nOverall (b) Symmetric context\\n2 4 6 8 1040\\n50\\n55\\n60\\n65\\n70\\n45\\nWindow Size\\nAccuracy [%]\\n \\n \\n \\nSemantic\\nSyntactic\\nOverall (c) Asymmetric context\\nFigure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are\\ntrained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.\\nWord similarity. While the analogy task is our\\nprimary focus since it tests for interesting vector\\nspace substructures, we also evaluate our model on\\na variety of word similarity tasks in Table 3. These\\ninclude WordSim-353 (Finkelstein et al., 2001),\\nMC (Miller and Charles, 1991), RG (Rubenstein\\nand Goodenough, 1965), SCWS (Huang et al.,\\n2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='MC (Miller and Charles, 1991), RG (Rubenstein\\nand Goodenough, 1965), SCWS (Huang et al.,\\n2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003\\nEnglish benchmark dataset for NER is a collec-\\ntion of documents from Reuters newswire articles,\\nannotated with four entity types: person, location,\\norganization, and miscellaneous. We train mod-\\nels on CoNLL-03 training data on test on three\\ndatasets: 1) ConLL-03 testing data, 2) ACE Phase\\n2 (2001-02) and ACE-2003 data, and 3) MUC7\\nFormal Run test set. We adopt the BIO2 annota-\\ntion standard, as well as all the preprocessing steps\\ndescribed in (Wang and Manning, 2013). We use a\\ncomprehensive set of discrete features that comes\\nwith the standard distribution of the Stanford NER\\nmodel (Finkel et al., 2005). A total of 437,905\\ndiscrete features were generated for the CoNLL-\\n2003 training dataset. In addition, 50-dimensional\\nvectors for each word of a Ô¨Åve-word context are\\nadded and used as continuous features. With these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='discrete features were generated for the CoNLL-\\n2003 training dataset. In addition, 50-dimensional\\nvectors for each word of a Ô¨Åve-word context are\\nadded and used as continuous features. With these\\nfeatures as input, we trained a conditional random\\nÔ¨Åeld (CRF) with exactly the same setup as the\\nCRFjoin model of (Wang and Manning, 2013).\\n4.2 Corpora and training details\\nWe trained our model on Ô¨Åve corpora of varying\\nsizes: a 2010 Wikipedia dump with 1 billion to-\\nkens; a 2014 Wikipedia dump with 1.6 billion to-\\nkens; Gigaword 5 which has 4.3 billion tokens; the\\ncombination Gigaword5 + Wikipedia2014, which\\nthe analogy task. This number is evaluated on a subset of the\\ndataset so it is not included in Table 2. 3COSMUL performed\\nworse than cosine similarity in almost all of our experiments.\\nhas 6 billion tokens; and on 42 billion tokens of\\nweb data, from Common Crawl 5. We tokenize\\nand lowercase each corpus with the Stanford to-\\nkenizer, build a vocabulary of the 400,000 most'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='has 6 billion tokens; and on 42 billion tokens of\\nweb data, from Common Crawl 5. We tokenize\\nand lowercase each corpus with the Stanford to-\\nkenizer, build a vocabulary of the 400,000 most\\nfrequent words6, and then construct a matrix of co-\\noccurrence counts X. In constructing X, we must\\nchoose how large the context window should be\\nand whether to distinguish left context from right\\ncontext. We explore the effect of these choices be-\\nlow. In all cases we use a decreasing weighting\\nfunction, so that word pairs that are d words apart\\ncontribute 1/d to the total count. This is one way\\nto account for the fact that very distant word pairs\\nare expected to contain less relevant information\\nabout the words‚Äô relationship to one another.\\nFor all our experiments, we set xmax = 100,\\nŒ± = 3/4, and train the model using AdaGrad\\n(Duchi et al., 2011), stochastically sampling non-\\nzero elements from X, with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Œ± = 3/4, and train the model using AdaGrad\\n(Duchi et al., 2011), stochastically sampling non-\\nzero elements from X, with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than\\n300 dimensions, and 100 iterations otherwise (see\\nSection 4.6 for more details about the convergence\\nrate). Unless otherwise noted, we use a context of\\nten words to the left and ten words to the right.\\nThe model generates two sets of word vectors,\\nW and ÀúW. When X is symmetric, W and ÀúW are\\nequivalent and differ only as a result of their ran-\\ndom initializations; the two sets of vectors should\\nperform equivalently. On the other hand, there is\\nevidence that for certain types of neural networks,\\ntraining multiple instances of the network and then\\ncombining the results can help reduce overÔ¨Åtting\\nand noise and generally improve results (Ciresan\\net al., 2012). With this in mind, we choose to use\\n5To demonstrate the scalability of the model, we also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and noise and generally improve results (Ciresan\\net al., 2012). With this in mind, we choose to use\\n5To demonstrate the scalability of the model, we also\\ntrained it on a much larger sixth corpus, containing 840 bil-\\nlion tokens of web data, but in this case we did not lowercase\\nthe vocabulary, so the results are not directly comparable.\\n6For the model trained on Common Crawl data, we use a\\nlarger vocabulary of about 2 million words.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='the sum W + ÀúW as our word vectors. Doing so typ-\\nically gives a small boost in performance, with the\\nbiggest increase in the semantic analogy task.\\nWe compare with the published results of a va-\\nriety of state-of-the-art models, as well as with\\nour own results produced using the word2vec\\ntool and with several baselines using SVDs. With\\nword2vec, we train the skip-gram (SG ‚Ä†) and\\ncontinuous bag-of-words (CBOW‚Ä†) models on the\\n6 billion token corpus (Wikipedia 2014 + Giga-\\nword 5) with a vocabulary of the top 400,000 most\\nfrequent words and a context window size of 10.\\nWe used 10 negative samples, which we show in\\nSection 4.6 to be a good choice for this corpus.\\nFor the SVD baselines, we generate a truncated\\nmatrix Xtrunc which retains the information of how\\nfrequently each word occurs with only the top\\n10,000 most frequent words. This step is typi-\\ncal of many matrix-factorization-based methods as\\nthe extra columns can contribute a disproportion-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='frequently each word occurs with only the top\\n10,000 most frequent words. This step is typi-\\ncal of many matrix-factorization-based methods as\\nthe extra columns can contribute a disproportion-\\nate number of zero entries and the methods are\\notherwise computationally expensive.\\nThe singular vectors of this matrix constitute\\nthe baseline ‚ÄúSVD‚Äù. We also evaluate two related\\nbaselines: ‚ÄúSVD-S‚Äù in which we take the SVD of‚àöXtrunc, and ‚ÄúSVD-L‚Äù in which we take the SVD\\nof log(1+ Xtrunc). Both methods help compress the\\notherwise large range of values in X.7\\n4.3 Results\\nWe present results on the word analogy task in Ta-\\nble 2. The GloVe model performs signiÔ¨Åcantly\\nbetter than the other baselines, often with smaller\\nvector sizes and smaller corpora. Our results us-\\ning the word2vec tool are somewhat better than\\nmost of the previously published results. This is\\ndue to a number of factors, including our choice to\\nuse negative sampling (which typically works bet-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ing the word2vec tool are somewhat better than\\nmost of the previously published results. This is\\ndue to a number of factors, including our choice to\\nuse negative sampling (which typically works bet-\\nter than the hierarchical softmax), the number of\\nnegative samples, and the choice of the corpus.\\nWe demonstrate that the model can easily be\\ntrained on a large 42 billion token corpus, with a\\nsubstantial corresponding performance boost. We\\nnote that increasing the corpus size does not guar-\\nantee improved results for other models, as can be\\nseen by the decreased performance of the SVD-\\n7We also investigated several other weighting schemes for\\ntransforming X; what we report here performed best. Many\\nweighting schemes like PPMI destroy the sparsity of X and\\ntherefore cannot feasibly be used with large vocabularies.\\nWith smaller vocabularies, these information-theoretic trans-\\nformations do indeed work well on word similarity measures,\\nbut they perform very poorly on the word analogy task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='With smaller vocabularies, these information-theoretic trans-\\nformations do indeed work well on word similarity measures,\\nbut they perform very poorly on the word analogy task.\\nTable 3: Spearman rank correlation on word simi-\\nlarity tasks. All vectors are 300-dimensional. The\\nCBOW‚àóvectors are from the word2vec website\\nand differ in that they contain phrase vectors.\\nModel Size WS353 MC RG SCWS RW\\nSVD 6B 35.3 35.1 42.5 38.3 25.6\\nSVD-S 6B 56.5 71.5 71.0 53.6 34.7\\nSVD-L 6B 65.7 72.7 75.1 56.5 37.0\\nCBOW‚Ä† 6B 57.2 65.6 68.2 57.0 32.5\\nSG‚Ä† 6B 62.8 65.2 69.7 58.1 37.2\\nGloVe 6B 65.8 72.7 77.8 53.9 38.1\\nSVD-L 42B 74.0 76.4 74.1 58.3 39.9\\nGloVe 42B 75.9 83.6 82.9 59.6 47.8\\nCBOW‚àó 100B 68.4 79.6 75.4 59.4 45.5\\nL model on this larger corpus. The fact that this\\nbasic SVD model does not scale well to large cor-\\npora lends further evidence to the necessity of the\\ntype of weighting scheme proposed in our model.\\nTable 3 shows results on Ô¨Åve different word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='basic SVD model does not scale well to large cor-\\npora lends further evidence to the necessity of the\\ntype of weighting scheme proposed in our model.\\nTable 3 shows results on Ô¨Åve different word\\nsimilarity datasets. A similarity score is obtained\\nfrom the word vectors by Ô¨Årst normalizing each\\nfeature across the vocabulary and then calculat-\\ning the cosine similarity. We compute Spearman‚Äôs\\nrank correlation coefÔ¨Åcient between this score and\\nthe human judgments. CBOW ‚àó denotes the vec-\\ntors available on the word2vec website that are\\ntrained with word and phrase vectors on 100B\\nwords of news data. GloVe outperforms it while\\nusing a corpus less than half the size.\\nTable 4 shows results on the NER task with the\\nCRF-based model. The L-BFGS training termi-\\nnates when no improvement has been achieved on\\nthe dev set for 25 iterations. Otherwise all conÔ¨Åg-\\nurations are identical to those used by Wang and\\nManning (2013). The model labeled Discrete is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='nates when no improvement has been achieved on\\nthe dev set for 25 iterations. Otherwise all conÔ¨Åg-\\nurations are identical to those used by Wang and\\nManning (2013). The model labeled Discrete is\\nthe baseline using a comprehensive set of discrete\\nfeatures that comes with the standard distribution\\nof the Stanford NER model, but with no word vec-\\ntor features. In addition to the HPCA and SVD\\nmodels discussed previously, we also compare to\\nthe models of Huang et al. (2012) (HSMN) and\\nCollobert and Weston (2008) (CW). We trained\\nthe CBOW model using the word2vec tool8.\\nThe GloVe model outperforms all other methods\\non all evaluation metrics, except for the CoNLL\\ntest set, on which the HPCA method does slightly\\nbetter. We conclude that the GloVe vectors are\\nuseful in downstream NLP tasks, as was Ô¨Årst\\n8We use the same parameters as above, except in this case\\nwe found 5 negative samples to work slightly better than 10.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Table 4: F1 score on NER task with 50d vectors.\\nDiscrete is the baseline without word vectors. We\\nuse publicly-available vectors for HPCA, HSMN,\\nand CW. See text for details.\\nModel Dev Test ACE MUC7\\nDiscrete 91.0 85.4 77.4 73.4\\nSVD 90.8 85.7 77.3 73.7\\nSVD-S 91.0 85.5 77.6 74.3\\nSVD-L 90.5 84.8 73.6 71.5\\nHPCA 92.6 88.7 81.7 80.7\\nHSMN 90.5 85.7 78.7 74.7\\nCW 92.2 87.4 81.7 80.2\\nCBOW 93.1 88.2 82.2 81.1\\nGloVe 93.2 88.3 82.9 82.2\\nshown for neural vectors in (Turian et al., 2010).\\n4.4 Model Analysis: Vector Length and\\nContext Size\\nIn Fig. 2, we show the results of experiments that\\nvary vector length and context window. A context\\nwindow that extends to the left and right of a tar-\\nget word will be called symmetric, and one which\\nextends only to the left will be called asymmet-\\nric. In (a), we observe diminishing returns for vec-\\ntors larger than about 200 dimensions. In (b) and\\n(c), we examine the effect of varying the window\\nsize for symmetric and asymmetric context win-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ric. In (a), we observe diminishing returns for vec-\\ntors larger than about 200 dimensions. In (b) and\\n(c), we examine the effect of varying the window\\nsize for symmetric and asymmetric context win-\\ndows. Performance is better on the syntactic sub-\\ntask for small and asymmetric context windows,\\nwhich aligns with the intuition that syntactic infor-\\nmation is mostly drawn from the immediate con-\\ntext and can depend strongly on word order. Se-\\nmantic information, on the other hand, is more fre-\\nquently non-local, and more of it is captured with\\nlarger window sizes.\\n4.5 Model Analysis: Corpus Size\\nIn Fig. 3, we show performance on the word anal-\\nogy task for 300-dimensional vectors trained on\\ndifferent corpora. On the syntactic subtask, there\\nis a monotonic increase in performance as the cor-\\npus size increases. This is to be expected since\\nlarger corpora typically produce better statistics.\\nInterestingly, the same trend is not true for the se-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='pus size increases. This is to be expected since\\nlarger corpora typically produce better statistics.\\nInterestingly, the same trend is not true for the se-\\nmantic subtask, where the models trained on the\\nsmaller Wikipedia corpora do better than those\\ntrained on the larger Gigaword corpus. This is\\nlikely due to the large number of city- and country-\\nbased analogies in the analogy dataset and the fact\\nthat Wikipedia has fairly comprehensive articles\\nfor most such locations. Moreover, Wikipedia‚Äôs\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\nOverallSyntacticSemantic\\nWiki2010\\n1B tokens\\nAccuracy [%]\\nWiki2014\\n1.6B tokens\\nGigaword5\\n4.3B tokens\\nGigaword5 + \\nWiki2014\\n6B tokens\\nCommon Crawl \\n42B tokens\\nFigure 3: Accuracy on the analogy task for 300-\\ndimensional vectors trained on different corpora.\\nentries are updated to assimilate new knowledge,\\nwhereas Gigaword is a Ô¨Åxed news repository with\\noutdated and possibly incorrect information.\\n4.6 Model Analysis: Run-time\\nThe total run-time is split between populating X'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='whereas Gigaword is a Ô¨Åxed news repository with\\noutdated and possibly incorrect information.\\n4.6 Model Analysis: Run-time\\nThe total run-time is split between populating X\\nand training the model. The former depends on\\nmany factors, including window size, vocabulary\\nsize, and corpus size. Though we did not do so,\\nthis step could easily be parallelized across mul-\\ntiple machines (see, e.g., Lebret and Collobert\\n(2014) for some benchmarks). Using a single\\nthread of a dual 2.1GHz Intel Xeon E5-2658 ma-\\nchine, populating X with a 10 word symmetric\\ncontext window, a 400,000 word vocabulary, and\\na 6 billion token corpus takes about 85 minutes.\\nGiven X, the time it takes to train the model de-\\npends on the vector size and the number of itera-\\ntions. For 300-dimensional vectors with the above\\nsettings (and using all 32 cores of the above ma-\\nchine), a single iteration takes 14 minutes. See\\nFig. 4 for a plot of the learning curve.\\n4.7 Model Analysis: Comparison with\\nword2vec'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='settings (and using all 32 cores of the above ma-\\nchine), a single iteration takes 14 minutes. See\\nFig. 4 for a plot of the learning curve.\\n4.7 Model Analysis: Comparison with\\nword2vec\\nA rigorous quantitative comparison of GloVe with\\nword2vec is complicated by the existence of\\nmany parameters that have a strong effect on per-\\nformance. We control for the main sources of vari-\\nation that we identiÔ¨Åed in Sections 4.4 and 4.5 by\\nsetting the vector length, context window size, cor-\\npus, and vocabulary size to the conÔ¨Åguration men-\\ntioned in the previous subsection.\\nThe most important remaining variable to con-\\ntrol for is training time. For GloVe, the rele-\\nvant parameter is the number of training iterations.\\nFor word2vec, the obvious choice would be the\\nnumber of training epochs. Unfortunately, the\\ncode is currently designed for only a single epoch:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='1 2 3 4 5 6\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n5 10 15 20 25\\n1357 10 15 20 25 30 40 50\\nAccuracy [%]\\nIterations (GloVe)\\nNegative Samples (CBOW)\\nTraining Time (hrs)\\n \\nGloVe\\nCBOW\\n(a) GloVe vs CBOW\\n3 6 9 12 15 18 21 24\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n20 40 60 80 100\\n1 2 3 4 5 6 7 10 12 15 20\\nGloVe\\nSkip-Gram\\nAccuracy [%]\\nIterations (GloVe)\\nNegative Samples (Skip-Gram)\\nTraining Time (hrs) (b) GloVe vs Skip-Gram\\nFigure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by\\nthe number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram\\n(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +\\nGigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.\\nit speciÔ¨Åes a learning schedule speciÔ¨Åc to a single\\npass through the data, making a modiÔ¨Åcation for\\nmultiple passes a non-trivial task. Another choice\\nis to vary the number of negative samples. Adding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='pass through the data, making a modiÔ¨Åcation for\\nmultiple passes a non-trivial task. Another choice\\nis to vary the number of negative samples. Adding\\nnegative samples effectively increases the number\\nof training words seen by the model, so in some\\nways it is analogous to extra epochs.\\nWe set any unspeciÔ¨Åed parameters to their de-\\nfault values, assuming that they are close to opti-\\nmal, though we acknowledge that this simpliÔ¨Åca-\\ntion should be relaxed in a more thorough analysis.\\nIn Fig. 4, we plot the overall performance on\\nthe analogy task as a function of training time.\\nThe two x-axes at the bottom indicate the corre-\\nsponding number of training iterations for GloVe\\nand negative samples for word2vec. We note\\nthat word2vec‚Äôs performance actually decreases\\nif the number of negative samples increases be-\\nyond about 10. Presumably this is because the\\nnegative sampling method does not approximate\\nthe target probability distribution well.9\\nFor the same corpus, vocabulary, window size,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='yond about 10. Presumably this is because the\\nnegative sampling method does not approximate\\nthe target probability distribution well.9\\nFor the same corpus, vocabulary, window size,\\nand training time, GloVe consistently outperforms\\nword2vec. It achieves better results faster, and\\nalso obtains the best results irrespective of speed.\\n5 Conclusion\\nRecently, considerable attention has been focused\\non the question of whether distributional word\\nrepresentations are best learned from count-based\\n9In contrast, noise-contrastive estimation is an approxi-\\nmation which improves with more negative samples. In Ta-\\nble 1 of (Mnih et al., 2013), accuracy on the analogy task is a\\nnon-decreasing function of the number of negative samples.\\nmethods or from prediction-based methods. Cur-\\nrently, prediction-based models garner substantial\\nsupport; for example, Baroni et al. (2014) argue\\nthat these models perform better across a range of\\ntasks. In this work we argue that the two classes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='support; for example, Baroni et al. (2014) argue\\nthat these models perform better across a range of\\ntasks. In this work we argue that the two classes\\nof methods are not dramatically different at a fun-\\ndamental level since they both probe the under-\\nlying co-occurrence statistics of the corpus, but\\nthe efÔ¨Åciency with which the count-based meth-\\nods capture global statistics can be advantageous.\\nWe construct a model that utilizes this main ben-\\neÔ¨Åt of count data while simultaneously capturing\\nthe meaningful linear substructures prevalent in\\nrecent log-bilinear prediction-based methods like\\nword2vec. The result, GloVe, is a new global\\nlog-bilinear regression model for the unsupervised\\nlearning of word representations that outperforms\\nother models on word analogy, word similarity,\\nand named entity recognition tasks.\\nAcknowledgments\\nWe thank the anonymous reviewers for their valu-\\nable comments. Stanford University gratefully\\nacknowledges the support of the Defense Threat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and named entity recognition tasks.\\nAcknowledgments\\nWe thank the anonymous reviewers for their valu-\\nable comments. Stanford University gratefully\\nacknowledges the support of the Defense Threat\\nReduction Agency (DTRA) under Air Force Re-\\nsearch Laboratory (AFRL) contract no. FA8650-\\n10-C-7020 and the Defense Advanced Research\\nProjects Agency (DARPA) Deep Exploration and\\nFiltering of Text (DEFT) Program under AFRL\\ncontract no. FA8750-13-2-0040. Any opinions,\\nÔ¨Åndings, and conclusion or recommendations ex-\\npressed in this material are those of the authors and\\ndo not necessarily reÔ¨Çect the view of the DTRA,\\nAFRL, DEFT, or the US government.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='References\\nTom M. Apostol. 1976. Introduction to Analytic\\nNumber Theory. Introduction to Analytic Num-\\nber Theory.\\nMarco Baroni, Georgiana Dinu, and Germ ¬¥an\\nKruszewski. 2014. Don‚Äôt count, predict! A\\nsystematic comparison of context-counting vs.\\ncontext-predicting semantic vectors. In ACL.\\nYoshua Bengio. 2009. Learning deep architectures\\nfor AI. Foundations and Trends in Machine\\nLearning.\\nYoshua Bengio, R ¬¥ejean Ducharme, Pascal Vin-\\ncent, and Christian Janvin. 2003. A neural prob-\\nabilistic language model. JMLR, 3:1137‚Äì1155.\\nJohn A. Bullinaria and Joseph P. Levy. 2007. Ex-\\ntracting semantic representations from word co-\\noccurrence statistics: A computational study.\\nBehavior Research Methods, 39(3):510‚Äì526.\\nDan C. Ciresan, Alessandro Giusti, Luca M. Gam-\\nbardella, and J ¬®urgen Schmidhuber. 2012. Deep\\nneural networks segment neuronal membranes\\nin electron microscopy images. In NIPS, pages\\n2852‚Äì2860.\\nRonan Collobert and Jason Weston. 2008. A uni-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='bardella, and J ¬®urgen Schmidhuber. 2012. Deep\\nneural networks segment neuronal membranes\\nin electron microscopy images. In NIPS, pages\\n2852‚Äì2860.\\nRonan Collobert and Jason Weston. 2008. A uni-\\nÔ¨Åed architecture for natural language process-\\ning: deep neural networks with multitask learn-\\ning. In Proceedings of ICML, pages 160‚Äì167.\\nRonan Collobert, Jason Weston, L ¬¥eon Bottou,\\nMichael Karlen, Koray Kavukcuoglu, and Pavel\\nKuksa. 2011. Natural Language Processing (Al-\\nmost) from Scratch. JMLR, 12:2493‚Äì2537.\\nScott Deerwester, Susan T. Dumais, George W.\\nFurnas, Thomas K. Landauer, and Richard\\nHarshman. 1990. Indexing by latent semantic\\nanalysis. Journal of the American Society for\\nInformation Science, 41.\\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\\nAdaptive subgradient methods for online learn-\\ning and stochastic optimization. JMLR, 12.\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma-\\ntias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. 2001. Placing search in con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ing and stochastic optimization. JMLR, 12.\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma-\\ntias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. 2001. Placing search in con-\\ntext: The concept revisited. In Proceedings\\nof the 10th international conference on World\\nWide Web, pages 406‚Äì414. ACM.\\nEric H. Huang, Richard Socher, Christopher D.\\nManning, and Andrew Y . Ng. 2012. Improving\\nWord Representations via Global Context and\\nMultiple Word Prototypes. In ACL.\\nR¬¥emi Lebret and Ronan Collobert. 2014. Word\\nembeddings through Hellinger PCA. In EACL.\\nOmer Levy, Yoav Goldberg, and Israel Ramat-\\nGan. 2014. Linguistic regularities in sparse and\\nexplicit word representations. CoNLL-2014.\\nKevin Lund and Curt Burgess. 1996. Producing\\nhigh-dimensional semantic spaces from lexical\\nco-occurrence. Behavior Research Methods, In-\\nstrumentation, and Computers, 28:203‚Äì208.\\nMinh-Thang Luong, Richard Socher, and Christo-\\npher D Manning. 2013. Better word represen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='co-occurrence. Behavior Research Methods, In-\\nstrumentation, and Computers, 28:203‚Äì208.\\nMinh-Thang Luong, Richard Socher, and Christo-\\npher D Manning. 2013. Better word represen-\\ntations with recursive neural networks for mor-\\nphology. CoNLL-2013.\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\\nfrey Dean. 2013a. EfÔ¨Åcient Estimation of Word\\nRepresentations in Vector Space. InICLR Work-\\nshop Papers.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg\\nCorrado, and Jeffrey Dean. 2013b. Distributed\\nrepresentations of words and phrases and their\\ncompositionality. In NIPS, pages 3111‚Äì3119.\\nTomas Mikolov, Wen tau Yih, and Geoffrey\\nZweig. 2013c. Linguistic regularities in con-\\ntinuous space word representations. In HLT-\\nNAACL.\\nGeorge A. Miller and Walter G. Charles. 1991.\\nContextual correlates of semantic similarity.\\nLanguage and cognitive processes, 6(1):1‚Äì28.\\nAndriy Mnih and Koray Kavukcuoglu. 2013.\\nLearning word embeddings efÔ¨Åciently with\\nnoise-contrastive estimation. In NIPS.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Language and cognitive processes, 6(1):1‚Äì28.\\nAndriy Mnih and Koray Kavukcuoglu. 2013.\\nLearning word embeddings efÔ¨Åciently with\\nnoise-contrastive estimation. In NIPS.\\nDouglas L. T. Rohde, Laura M. Gonnerman,\\nand David C. Plaut. 2006. An improved\\nmodel of semantic similarity based on lexical\\nco-occurence. Communications of the ACM,\\n8:627‚Äì633.\\nHerbert Rubenstein and John B. Goodenough.\\n1965. Contextual correlates of synonymy.Com-\\nmunications of the ACM, 8(10):627‚Äì633.\\nFabrizio Sebastiani. 2002. Machine learning in au-\\ntomated text categorization. ACM Computing\\nSurveys, 34:1‚Äì47.\\nRichard Socher, John Bauer, Christopher D. Man-\\nning, and Andrew Y . Ng. 2013. Parsing With\\nCompositional Vector Grammars. In ACL.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron\\nFernandes, and Gregory Marton. 2003. Quanti-\\ntative evaluation of passage retrieval algorithms\\nfor question answering. In Proceedings of the\\nSIGIR Conference on Research and Develop-\\nment in Informaion Retrieval.\\nErik F. Tjong Kim Sang and Fien De Meul-\\nder. 2003. Introduction to the CoNLL-2003\\nshared task: Language-independent named en-\\ntity recognition. In CoNLL-2003.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\\n2010. Word representations: a simple and gen-\\neral method for semi-supervised learning. In\\nProceedings of ACL, pages 384‚Äì394.\\nMengqiu Wang and Christopher D. Manning.\\n2013. Effect of non-linear deep architecture in\\nsequence labeling. In Proceedings of the 6th\\nInternational Joint Conference on Natural Lan-\\nguage Processing (IJCNLP).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be Ô¨Åne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciÔ¨Åc architecture modiÔ¨Åcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='language inference, without substantial task-\\nspeciÔ¨Åc architecture modiÔ¨Åcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='natural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce Ô¨Åne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and Ô¨Åne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciÔ¨Åc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The Ô¨Åne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciÔ¨Åc parameters, and is trained on the\\ndownstream tasks by simply Ô¨Åne-tuning all pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='the Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciÔ¨Åc parameters, and is trained on the\\ndownstream tasks by simply Ô¨Åne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the Ô¨Åne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying Ô¨Åne-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='of the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying Ô¨Åne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the Ô¨Åne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a ‚Äúmasked lan-\\nguage model‚Äù (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\narXiv:1810.04805v2  [cs.CL]  24 May 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na ‚Äúnext sentence prediction‚Äù task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n‚Ä¢ We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n‚Ä¢ We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='trained left-to-right and right-to-left LMs.\\n‚Ä¢ We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciÔ¨Åc architectures.\\n‚Ä¢ BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieÔ¨Çy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='words has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiÔ¨Åcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciÔ¨Åc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='benchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the Ô¨Årst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nÔ¨Åne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='which produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nÔ¨Åne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT BERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nNERMNLI\\nFigure 1: Overall pre-training and Ô¨Åne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During Ô¨Åne-tuning, all parameters are Ô¨Åne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to Ô¨Åne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and Ô¨Åne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT model is Ô¨Årst initialized with\\nthe pre-trained parameters, and all of the param-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT model is Ô¨Årst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are Ô¨Åne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate Ô¨Åne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniÔ¨Åed ar-\\nchitecture across different tasks. There is mini-\\nmal difference between the pre-trained architec-\\nture and the Ô¨Ånal downstream architecture.\\nModel Architecture BERT‚Äôs model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='coder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as ‚ÄúThe Annotated Transformer.‚Äù2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='size as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorÔ¨Çow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/Ô¨Ålter size to be 4H,\\ni.e., 3072 for the H = 768and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ‚ü®Question, Answer ‚ü©) in one token sequence.\\nThroughout this work, a ‚Äúsentence‚Äù can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A ‚Äúsequence‚Äù refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The Ô¨Årst\\ntoken of every sequence is always a special clas-\\nsiÔ¨Åcation token ( [CLS]). The Ô¨Ånal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiÔ¨Åcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the Ô¨Ånal hidden\\nvector of the special [CLS] token as C ‚ààRH,\\nand the Ô¨Ånal hidden vector for the ith input token\\nas Ti ‚ààRH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Instead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly ‚Äúsee itself‚Äù, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a ‚ÄúTransformer encoder‚Äù while\\nthe left-context-only version is referred to as a ‚ÄúTransformer\\ndecoder‚Äù since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='decoder‚Äù since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a ‚Äúmasked\\nLM‚Äù (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the Ô¨Ånal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nÔ¨Åne-tuning, since the [MASK] token does not ap-\\npear during Ô¨Åne-tuning. To mitigate this, we do'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nÔ¨Åne-tuning, since the [MASK] token does not ap-\\npear during Ô¨Åne-tuning. To mitigate this, we do\\nnot always replace ‚Äúmasked‚Äù words with the ac-\\ntual [MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthe i-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTi will be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='ence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-\\ntionships, we pre-train for a binarized next sen-\\ntence prediction task that can be trivially gener-\\nated from any monolingual corpus. SpeciÔ¨Åcally,\\nwhen choosing the sentencesA and B for each pre-\\ntraining example, 50% of the time B is the actual\\nnext sentence that follows A (labeled as IsNext),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext). As we show\\nin Figure 1, C is used for next sentence predic-\\ntion (NSP). 5 Despite its simplicity, we demon-\\nstrate in Section 5.1 that pre-training towards this\\ntask is very beneÔ¨Åcial to both QA and NLI. 6\\n5The Ô¨Ånal model achieves 97%-98% accuracy on NSP.\\n6The vector C is not a meaningful sentence representation\\nwithout Ô¨Åne-tuning, since it was trained with NSP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='[CLS] he likes play ##ing [SEP]my dog is cute [SEP]Input\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP]Emy Edog Eis Ecute E[SEP]\\nToken\\nEmbeddings\\nEA EB EB EB EB EBEA EA EA EA EASegment\\nEmbeddings\\nE0 E6 E7 E8 E9 E10E1 E2 E3 E4 E5Position\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='model pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufÔ¨Çed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks‚Äî\\nwhether they involve single text or text pairs‚Äîby\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='fore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciÔ¨Åc inputs and outputs into BERT and Ô¨Åne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence A and sentence B from pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and\\n(4) a degenerate text- ‚àÖ pair in text classiÔ¨Åcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiÔ¨Åcation, such as en-\\ntailment or sentiment analysis.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='level tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiÔ¨Åcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, Ô¨Åne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model. 7 We de-\\nscribe the task-speciÔ¨Åc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT Ô¨Åne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo Ô¨Åne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo Ô¨Åne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the Ô¨Ånal hid-\\nden vector C ‚àà RH corresponding to the Ô¨Årst\\ninput token ([CLS]) as the aggregate representa-\\ntion. The only new parameters introduced during\\nÔ¨Åne-tuning are classiÔ¨Åcation layer weights W ‚àà\\nRK√óH, where Kis the number of labels. We com-\\npute a standard classiÔ¨Åcation loss with C and W,\\ni.e., log(softmax(CWT )).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\\nBERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\\nBERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard).\\nThe number below each task denotes the number of training examples. The ‚ÄúAverage‚Äù column is slightly different\\nthan the ofÔ¨Åcial GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are single-\\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\nWe use a batch size of 32 and Ô¨Åne-tune for 3\\nepochs over the data for all GLUE tasks. For each\\ntask, we selected the best Ô¨Åne-tuning learning rate\\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\\nAdditionally, for BERTLARGE we found that Ô¨Åne-\\ntuning was sometimes unstable on small datasets,\\nso we ran several random restarts and selected the\\nbest model on the Dev set. With random restarts,\\nwe use the same pre-trained checkpoint but per-\\nform different Ô¨Åne-tuning data shufÔ¨Çing and clas-\\nsiÔ¨Åer layer initialization.9\\nResults are presented in Table 1. Both\\nBERTBASE and BERTLARGE outperform all sys-\\ntems on all tasks by a substantial margin, obtaining\\n4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERTBASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERTBASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-\\ntention masking. For the largest and most widely\\nreported GLUE task, MNLI, BERT obtains a 4.6%\\nabsolute accuracy improvement. On the ofÔ¨Åcial\\nGLUE leaderboard10, BERTLARGE obtains a score\\nof 80.5, compared to OpenAI GPT, which obtains\\n72.8 as of the date of writing.\\nWe Ô¨Ånd that BERT LARGE signiÔ¨Åcantly outper-\\nforms BERTBASE across all tasks, especially those\\nwith very little training data. The effect of model\\nsize is explored more thoroughly in Section 5.2.\\n4.2 SQuAD v1.1\\nThe Stanford Question Answering Dataset\\n(SQuAD v1.1) is a collection of 100k crowd-\\nsourced question/answer pairs (Rajpurkar et al.,\\n2016). Given a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='2016). Given a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server\\nsubmission for each of BERTBASE and BERTLARGE .\\n10https://gluebenchmark.com/leaderboard\\nWikipedia containing the answer, the task is to\\npredict the answer text span in the passage.\\nAs shown in Figure 1, in the question answer-\\ning task, we represent the input question and pas-\\nsage as a single packed sequence, with the ques-\\ntion using the A embedding and the passage using\\nthe B embedding. We only introduce a start vec-\\ntor S ‚ààRH and an end vector E ‚ààRH during\\nÔ¨Åne-tuning. The probability of word i being the\\nstart of the answer span is computed as a dot prod-\\nuct between Ti and S followed by a softmax over\\nall of the words in the paragraph: Pi = eS¬∑Ti\\n‚àë\\nj eS¬∑Tj .\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\nposition ito position jis deÔ¨Åned as S¬∑Ti + E¬∑Tj,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='‚àë\\nj eS¬∑Tj .\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\nposition ito position jis deÔ¨Åned as S¬∑Ti + E¬∑Tj,\\nand the maximum scoring span where j ‚â• i is\\nused as a prediction. The training objective is the\\nsum of the log-likelihoods of the correct start and\\nend positions. We Ô¨Åne-tune for 3 epochs with a\\nlearning rate of 5e-5 and a batch size of 32.\\nTable 2 shows top leaderboard entries as well\\nas results from top published systems (Seo et al.,\\n2017; Clark and Gardner, 2018; Peters et al.,\\n2018a; Hu et al., 2018). The top results from the\\nSQuAD leaderboard do not have up-to-date public\\nsystem descriptions available,11 and are allowed to\\nuse any public data when training their systems.\\nWe therefore use modest data augmentation in\\nour system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi\\net al., 2017) befor Ô¨Åne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='our system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi\\net al., 2017) befor Ô¨Åne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and\\n+1.3 F1 as a single system. In fact, our single\\nBERT model outperforms the top ensemble sys-\\ntem in terms of F1 score. Without TriviaQA Ô¨Åne-\\n11QANet is described in Yu et al. (2018), but the system\\nhas improved substantially after publication.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='System Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman - - 82.3 91.2\\n#1 Ensemble - nlnet - - 86.0 91.7\\n#2 Ensemble - QANet - - 84.5 90.5\\nPublished\\nBiDAF+ELMo (Single) - 85.6 - 85.8\\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\\nOurs\\nBERTBASE (Single) 80.8 88.5 - -\\nBERTLARGE (Single) 84.1 90.9 - -\\nBERTLARGE (Ensemble) 85.8 91.8 - -\\nBERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2: SQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and Ô¨Åne-tuning seeds.\\nSystem Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman 86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\\n#2 Single - nlnet - - 74.2 77.1\\nPublished\\nunet (Ensemble) - - 71.4 74.9\\nSLQA+ (Single) - 71.4 74.4\\nOurs\\nBERTLARGE (Single) 78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Ours\\nBERTLARGE (Single) 78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-\\nforming all existing systems by a wide margin.12\\n4.3 SQuAD v2.0\\nThe SQuAD 2.0 task extends the SQuAD 1.1\\nproblem deÔ¨Ånition by allowing for the possibility\\nthat no short answer exists in the provided para-\\ngraph, making the problem more realistic.\\nWe use a simple approach to extend the SQuAD\\nv1.1 BERT model for this task. We treat ques-\\ntions that do not have an answer as having an an-\\nswer span with start and end at the [CLS] to-\\nken. The probability space for the start and end\\nanswer span positions is extended to include the\\nposition of the [CLS] token. For prediction, we\\ncompare the score of the no-answer span: snull =\\nS¬∑C+ E¬∑C to the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='S¬∑C+ E¬∑C to the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents,\\nthat contain at least one of the provided possible answers.\\nSystem Dev Test\\nESIM+GloVe 51.9 52.7\\nESIM+ELMo 59.1 59.2\\nOpenAI GPT - 78.0\\nBERTBASE 81.6 -\\nBERTLARGE 86.6 86.3\\nHuman (expert)‚Ä† - 85.0\\nHuman (5 annotations)‚Ä† - 88.0\\nTable 4: SW AG Dev and Test accuracies.‚Ä†Human per-\\nformance is measured with 100 samples, as reported in\\nthe SW AG paper.\\nÀÜsi,j = maxj‚â•iS¬∑Ti + E¬∑Tj. We predict a non-null\\nanswer when ÀÜsi,j > snull + œÑ, where the thresh-\\nold œÑ is selected on the dev set to maximize F1.\\nWe did not use TriviaQA data for this model. We\\nÔ¨Åne-tuned for 2 epochs with a learning rate of 5e-5\\nand a batch size of 48.\\nThe results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='The results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-\\nponents. We observe a +5.1 F1 improvement over\\nthe previous best system.\\n4.4 SWAG\\nThe Situations With Adversarial Generations\\n(SW AG) dataset contains 113k sentence-pair com-\\npletion examples that evaluate grounded common-\\nsense inference (Zellers et al., 2018). Given a sen-\\ntence, the task is to choose the most plausible con-\\ntinuation among four choices.\\nWhen Ô¨Åne-tuning on the SW AG dataset, we\\nconstruct four input sequences, each containing\\nthe concatenation of the given sentence (sentence\\nA) and a possible continuation (sentence B). The\\nonly task-speciÔ¨Åc parameters introduced is a vec-\\ntor whose dot product with the [CLS] token rep-\\nresentation C denotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe Ô¨Åne-tune the model for 3 epochs with a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tor whose dot product with the [CLS] token rep-\\nresentation C denotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe Ô¨Åne-tune the model for 3 epochs with a\\nlearning rate of 2e-5 and a batch size of 16. Re-\\nsults are presented in Table 4. BERT LARGE out-\\nperforms the authors‚Äô baseline ESIM+ELMo sys-\\ntem by +27.1% and OpenAI GPT by 8.3%.\\n5 Ablation Studies\\nIn this section, we perform ablation experiments\\nover a number of facets of BERT in order to better\\nunderstand their relative importance. Additional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Dev Set\\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\\n(Acc) (Acc) (Acc) (Acc) (F1)\\nBERTBASE 84.4 88.4 86.7 92.7 88.5\\nNo NSP 83.9 84.9 86.5 92.6 87.9\\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\\nTable 5: Ablation over the pre-training tasks using the\\nBERTBASE architecture. ‚ÄúNo NSP‚Äù is trained without\\nthe next sentence prediction task. ‚ÄúLTR & No NSP‚Äù is\\ntrained as a left-to-right LM without the next sentence\\nprediction, like OpenAI GPT. ‚Äú+ BiLSTM‚Äù adds a ran-\\ndomly initialized BiLSTM on top of the ‚ÄúLTR + No\\nNSP‚Äù model during Ô¨Åne-tuning.\\nablation studies can be found in Appendix C.\\n5.1 Effect of Pre-training Tasks\\nWe demonstrate the importance of the deep bidi-\\nrectionality of BERT by evaluating two pre-\\ntraining objectives using exactly the same pre-\\ntraining data, Ô¨Åne-tuning scheme, and hyperpa-\\nrameters as BERTBASE :\\nNo NSP: A bidirectional model which is trained\\nusing the ‚Äúmasked LM‚Äù (MLM) but without the\\n‚Äúnext sentence prediction‚Äù (NSP) task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='training data, Ô¨Åne-tuning scheme, and hyperpa-\\nrameters as BERTBASE :\\nNo NSP: A bidirectional model which is trained\\nusing the ‚Äúmasked LM‚Äù (MLM) but without the\\n‚Äúnext sentence prediction‚Äù (NSP) task.\\nLTR & No NSP: A left-context-only model which\\nis trained using a standard Left-to-Right (LTR)\\nLM, rather than an MLM. The left-only constraint\\nwas also applied at Ô¨Åne-tuning, because removing\\nit introduced a pre-train/Ô¨Åne-tune mismatch that\\ndegraded downstream performance. Additionally,\\nthis model was pre-trained without the NSP task.\\nThis is directly comparable to OpenAI GPT, but\\nusing our larger training dataset, our input repre-\\nsentation, and our Ô¨Åne-tuning scheme.\\nWe Ô¨Årst examine the impact brought by the NSP\\ntask. In Table 5, we show that removing NSP\\nhurts performance signiÔ¨Åcantly on QNLI, MNLI,\\nand SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR\\nmodel performs worse than the MLM model on all'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='and SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR\\nmodel performs worse than the MLM model on all\\ntasks, with large drops on MRPC and SQuAD.\\nFor SQuAD it is intuitively clear that a LTR\\nmodel will perform poorly at token predictions,\\nsince the token-level hidden states have no right-\\nside context. In order to make a good faith at-\\ntempt at strengthening the LTR system, we added\\na randomly initialized BiLSTM on top. This does\\nsigniÔ¨Åcantly improve results on SQuAD, but the\\nresults are still far worse than those of the pre-\\ntrained bidirectional models. The BiLSTM hurts\\nperformance on the GLUE tasks.\\nWe recognize that it would also be possible to\\ntrain separate LTR and RTL models and represent\\neach token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='each token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL\\nmodel would not be able to condition the answer\\non the question; (c) this it is strictly less powerful\\nthan a deep bidirectional model, since it can use\\nboth left and right context at every layer.\\n5.2 Effect of Model Size\\nIn this section, we explore the effect of model size\\non Ô¨Åne-tuning task accuracy. We trained a number\\nof BERT models with a differing number of layers,\\nhidden units, and attention heads, while otherwise\\nusing the same hyperparameters and training pro-\\ncedure as described previously.\\nResults on selected GLUE tasks are shown in\\nTable 6. In this table, we report the average Dev\\nSet accuracy from 5 random restarts of Ô¨Åne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Set accuracy from 5 random restarts of Ô¨Åne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-\\ning examples, and is substantially different from\\nthe pre-training tasks. It is also perhaps surpris-\\ning that we are able to achieve such signiÔ¨Åcant\\nimprovements on top of models which are al-\\nready quite large relative to the existing literature.\\nFor example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018). By contrast, BERT BASE\\ncontains 110M parameters and BERT LARGE con-\\ntains 340M parameters.\\nIt has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='It has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated\\nby the LM perplexity of held-out training data\\nshown in Table 6. However, we believe that\\nthis is the Ô¨Årst work to demonstrate convinc-\\ningly that scaling to extreme model sizes also\\nleads to large improvements on very small scale\\ntasks, provided that the model has been sufÔ¨Å-\\nciently pre-trained. Peters et al. (2018b) presented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='mixed results on the downstream task impact of\\nincreasing the pre-trained bi-LM size from two\\nto four layers and Melamud et al. (2016) men-\\ntioned in passing that increasing hidden dimen-\\nsion size from 200 to 600 helped, but increasing\\nfurther to 1,000 did not bring further improve-\\nments. Both of these prior works used a feature-\\nbased approach ‚Äî we hypothesize that when the\\nmodel is Ô¨Åne-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspeciÔ¨Åc models can beneÔ¨Åt from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3 Feature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe Ô¨Åne-tuning approach, where a simple classiÔ¨Å-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly Ô¨Åne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere Ô¨Åxed features are extracted from the pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='cation layer is added to the pre-trained model, and\\nall parameters are jointly Ô¨Åne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere Ô¨Åxed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-speciÔ¨Åc model architecture to be added.\\nSecond, there are major computational beneÔ¨Åts\\nto pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). In the input to BERT, we\\nuse a case-preserving WordPiece model, and we\\ninclude the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams Dev Set Accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='include the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams Dev Set Accuracy\\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\\n3 768 12 5.84 77.9 79.8 88.4\\n6 768 3 5.24 80.6 82.2 90.7\\n6 768 12 4.68 81.9 84.8 91.3\\n12 768 12 3.99 84.4 86.7 92.9\\n12 1024 16 3.54 85.7 86.9 93.3\\n24 1024 16 3.23 86.6 87.8 93.7\\nTable 6: Ablation over BERT model size. #L = the\\nnumber of layers; #H = hidden size; #A = number of at-\\ntention heads. ‚ÄúLM (ppl)‚Äù is the masked LM perplexity\\nof held-out training data.\\nSystem Dev F1 Test F1\\nELMo (Peters et al., 2018a) 95.7 92.2\\nCVT (Clark et al., 2018) - 92.6\\nCSE (Akbik et al., 2018) - 93.1\\nFine-tuning approach\\nBERTLARGE 96.6 92.8\\nBERTBASE 96.4 92.4\\nFeature-based approach (BERTBASE )\\nEmbeddings 91.0 -\\nSecond-to-Last Hidden 95.6 -\\nLast Hidden 94.9 -\\nWeighted Sum Last Four Hidden 95.9 -\\nConcat Last Four Hidden 96.1 -\\nWeighted Sum All 12 Layers 95.5 -'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Embeddings 91.0 -\\nSecond-to-Last Hidden 95.6 -\\nLast Hidden 94.9 -\\nWeighted Sum Last Four Hidden 95.9 -\\nConcat Last Four Hidden 96.1 -\\nWeighted Sum All 12 Layers 95.5 -\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\nlayer in the output. We use the representation of\\nthe Ô¨Årst sub-token as the input to the token-level\\nclassiÔ¨Åer over the NER label set.\\nTo ablate the Ô¨Åne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without Ô¨Åne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classiÔ¨Åcation layer.\\nResults are presented in Table 7. BERT LARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='the classiÔ¨Åcation layer.\\nResults are presented in Table 7. BERT LARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind Ô¨Åne-tuning the entire model. This\\ndemonstrates that BERT is effective for both Ô¨Åne-\\ntuning and feature-based approaches.\\n6 Conclusion\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to beneÔ¨Åt from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these Ô¨Åndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='References\\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\\n2018. Contextual string embeddings for sequence\\nlabeling. In Proceedings of the 27th International\\nConference on Computational Linguistics , pages\\n1638‚Äì1649.\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817‚Äì1853.\\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe Ô¨Åfth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120‚Äì128. Association for Computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='dence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120‚Äì128. Association for Computa-\\ntional Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural language.\\nComputational linguistics, 18(4):467‚Äì479.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\ncrosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017) , pages 1‚Äì14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Evaluation (SemEval-2017) , pages 1‚Äì14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\nQuora question pairs.\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nning, and Quoc Le. 2018. Semi-supervised se-\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 1914‚Äì\\n1925.\\nRonan Collobert and Jason Weston. 2008. A uniÔ¨Åed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning. In Pro-\\nceedings of the 25th international conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Ronan Collobert and Jason Weston. 2008. A uniÔ¨Åed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning. In Pro-\\nceedings of the 25th international conference on\\nMachine learning, pages 160‚Äì167. ACM.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¬®ƒ±c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670‚Äì680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079‚Äì3087.\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09.\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Fei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09.\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via Ô¨Ålling in\\nthe . arXiv preprint arXiv:1801.07736.\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the 2016\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Language Technologies. Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\\nACL. Association for Computational Linguistics.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nFuru Wei, and Ming Zhou. 2018. Reinforced\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors. In\\nAdvances in neural information processing systems,\\npages 3294‚Äì3302.\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning , pages\\n1188‚Äì1196.\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge. In\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nefÔ¨Åcient framework for learning sentence represen-\\ntations. In International Conference on Learning\\nRepresentations.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tations. In International Conference on Learning\\nRepresentations.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26 , pages 3111‚Äì3119. Curran Associates,\\nInc.\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\\nable hierarchical distributed language model. In\\nD. Koller, D. Schuurmans, Y . Bengio, and L. Bot-\\ntou, editors, Advances in Neural Information Pro-\\ncessing Systems 21 , pages 1081‚Äì1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='cessing Systems 21 , pages 1081‚Äì1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In EMNLP.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 1532‚Äì\\n1543.\\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nIn ACL.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018a. Deep contextualized word rep-\\nresentations. In NAACL.\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\\nand Wen-tau Yih. 2018b. Dissecting contextual\\nword embeddings: Architecture and representation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n1499‚Äì1509.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='word embeddings: Architecture and representation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n1499‚Äì1509.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018. Improving language under-\\nstanding with unsupervised learning. Technical re-\\nport, OpenAI.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 2383‚Äì2392.\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nÔ¨Çow for machine comprehension. In ICLR.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of the 2013 conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Chuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of the 2013 conference on\\nempirical methods in natural language processing ,\\npages 1631‚Äì1642.\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\\n2018. U-net: Machine reading comprehension\\nwith unanswerable questions. arXiv preprint\\narXiv:1810.06638.\\nWilson L Taylor. 1953. Cloze procedure: A new\\ntool for measuring readability. Journalism Bulletin,\\n30(4):415‚Äì433.\\nErik F Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In\\nCoNLL.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\\nWord representations: A simple and general method\\nfor semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics, ACL ‚Äô10, pages 384‚Äì394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='for semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics, ACL ‚Äô10, pages 384‚Äì394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems, pages 6000‚Äì6010.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\\nPierre-Antoine Manzagol. 2008. Extracting and\\ncomposing robust features with denoising autoen-\\ncoders. In Proceedings of the 25th international\\nconference on Machine learning, pages 1096‚Äì1103.\\nACM.\\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\\nGlue: A multi-task benchmark and analysis platform'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='for natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: An-\\nalyzing and Interpreting Neural Networks for NLP ,\\npages 353‚Äì355.\\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\\ngranularity hierarchical attention fusion networks\\nfor reading comprehension and question answering.\\nIn Proceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers). Association for Computational Lin-\\nguistics.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\\nman. 2018. Neural network acceptability judg-\\nments. arXiv preprint arXiv:1805.12471.\\nAdina Williams, Nikita Nangia, and Samuel R Bow-\\nman. 2018. A broad-coverage challenge corpus\\nfor sentence understanding through inference. In\\nNAACL.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Le, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144.\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\\nLipson. 2014. How transferable are features in deep\\nneural networks? In Advances in neural information\\nprocessing systems, pages 3320‚Äì3328.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\\nLe. 2018. QANet: Combining local convolution\\nwith global self-attention for reading comprehen-\\nsion. In ICLR.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\\nChoi. 2018. Swag: A large-scale adversarial dataset\\nfor grounded commonsense inference. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP).\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='ings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP).\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. 2015. Aligning books and movies: Towards\\nstory-like visual explanations by watching movies\\nand reading books. In Proceedings of the IEEE\\ninternational conference on computer vision , pages\\n19‚Äì27.\\nAppendix for ‚ÄúBERT: Pre-training of\\nDeep Bidirectional Transformers for\\nLanguage Understanding‚Äù\\nWe organize the appendix into three sections:\\n‚Ä¢ Additional implementation details for BERT\\nare presented in Appendix A;\\n‚Ä¢ Additional details for our experiments are\\npresented in Appendix B; and\\n‚Ä¢ Additional ablation studies are presented in\\nAppendix C.\\nWe present additional ablation studies for\\nBERT including:\\n‚Äì Effect of Number of Training Steps; and\\n‚Äì Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT including:\\n‚Äì Effect of Number of Training Steps; and\\n‚Äì Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks\\nWe provide examples of the pre-training tasks in\\nthe following.\\nMasked LM and the Masking ProcedureAs-\\nsuming the unlabeled sentence is my dog is\\nhairy, and during the random masking procedure\\nwe chose the 4-th token (which corresponding to\\nhairy), our masking procedure can be further il-\\nlustrated by\\n‚Ä¢ 80% of the time: Replace the word with the\\n[MASK] token, e.g., my dog is hairy ‚Üí\\nmy dog is [MASK]\\n‚Ä¢ 10% of the time: Replace the word with a\\nrandom word, e.g., my dog is hairy ‚Üí my\\ndog is apple\\n‚Ä¢ 10% of the time: Keep the word un-\\nchanged, e.g., my dog is hairy ‚Üí my dog\\nis hairy. The purpose of this is to bias the\\nrepresentation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='representation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-\\nplaced by random words, so it is forced to keep\\na distributional contextual representation of ev-\\nery input token. Additionally, because random\\nreplacement only occurs for 1.5% of all tokens\\n(i.e., 10% of 15%), this does not seem to harm\\nthe model‚Äôs language understanding capability. In\\nSection C.2, we evaluate the impact this proce-\\ndure.\\nCompared to standard langauge model training,\\nthe masked LM only make predictions on 15% of\\ntokens in each batch, which suggests that more\\npre-training steps may be required for the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT (Ours)\\nTrm Trm Trm\\nTrm Trm Trm\\n...\\n...\\nTrm Trm Trm\\nTrm Trm Trm\\n...\\n...\\nOpenAI GPT\\nLstm\\nELMo\\nLstm Lstm\\nLstm Lstm Lstm\\nLstm Lstm Lstm\\nLstm Lstm Lstm\\n T1 T2  TN...\\n...\\n...\\n...\\n...\\n E1 E2  EN...\\n T1 T2 TN...\\n E1 E2  EN...\\n T1 T2  TN...\\n E1 E2  EN...\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\nOpenAI GPT are Ô¨Åne-tuning approaches, while ELMo is a feature-based approach.\\nto converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='to converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model\\nfar outweigh the increased training cost.\\nNext Sentence Prediction The next sentence\\nprediction task can be illustrated in the following\\nexamples.\\nInput = [CLS] the man went to [MASK] store [SEP]\\nhe bought a gallon [MASK] milk [SEP]\\nLabel = IsNext\\nInput = [CLS] the man [MASK] to the store [SEP]\\npenguin [MASK] are flight ##less birds [SEP]\\nLabel = NotNext\\nA.2 Pre-training Procedure\\nTo generate each training input sequence, we sam-\\nple two spans of text from the corpus, which we\\nrefer to as ‚Äúsentences‚Äù even though they are typ-\\nically much longer than single sentences (but can\\nbe shorter also). The Ô¨Årst sentence receives the A\\nembedding and the second receives the B embed-\\nding. 50% of the time B is the actual next sentence\\nthat follows A and 50% of the time it is a random'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='be shorter also). The Ô¨Årst sentence receives the A\\nembedding and the second receives the B embed-\\nding. 50% of the time B is the actual next sentence\\nthat follows A and 50% of the time it is a random\\nsentence, which is done for the ‚Äúnext sentence pre-\\ndiction‚Äù task. They are sampled such that the com-\\nbined length is ‚â§512 tokens. The LM masking is\\napplied after WordPiece tokenization with a uni-\\nform masking rate of 15%, and no special consid-\\neration given to partial word pieces.\\nWe train with batch size of 256 sequences (256\\nsequences * 512 tokens = 128,000 tokens/batch)\\nfor 1,000,000 steps, which is approximately 40\\nepochs over the 3.3 billion word corpus. We\\nuse Adam with learning rate of 1e-4, Œ≤1 = 0.9,\\nŒ≤2 = 0.999, L2 weight decay of 0.01, learning\\nrate warmup over the Ô¨Årst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='rate warmup over the Ô¨Årst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERT BASE was performed on 4\\nCloud TPUs in Pod conÔ¨Åguration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre-\\ntraining took 4 days to complete.\\nLonger sequences are disproportionately expen-\\nsive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor Ô¨Åne-tuning, most model hyperparameters are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor Ô¨Åne-tuning, most model hyperparameters are\\nthe same as in pre-training, with the exception of\\nthe batch size, learning rate, and number of train-\\ning epochs. The dropout probability was always\\nkept at 0.1. The optimal hyperparameter values\\nare task-speciÔ¨Åc, but we found the following range\\nof possible values to work well across all tasks:\\n‚Ä¢ Batch size: 16, 32\\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\\nTPU-now-offers-preemptible-pricing-and-global-\\navailability.html'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n‚Ä¢ Number of epochs: 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are Ô¨Åne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='approach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n‚Ä¢ GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n‚Ä¢ GPT uses a sentence separator ( [SEP]) and\\nclassiÔ¨Åer token ( [CLS]) which are only in-\\ntroduced at Ô¨Åne-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n‚Ä¢ GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='troduced at Ô¨Åne-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n‚Ä¢ GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n‚Ä¢ GPT used the same learning rate of 5e-5 for\\nall Ô¨Åne-tuning experiments; BERT chooses a\\ntask-speciÔ¨Åc Ô¨Åne-tuning learning rate which\\nperforms the best on the development set.\\nTo isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of Ô¨Åne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciÔ¨Åc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='models are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe Ô¨Ågure, E represents the input embedding, Ti\\nrepresents the contextual representation of tokeni,\\n[CLS] is the special symbol for classiÔ¨Åcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB Detailed Experimental Setup\\nB.1 Detailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur GLUE results in Table1 are obtained\\nfrom https://gluebenchmark.com/\\nleaderboard and https://blog.\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiÔ¨Å-\\ncation task (Williams et al., 2018). Given a pair of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='summarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiÔ¨Å-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the Ô¨Årst one.\\nQQP Quora Question Pairs is a binary classiÔ¨Å-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiÔ¨Åcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok \\n1\\n [SEP]... Tok \\nN\\nTok \\n1 ... Tok\\nM\\nQuestion Paragraph\\nBERT\\nE[CLS] E1  E2  EN\\nC\\n T1\\n  T2\\n  TN\\nSingle Sentence \\n...\\n...\\nBERT\\nTok 1  Tok 2  Tok N...[CLS]\\nE[CLS] E1  E2  EN\\nC\\n T1\\n  T2\\n  TN\\nSingle Sentence \\nB-PERO O\\n...\\n...E[CLS] E1  E[SEP]\\nClass \\nLabel\\n... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\nStart/End Span\\nClass \\nLabel\\nBERT\\nTok 1  Tok 2  Tok N...[CLS] Tok 1[CLS][CLS] Tok \\n1\\n [SEP]... Tok \\nN\\nTok \\n1 ... Tok\\nM\\nSentence 1\\n...\\nSentence 2\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classiÔ¨Åcation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classiÔ¨Åcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically ‚Äúacceptable‚Äù or not (Warstadt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='CoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classiÔ¨Åcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically ‚Äúacceptable‚Äù or not (Warstadt\\net al., 2018).\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='much less training data (Bentivogli et al., 2009).14\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that‚Äôs been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n14Note that we only report single-task Ô¨Åne-tuning results\\nin this paper. A multitask Ô¨Åne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n15https://gluebenchmark.com/faq'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='jority class.\\nC Additional Ablation Studies\\nC.1 Effect of Number of Training Steps\\nFigure 5 presents MNLI Dev accuracy after Ô¨Åne-\\ntuning from a checkpoint that has been pre-trained\\nfor ksteps. This allows us to answer the following\\nquestions:\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh Ô¨Åne-tuning accuracy?\\nAnswer: Yes, BERT BASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='almost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n200 400 600 800 1,000\\n76\\n78\\n80\\n82\\n84\\nPre-training Steps (Thousands)\\nMNLI Dev Accuracy\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after Ô¨Åne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nksteps. The x-axis is the value of k.\\nNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand Ô¨Åne-tuning, as the [MASK] symbol never ap-\\npears during the Ô¨Åne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both Ô¨Åne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='pears during the Ô¨Åne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both Ô¨Åne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npliÔ¨Åed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\nMasking Rates Dev Set Results\\nMASK SAME RND MNLI NER\\nFine-tune Fine-tune Feature-based\\n80% 10% 10% 84.2 95.4 94.9\\n100% 0% 0% 84.3 94.9 94.0\\n80% 0% 20% 84.1 95.2 94.6\\n80% 20% 0% 84.4 95.2 94.7\\n0% 20% 80% 83.7 94.8 94.6\\n0% 0% 100% 83.6 94.9 94.6\\nTable 8: Ablation over different masking strategies.\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; R ND means that\\nwe replace the target token with another random\\ntoken.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciÔ¨Åc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='token.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciÔ¨Åc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\nFrom the table it can be seen that Ô¨Åne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe R ND strategy performs much worse than our\\nstrategy as well.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='DeepSeekMath: Pushing the Limits of Mathematical\\nReasoning in Open Language Models\\nZhihong Shao1,2‚àó‚Ä†, Peiyi Wang1,3‚àó‚Ä†, Qihao Zhu1,3‚àó‚Ä†, Runxin Xu1, Junxiao Song1\\nXiao Bi1, Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1‚àó\\n1DeepSeek-AI, 2Tsinghua University,3Peking University\\n{zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com\\nhttps://github.com/deepseek-ai/DeepSeek-Math\\nAbstract\\nMathematical reasoning poses a significant challenge for language models due to its complex\\nand structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-\\ntraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common\\nCrawl, together with natural language and code data. DeepSeekMath 7B has achieved an\\nimpressive score of 51.7% on the competition-level MATH benchmark without relying on\\nexternal toolkits and voting techniques, approaching the performance level of Gemini-Ultra'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='impressive score of 51.7% on the competition-level MATH benchmark without relying on\\nexternal toolkits and voting techniques, approaching the performance level of Gemini-Ultra\\nand GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,\\nwe harness the significant potential of publicly available web data through a meticulously\\nengineered data selection pipeline. Second, we introduce Group Relative Policy Optimization\\n(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning\\nabilities while concurrently optimizing the memory usage of PPO.\\nFigure 1 |Top1 accuracy of open-source models on the competition-level MATH benchmark\\n(Hendrycks et al., 2021) without the use of external toolkits and voting techniques.\\n‚àóCore contributors.\\n‚Ä†Work done during internship at DeepSeek-AI.\\narXiv:2402.03300v3  [cs.CL]  27 Apr 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='1. Introduction\\nLarge language models (LLM) have revolutionized the approach to mathematical reasoning\\nin artificial intelligence, spurring significant advancements in both the quantitative reasoning\\nbenchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).\\nMoreover, these models have proven instrumental in assisting humans in solving complex\\nmathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,\\n2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible\\nopen-source models considerably trail behind in performance.\\nIn this study, we introduce DeepSeekMath, a domain-specific language model that signifi-\\ncantly outperforms the mathematical capabilities of open-source models and approaches the\\nperformance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-\\nMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-\\nMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This\\ndataset is extracted from the Common Crawl (CC) using a fastText-based classifier (Joulin et al.,\\n2016). In the initial iteration, the classifier is trained using instances from OpenWebMath (Paster\\net al., 2023) as positive examples, while incorporating a diverse selection of other web pages to\\nserve as negative examples. Subsequently, we employ the classifier to mine additional positive\\ninstances from the CC, which are further refined through human annotation. The classifier is\\nthen updated with this enhanced dataset to improve its performance. The evaluation results\\nindicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base\\n7B achieves 64.2% on GSM8K (Cobbe et al., 2021) and 36.2% on the competition-level MATH'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base\\n7B achieves 64.2% on GSM8K (Cobbe et al., 2021) and 36.2% on the competition-level MATH\\ndataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In\\naddition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese\\nmathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience\\nin mathematical data processing is a starting point for the research community, and there is\\nsignificant room for improvement in the future.\\nDeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as\\nwe notice that starting from a code training model is a better choice compared to a general\\nLLM. Furthermore, we observe the math training also improves model capability on MMLU\\n(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='LLM. Furthermore, we observe the math training also improves model capability on MMLU\\n(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only\\nenhance the model‚Äôs mathematical abilities but also amplifies general reasoning capabilities.\\nAfter pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with\\nchain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and\\ntool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct\\n7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.\\nFurthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant rein-\\nforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).\\nGRPO foregoes the critic model, instead estimating the baseline from group scores, significantly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='forcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).\\nGRPO foregoes the critic model, instead estimating the baseline from group scores, significantly\\nreducing training resources. By solely using a subset of English instruction tuning data, GRPO\\nobtains a substantial improvement over the strong DeepSeekMath-Instruct, including both\\nin-domain (GSM8K: 82.9% ‚Üí88.2%, MATH: 46.8% ‚Üí51.7%) and out-of-domain mathematical\\ntasks (e.g., CMATH: 84.6% ‚Üí88.8%) during the reinforcement learning phase. We also provide\\na unified paradigm to understand different methods, such as Rejection Sampling Fine-Tuning\\n(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and\\nGRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as\\neither direct or simplified RL techniques. We also conduct extensive experiments, e.g., online'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as\\neither direct or simplified RL techniques. We also conduct extensive experiments, e.g., online\\nv.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='to deeply investigate the essential elements of this paradigm. At last, we explain why our RL\\nboosts the performance of instruction-tuned models, and further summarize potential directions\\nto achieve more effective RL based on this unified paradigm.\\n1.1. Contributions\\nOur contribution includes scalable math pre-training, along with the exploration and analysis of\\nreinforcement learning.\\nMath Pre-Training at Scale\\n‚Ä¢ Our research provides compelling evidence that the publicly accessible Common Crawl\\ndata contains valuable information for mathematical purposes. By implementing a metic-\\nulously designed data selection pipeline, we successfully construct the DeepSeekMath\\nCorpus, a high-quality dataset of 120B tokens from web pages filtered for mathemati-\\ncal content, which is almost 7 times the size of the math web pages used by Minerva\\n(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath\\n(Paster et al., 2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='cal content, which is almost 7 times the size of the math web pages used by Minerva\\n(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath\\n(Paster et al., 2023).\\n‚Ä¢ Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance\\nwith Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not\\nthe only key factor in mathematical reasoning capability. A smaller model pre-trained on\\nhigh-quality data could achieve strong performance as well.\\n‚Ä¢ We share our findings from math training experiments. Code training prior to math\\ntraining improves models‚Äô ability to solve mathematical problems both with and without\\ntool use. This offers a partial answer to the long-standing question: does code training\\nimprove reasoning abilities?We believe it does, at least for mathematical reasoning.\\n‚Ä¢ Although training on arXiv papers is common, especially in many math-related papers, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='improve reasoning abilities?We believe it does, at least for mathematical reasoning.\\n‚Ä¢ Although training on arXiv papers is common, especially in many math-related papers, it\\nbrings no notable improvements on all mathematical benchmarks adopted in this paper.\\nExploration and Analysis of Reinforcement Learning\\n‚Ä¢ We introduce Group Relative Policy Optimization (GRPO), an efficient and effective\\nreinforcement learning algorithm. GRPO foregoes the critic model, instead estimating\\nthe baseline from group scores, significantly reducing training resources compared to\\nProximal Policy Optimization (PPO).\\n‚Ä¢ We demonstrate that GRPO significantly enhances the performance of our instruction-\\ntuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Further-\\nmore, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='more, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,\\nPPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training,\\noutcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so\\non to deeply investigate the essential elements of this paradigm.\\n‚Ä¢ Based on our unified paradigm, we explore the reasons behind the effectiveness of rein-\\nforcement learning, and summarize several potential directions to achieve more effective\\nreinforcement learning of LLMs.\\n1.2. Summary of Evaluations and Metrics\\n‚Ä¢ English and Chinese Mathematical Reasoning: We conduct comprehensive assessments\\nof our models on English and Chinese benchmarks, covering mathematical problems\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='from grade-school level to college level. English benchmarks include GSM8K (Cobbe\\net al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses\\n(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks\\ninclude MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong\\net al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate models‚Äô ability\\nto generate self-contained text solutions without tool use, and also the ability to solve\\nproblems using Python.\\nOn English benchmarks, DeepSeekMath-Base is competitive with the closed-source Min-\\nerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mis-\\ntral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether\\nthey‚Äôve undergone math pre-training or not, often by a significant margin. Notably,\\nDeepSeekMath-Base is superior on Chinese benchmarks, likely because we don‚Äôt follow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='they‚Äôve undergone math pre-training or not, often by a significant margin. Notably,\\nDeepSeekMath-Base is superior on Chinese benchmarks, likely because we don‚Äôt follow\\nprevious works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only\\nmath pre-training data, and also include high-quality non-English ones. With mathemati-\\ncal instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct\\nand DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over\\n50% on the competition-level MATH dataset for the first time within the open-source\\ncommunity.\\n‚Ä¢ Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal\\ntheorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle\\n(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates\\nstrong few-shot autoformalization performance.\\n‚Ä¢ Natural Language Understanding, Reasoning, and Code : To build a comprehensive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='strong few-shot autoformalization performance.\\n‚Ä¢ Natural Language Understanding, Reasoning, and Code : To build a comprehensive\\nprofile of models‚Äô general understanding, reasoning, and coding capabilities, we eval-\\nuate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)\\nbenchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering\\ndiverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 chal-\\nlenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval\\n(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code\\nlanguage models. Math pre-training benefits both language understanding and reasoning\\nperformance.\\n2. Math Pre-Training\\n2.1. Data Collection and Decontamination\\nIn this section, we will outline the process of constructing the DeepSeekMath Corpus from\\nCommon Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='In this section, we will outline the process of constructing the DeepSeekMath Corpus from\\nCommon Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates\\nhow to systematically gather a large-scale mathematical corpus from Common Crawl, starting\\nwith a seed corpus (e.g., a small but high-quality collection of math-related dataset). It‚Äôs worth\\nnoting that this approach is also applicable to other domains, such as coding.\\nFirst, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical\\nweb texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,\\n2016) to recall more OpenWebMath-like mathematical web pages. Specifically, we randomly\\nselect 500,000 data points from the seed corpus as positive training examples and another\\n500,000 web pages from Common Crawl as negative ones. We employ an open-source library1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='select 500,000 data points from the seed corpus as positive training examples and another\\n500,000 web pages from Common Crawl as negative ones. We employ an open-source library1\\nfor training, configuring the vector dimension to 256, learning rate to 0.1, the maximum length\\n1https://fasttext.cc\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Math Seed\\n Math Corpus\\n1. Train a FastTextModel 2. Recall Math-Related Webpages From Common Crawl\\n3. Discover Math-Related Domains4. Annotate Math-Related URL Path From Labelers\\nDeduplicated Common Crawl40B HTML pages\\nFigure 2 |An iterative pipeline that collects mathematical web pages from Common Crawl.\\nof word n-gram to 3, the minimum number of word occurrences to 3, and the number of\\ntraining epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based\\ndeduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then\\nrecall mathematical web pages from deduplicated Common Crawl with the fastText model.\\nTo filter out low-quality mathematical content, we rank the collected pages according to their\\nscores predicted by the fastText model, and only preserve the top-ranking ones. The volume\\nof data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='scores predicted by the fastText model, and only preserve the top-ranking ones. The volume\\nof data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and\\n160B tokens. In the first iteration, we choose to keep the top 40B tokens.\\nAfter the first iteration of data collection, numerous mathematical web pages remain un-\\ncollected, mainly because the fastText model is trained on a set of positive examples that lacks\\nsufficient diversity. We therefore identify additional mathematical web sources to enrich the seed\\ncorpus, so that we can optimize the fastText model. Specifically, we first organize the entire Com-\\nmon Crawl into disjoint domains; a domain is defined as web pages sharing the same base URL.\\nFor each domain, we calculate the percentage of web pages that are collected in the first iteration.\\nDomains where over 10% of the web pages have been collected are classified as math-related'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='For each domain, we calculate the percentage of web pages that are collected in the first iteration.\\nDomains where over 10% of the web pages have been collected are classified as math-related\\n(e.g., mathoverflow.net ). Subsequently, we manually annotate the URLs associated with\\nmathematical content within these identified domains (e.g., mathoverflow.net/questions).\\nWeb pages linked to these URLs, yet uncollected, will be added to the seed corpus. This ap-\\nproach enables us to gather more positive examples, thereby training an improved fastText\\nmodel capable of recalling more mathematical data in the subsequent iteration. After four\\niterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B\\ntokens. In the fourth iteration, we notice that nearly 98% of the data has already been collected\\nin the third iteration, so we decide to cease data collection.\\nTo avoid benchmark contamination, we follow Guo et al. (2024) to filter out web pages'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='in the third iteration, so we decide to cease data collection.\\nTo avoid benchmark contamination, we follow Guo et al. (2024) to filter out web pages\\ncontaining questions or answers from English mathematical benchmarks such as GSM8K (Cobbe\\net al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH\\n(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The filtering criteria are as follows: any\\ntext segment containing a 10-gram string that matches exactly with any sub-string from the\\nevaluation benchmarks is removed from our math training corpus. For benchmark texts that\\nare shorter than 10 grams but have at least 3 grams, we employ exact matching to filter out\\ncontaminated web pages.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2.2. Validating the Quality of the DeepSeekMath Corpus\\nWe run pre-training experiments to investigate how the DeepSeekMath Corpus is compared\\nwith the recently released math-training corpora:\\n‚Ä¢ MathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from\\ntextbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the\\nmajority (over 85%) sourced from arXiv;\\n‚Ä¢ OpenWebMath (Paster et al., 2023): CommonCrawl data filtered for mathematical content,\\ntotaling 13.6B tokens;\\n‚Ä¢ Proof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWeb-\\nMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B to-\\nkens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an\\narXiv:Web:Code ratio of 2:4:1.\\n2.2.1. Training Setting\\nWe apply math training to a general pre-trained language model with 1.3B parameters, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='arXiv:Web:Code ratio of 2:4:1.\\n2.2.1. Training Setting\\nWe apply math training to a general pre-trained language model with 1.3B parameters, which\\nshares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeek-\\nLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All\\nexperiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023)\\ntraining framework. Following the training practice of DeepSeek LLMs, we use the AdamW\\noptimizer (Loshchilov and Hutter, 2017) with ùõΩ1 = 0.9, ùõΩ2 = 0.95, and weight_decay = 0.1, along\\nwith a multi-step learning rate schedule where the learning rate reaches the peak after 2,000\\nwarmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to\\n10.0% of the peak after 90% of the training process. We set the maximum value of learning rate\\nto 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\\nMath Corpus Size'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='10.0% of the peak after 90% of the training process. We set the maximum value of learning rate\\nto 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\\nMath Corpus Size\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SAT MMLU\\nSTEM CMATH Gaokao\\nMathCloze\\nGaokao\\nMathQA\\nNo Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9%\\nMathPile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8%\\nOpenWebMath 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2%\\nProof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7%\\nDeepSeekMath Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6%\\nTable 1 |Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evalu-\\nated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer\\nwith a vocabulary size of 100K.\\n2.2.2. Evaluation Results\\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and\\nis the largest in size.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='with a vocabulary size of 100K.\\n2.2.2. Evaluation Results\\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and\\nis the largest in size.\\n‚Ä¢ High-quality: We evaluate downstream performance on 8 mathematical benchmarks using\\nfew-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear\\nperformance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that\\nthe model trained on the DeepSeekMath Corpus demonstrates better performance than\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Figure 3 |Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.\\nProof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of\\nDeepSeekMath Corpus is higher.\\n‚Ä¢ Multilingual: The DeepSeekMath Corpus encompasses data in multiple languages, pre-\\ndominantly featuring English and Chinese as the two most represented languages. As\\nshown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning\\nperformance in both English and Chinese. In contrast, existing mathematical corpora,\\nwhich are primarily English-centric, show limited improvement and may even hinder\\nperformance in Chinese mathematical reasoning.\\n‚Ä¢ Large-scale: The DeepSeekMath Corpus is several times larger than existing mathematical\\ncorpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-\\nMath Corpus, shows a steeper learning curve along with more lasting improvements. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='corpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-\\nMath Corpus, shows a steeper learning curve along with more lasting improvements. In\\ncontrast, the baseline corpora are much smaller, and have already been repeated multiple\\nrounds during training, with the resulting model performance quickly reaching a plateau.\\n2.3. Training and Evaluating DeepSeekMath-Base 7B\\nIn this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning\\nabilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: 56%\\nis from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github\\ncode, and the remaining 10% is natural language data from Common Crawl in both English and\\nChinese. We mainly adopt the training setting specified in Section 2.2.1, except that we set the\\nmaximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.\\nWe conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-\\nBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying\\non external tools, solve mathematical problems using tools, and conduct formal theorem proving.\\nBeyond mathematics, we also provide a more general profile of the base model, including its\\nperformance of natural language understanding, reasoning, and programming skills.\\nMathematical Problem Solving with Step-by-Step Reasoning We evaluate DeepSeekMath-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='performance of natural language understanding, reasoning, and programming skills.\\nMathematical Problem Solving with Step-by-Step Reasoning We evaluate DeepSeekMath-\\nBase‚Äôs performance of solving mathematical problems using few-shot chain-of-thought prompt-\\ning (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encom-\\npass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),\\nand CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks\\net al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse fields of mathematics\\nfrom elementary to college-level complexity.\\nAs shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight bench-\\nmarks among the open-source base models (including the widely-used general model Mistral\\n7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='marks among the open-source base models (including the widely-used general model Mistral\\n7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which\\nunderwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition-\\nlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over\\n10% absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base\\nmodel 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained\\non mathematical texts.\\nModel Size\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SAT MMLU\\nSTEM CMATH Gaokao\\nMathCloze\\nGaokao\\nMathQA\\nClosed-Source Base Model\\nMinerva 7B 16.2% 14.1% 7.7% - 35.6% - - -\\nMinerva 62B 52.4% 27.6% 12.0% - 53.9% - - -\\nMinerva 540B 58.8% 33.6% 17.6% - 63.9% - - -\\nOpen-Source Base Model\\nMistral 7B 40.3% 14.3% 9.2% 71.9% 51.1% 44.9% 5.1% 23.4%\\nLlemma 7B 37.4% 18.1% 6.3% 59.4% 43.1% 43.4% 11.9% 23.6%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Minerva 540B 58.8% 33.6% 17.6% - 63.9% - - -\\nOpen-Source Base Model\\nMistral 7B 40.3% 14.3% 9.2% 71.9% 51.1% 44.9% 5.1% 23.4%\\nLlemma 7B 37.4% 18.1% 6.3% 59.4% 43.1% 43.4% 11.9% 23.6%\\nLlemma 34B 54.0% 25.3% 10.3% 71.9% 52.9% 56.1% 11.9% 26.2%\\nDeepSeekMath-Base 7B 64.2% 36.2% 15.4% 84.4% 56.5% 71.7% 20.3% 35.3%\\nTable 2 |Comparisons between DeepSeekMath-Base 7B and strong base models on English and\\nChinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.\\nMinerva results are quoted from Lewkowycz et al. (2022a).\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Mathematical Problem Solving with Tool Use We evaluate program-aided mathematical\\nreasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,\\n2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program\\nwhere libraries such as math and sympy can be utilized for intricate computations. The execution\\nresult of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B\\noutperforms the prior state-of-the-art Llemma 34B.\\nModel Size Problem Solving w/ Tools Informal-to-Formal Proving\\nGSM8K+Python MATH+Python miniF2F-valid miniF2F-test\\nMistral 7B 48.5% 18.2% 18.9% 18.0%\\nCodeLlama 7B 27.1% 17.2% 16.3% 17.6%\\nCodeLlama 34B 52.7% 23.5% 18.5% 18.0%\\nLlemma 7B 41.0% 18.6% 20.6% 22.1%\\nLlemma 34B 64.6% 26.3% 21.0% 21.3%\\nDeepSeekMath-Base 7B 66.9% 31.4% 25.8% 24.6%\\nTable 3 |Few-shot evaluation of base models‚Äô ability to solve mathematical problems using tools'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Llemma 34B 64.6% 26.3% 21.0% 21.3%\\nDeepSeekMath-Base 7B 66.9% 31.4% 25.8% 24.6%\\nTable 3 |Few-shot evaluation of base models‚Äô ability to solve mathematical problems using tools\\nand the ability to conduct informal-to-formal theorem proving in Isabelle.\\nFormal Mathematics Formal proof automation is beneficial to ensure the accuracy and relia-\\nbility of mathematical proofs and enhance efficiency, with increasing attention in recent years.\\nWe evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,\\n2022) which is to generate a formal proof based on an informal statement, a formal counterpart\\nof the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a bench-\\nmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each\\nproblem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each\\nproblem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate\\nproof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)\\nto fill in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strong\\nperformance in proof autoformalization.\\nModel Size MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)\\nMistral 7B 62.4% 55.7% 28.0% 41.4%\\nDeepSeek-Coder-Base-v1.5‚Ä† 7B 42.9% 42.9% 40.2% 52.6%\\nDeepSeek-Coder-Base-v1.5 7B 49.1% 55.2% 43.2% 60.4%\\nDeepSeekMath-Base 7B 54.9% 59.5% 40.9% 52.6%\\nTable 4 |Evaluation on natural language understanding, reasoning, and code benchmarks.\\nDeepSeek-Coder-Base-v1.5‚Ä†is the checkpoint right before learning rate decay, which is used to\\ntrain DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='DeepSeek-Coder-Base-v1.5‚Ä†is the checkpoint right before learning rate decay, which is used to\\ntrain DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.\\nOn HumanEval and MBPP , we evaluate model performance under the zero-shot setting and a\\nfew-shot setting, respectively.\\nNatural Language Understanding, Reasoning, and Code We evaluate model performance of\\nnatural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun\\net al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits significant enhancements in per-\\nformance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),\\nillustrating the positive impact of math training on language understanding and reasoning.\\nAdditionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively\\nmaintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Over-\\nall, DeepSeekMath-Base 7B significantly outperforms the general model Mistral 7B (Jiang et al.,\\n2023) on the three reasoning and coding benchmarks.\\n3. Supervised Fine-T uning\\n3.1. SFT Data Curation\\nWe construct a mathematical instruction-tuning dataset covering English and Chinese problems\\nfrom different mathematical fields and of varying complexity levels: problems are paired with\\nsolutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='from different mathematical fields and of varying complexity levels: problems are paired with\\nsolutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,\\n2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number\\nof training examples is 776K.\\n‚Ä¢ English mathematical datasets: We annotate GSM8K and MATH problems with tool-\\nintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the\\ntraining set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or\\nPoT. Our English collection covers diverse fields of mathematics, e.g., algebra, probability,\\nnumber theory, calculus, and geometry.\\n‚Ä¢ Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning\\n76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-\\nintegrated reasoning format.\\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-\\nintegrated reasoning format.\\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B\\nIn this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruc-\\ntion tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until\\nreaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch\\nsize of 256 and a constant learning rate of 5e-5.\\nWe evaluate models‚Äô mathematical performance both without and with tool use, on 4\\nquantitative reasoning benchmarks in English and Chinese. We benchmark our model against\\nthe leading models of the time:\\n‚Ä¢ Closed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)\\nand GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil\\net al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1 3, as well as models recently'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='and GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil\\net al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1 3, as well as models recently\\nreleased by Chinese companies including (5) Baichuan-3 4, (6) the latest GLM-4 5 from the\\nGLM family (Du et al., 2022). These models are for general purposes, most of which have\\nundergone a series of alignment procedures.\\n‚Ä¢ Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeek-\\nAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)\\n2https://openai.com/blog/chatgpt-plugins#code-interpreter\\n3https://x.ai/model-card\\n4https://www.baichuan-ai.com\\n5https://open.bigmodel.cn/dev/api#glm-4\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathemat-\\nics including (5) InternLM2-Math 20B 6 which builds on InternLM2 and underwent math\\ntraining followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO\\ntraining (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised\\nreward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical\\nreasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,\\na version of instruction tuning that uses AI-evolved instructions) and PPO training with\\ntraining problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,\\n2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH,\\n(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fine-tuned to do tool-integrated\\nmathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B\\ninstruction-tuned on MathInstruct.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B\\ninstruction-tuned on MathInstruct.\\nAs shown in Table 5, under the evaluation setting where tool use is disallowed, DeepSeekMath-\\nInstruct 7B demonstrates strong performance of step-by-step reasoning. Notably, on the\\ncompetition-level MATH dataset, our model surpasses all open-source models and the ma-\\njority of proprietary models (e.g., Inflection-2 and Gemini Pro) by at least 9% absolute. This\\nis true even for models that are substantially larger (e.g., Qwen 72B) or have been specifi-\\ncally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While\\nDeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,\\nit still underperforms GPT-4 and Gemini Ultra.\\nUnder the evaluation setting where models are allowed to integrate natural language rea-\\nsoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Under the evaluation setting where models are allowed to integrate natural language rea-\\nsoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches\\nan accuracy of 60% on MATH, surpassing all existing open-source models. On the other bench-\\nmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is\\n10 times larger.\\n4. Reinforcement Learning\\n4.1. Group Relative Policy Optimization\\nReinforcement learning (RL) has been proven to be effective in further improving the mathe-\\nmatical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;\\nWang et al., 2023b). In this section, we introduce our efficient and effective RL algorithm, Group\\nRelative Policy Optimization (GRPO).\\n4.1.1. From PPO to GRPO\\nProximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is\\nwidely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is\\nwidely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes\\nLLMs by maximizing the following surrogate objective:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉ(ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n, (1)\\nwhere ùúãùúÉ and ùúãùúÉùëúùëôùëë are the current and old policy models, and ùëû, ùëúare questions and outputs\\nsampled from the question dataset and the old policy ùúãùúÉùëúùëôùëë, respectively. ùúÄis a clipping-related\\nhyper-parameter introduced in PPO for stabilizing training. ùê¥ùë° is the advantage, which is\\ncomputed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based\\n6https://github.com/InternLM/InternLM-Math\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Model Size English Benchmarks Chinese Benchmarks\\nGSM8K MATH MGSM-zh CMATH\\nChain-of-Thought Reasoning\\nClosed-Source Model\\nGemini Ultra - 94.4% 53.2% - -\\nGPT-4 - 92.0% 52.9% - 86.0%\\nInflection-2 - 81.4% 34.8% - -\\nGPT-3.5 - 80.8% 34.1% - 73.8%\\nGemini Pro - 86.5% 32.6% - -\\nGrok-1 - 62.9% 23.9% - -\\nBaichuan-3 - 88.2% 49.2% - -\\nGLM-4 - 87.6% 47.9% - -\\nOpen-Source Model\\nInternLM2-Math 20B 82.6% 37.7% - -\\nQwen 72B 78.9% 35.2% - -\\nMath-Shepherd-Mistral 7B 84.1% 33.0% - -\\nWizardMath-v1.1 7B 83.2% 33.0% - -\\nDeepSeek-LLM-Chat 67B 84.1% 32.6% 74.0% 80.3%\\nMetaMath 70B 82.3% 26.6% 66.4% 70.9%\\nSeaLLM-v2 7B 78.2% 27.5% 64.8% -\\nChatGLM3 6B 72.3% 25.7% - -\\nWizardMath-v1.0 70B 81.6% 22.7% 64.8% 65.4%\\nDeepSeekMath-Instruct 7B 82.9% 46.8% 73.2% 84.6%\\nDeepSeekMath-RL 7B 88.2% 51.7% 79.6% 88.8%\\nTool-Integrated Reasoning\\nClosed-Source Model\\nGPT-4 Code Interpreter - 97.0% 69.7% - -\\nOpen-Source Model\\nInternLM2-Math 20B 80.7% 54.3% - -\\nDeepSeek-LLM-Chat 67B 86.7% 51.1% 76.4% 85.4%\\nToRA 34B 80.7% 50.8% 41.2% 53.4%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Closed-Source Model\\nGPT-4 Code Interpreter - 97.0% 69.7% - -\\nOpen-Source Model\\nInternLM2-Math 20B 80.7% 54.3% - -\\nDeepSeek-LLM-Chat 67B 86.7% 51.1% 76.4% 85.4%\\nToRA 34B 80.7% 50.8% 41.2% 53.4%\\nMAmmoTH 70B 76.9% 41.8% - -\\nDeepSeekMath-Instruct 7B 83.7% 57.4% 72.0% 84.3%\\nDeepSeekMath-RL 7B 86.7% 58.8% 78.4% 87.6%\\nTable 5 |Performance of Open- and Closed-Source models with both Chain-of-Thought and\\nTool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority\\nvotes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all open-\\nsource models from 7B to 70B, as well as the majority of closed-source models. Although\\nDeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data\\nof GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëûùëû\\nùëúùëú!\\nùëúùëú\"\\nùëúùëú#\\nùëüùëü!\\nùëüùëü\"\\nùëüùëü#\\nùê¥ùê¥!\\nùê¥ùê¥\"\\nùê¥ùê¥#\\nùëûùëû ùëúùëú GAE ùê¥ùê¥\\nùëüùëü\\nùë£ùë£\\nReward \\nModel\\nPolicy \\nModel\\nValue \\nModel\\n‚Ä¶ ‚Ä¶ ‚Ä¶\\nPolicy \\nModel\\nReference \\nModel\\nReward \\nModel\\nPPO\\nGRPO\\nTrained\\nModels\\nFrozen\\nModelsReference \\nModel\\n‚äï\\nùêæùêæùêæùêæ\\nùêæùêæùêæùêæ\\nGroup \\nComputation\\nFigure 4 |Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead\\nestimating the baseline from group scores, significantly reducing training resources.\\non the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì. Thus, in PPO, a value function needs to\\nbe trained alongside the policy model and to mitigate over-optimization of the reward model,\\nthe standard approach is to add a per-token KL penalty from a reference model in the reward at\\neach token (Ouyang et al., 2022), i.e.,\\nùëüùë° = ùëüùúë(ùëû, ùëú‚â§ùë°)‚àíùõΩlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùëüùëíùëì (ùëúùë°|ùëû, ùëú<ùë°), (2)\\nwhere ùëüùúë is the reward model, ùúãùëüùëíùëì is the reference model, which is usually the initial SFT model,\\nand ùõΩ is the coefficient of the KL penalty.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëüùë° = ùëüùúë(ùëû, ùëú‚â§ùë°)‚àíùõΩlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùëüùëíùëì (ùëúùë°|ùëû, ùëú<ùë°), (2)\\nwhere ùëüùúë is the reward model, ùúãùëüùëíùëì is the reference model, which is usually the initial SFT model,\\nand ùõΩ is the coefficient of the KL penalty.\\nAs the value function employed in PPO is typically another model of comparable size as\\nthe policy model, it brings a substantial memory and computational burden. Additionally,\\nduring RL training, the value function is treated as a baseline in the calculation of the advantage\\nfor variance reduction. While in the LLM context, usually only the last token is assigned a\\nreward score by the reward model, which may complicate the training of a value function that is\\naccurate at each token. To address this, as shown in Figure 4, we propose Group Relative Policy\\nOptimization (GRPO), which obviates the need for additional value function approximation as\\nin PPO, and instead uses the average reward of multiple sampled outputs, produced in response'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Optimization (GRPO), which obviates the need for additional value function approximation as\\nin PPO, and instead uses the average reward of multiple sampled outputs, produced in response\\nto the same question, as the baseline. More specifically, for each question ùëû, GRPO samples a\\ngroup of outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}from the old policy ùúãùúÉùëúùëôùëë and then optimizes the policy model\\nby maximizing the following objective:\\nJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉ(ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x1a\\nmin\\n\\x14 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nÀÜùê¥ùëñ,ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nÀÜùê¥ùëñ,ùë°\\n\\x15\\n‚àíùõΩDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\x1b\\n,\\n(3)\\nwhere ùúÄ and ùõΩ are hyper-parameters, and ÀÜùê¥ùëñ,ùë° is the advantage calculated based on relative\\nrewards of the outputs inside each group only, which will be detailed in the following subsec-\\ntions. The group relative way that GRPO leverages to calculate the advantages, aligns well with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='rewards of the outputs inside each group only, which will be detailed in the following subsec-\\ntions. The group relative way that GRPO leverages to calculate the advantages, aligns well with\\nthe comparative nature of rewards models, as reward models are typically trained on datasets\\nof comparisons between outputs on the same question. Also note that, instead of adding KL\\npenalty in the reward, GRPO regularizes by directly adding the KL divergence between the\\ntrained policy and the reference policy to the loss, avoiding complicating the calculation of ÀÜùê¥ùëñ,ùë°.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Algorithm 1 Iterative Group Relative Policy Optimization\\nInput initial policy model ùúãùúÉinit ; reward models ùëüùúë; task prompts D; hyperparameters ùúÄ, ùõΩ, ùúá\\n1: policy model ùúãùúÉ ‚ÜêùúãùúÉinit\\n2: for iteration = 1, . . . , Ido\\n3: reference model ùúãùëüùëíùëì ‚ÜêùúãùúÉ\\n4: for step = 1, . . . , Mdo\\n5: Sample a batch Dùëè from D\\n6: Update the old policy model ùúãùúÉùëúùëôùëë ‚ÜêùúãùúÉ\\n7: Sample ùê∫ outputs {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (¬∑| ùëû)for each question ùëû ‚ààDùëè\\n8: Compute rewards {ùëüùëñ}ùê∫\\nùëñ=1 for each sampled output ùëúùëñ by running ùëüùúë\\n9: Compute ÀÜùê¥ùëñ,ùë° for the ùë°-th token of ùëúùëñ through group relative advantage estimation.\\n10: for GRPO iteration = 1, . . . ,ùúá do\\n11: Update the policy model ùúãùúÉ by maximizing the GRPO objective (Equation 21)\\n12: Update ùëüùúë through continuous training using a replay mechanism.\\nOutput ùúãùúÉ\\nAnd different from the KL penalty term used in (2), we estimate the KL divergence with the\\nfollowing unbiased estimator (Schulman, 2020):\\nDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\n=\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='following unbiased estimator (Schulman, 2020):\\nDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\n=\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àí1, (4)\\nwhich is guaranteed to be positive.\\n4.1.2. Outcome Supervision RL with GRPO\\nFormally, for each question ùëû, a group of outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}are sampled from the old\\npolicy model ùúãùúÉùëúùëôùëë. A reward model is then used to score the outputs, yielding ùê∫ rewards\\nr = {ùëü1, ùëü2, ¬∑¬∑¬∑ , ùëüùê∫}correspondingly. Subsequently, these rewards are normalized by subtracting\\nthe group average and dividing by the group standard deviation. Outcome supervision provides\\nthe normalized reward at the end of each output ùëúùëñ and sets the advantages ÀÜùê¥ùëñ,ùë° of all tokens in\\nthe output as the normalized reward, i.e., ÀÜùê¥ùëñ,ùë° = eùëüùëñ = ùëüùëñ‚àímean(r)\\nstd(r) , and then optimizes the policy by\\nmaximizing the objective defined in equation (3).\\n4.1.3. Process Supervision RL with GRPO\\nOutcome supervision only provides a reward at the end of each output, which may not be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='maximizing the objective defined in equation (3).\\n4.1.3. Process Supervision RL with GRPO\\nOutcome supervision only provides a reward at the end of each output, which may not be\\nsufficient and efficient to supervise the policy in complex mathematical tasks. Following Wang\\net al. (2023b), we also explore process supervision, which provides a reward at the end of\\neach reasoning step. Formally, given the question ùëûand ùê∫ sampled outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}, a\\nprocess reward model is used to score each step of the outputs, yielding corresponding rewards:\\nR = {{ùëüùëñùëõùëëùëíùë•(1)\\n1 , ¬∑¬∑¬∑ , ùëüùëñùëõùëëùëíùë•(ùêæ1 )\\n1 }, ¬∑¬∑¬∑ , {ùëüùëñùëõùëëùëíùë•(1)\\nùê∫ , ¬∑¬∑¬∑ , ùëüùëñùëõùëëùëíùë•(ùêæùê∫)\\nùê∫ }}, where ùëñùëõùëëùëíùë•(ùëó)is the end token index\\nof the ùëó-th step, and ùêæùëñ is the total number of steps in the ùëñ-th output. We also normalize these\\nrewards with the average and the standard deviation, i.e.,eùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ =\\nùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ ‚àímean(R)\\nstd(R) . Subsequently,\\nthe process supervision calculates the advantage of each token as the sum of the normalized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëñ =\\nùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ ‚àímean(R)\\nstd(R) . Subsequently,\\nthe process supervision calculates the advantage of each token as the sum of the normalized\\nrewards from the following steps, i.e., ÀÜùê¥ùëñ,ùë° = √ç\\nùëñùëõùëëùëíùë•(ùëó)‚â•ùë°eùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ , and then optimizes the policy by\\nmaximizing the objective defined in equation (3).\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='4.1.4. Iterative RL with GRPO\\nAs the reinforcement learning training process progresses, the old reward model may not be\\nsufficient to supervise the current policy model. Therefore, we also explore the iterative RL\\nwith GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the\\nreward model based on the sampling results from the policy model and continually train the\\nold reward model using a replay mechanism that incorporates 10% of historical data. Then, we\\nset the reference model as the policy model, and continually train the policy model with the\\nnew reward model.\\n4.2. Training and Evaluating DeepSeekMath-RL\\nWe conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-of-\\nthought-format questions related to GSM8K and MATH from the SFT data, which consists\\nof around 144K questions. We exclude other SFT questions to investigate the impact of RL\\non benchmarks that lack data throughout the RL phase. We construct the training set of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='of around 144K questions. We exclude other SFT questions to investigate the impact of RL\\non benchmarks that lack data throughout the RL phase. We construct the training set of\\nreward models following (Wang et al., 2023b). We train our initial reward model based on the\\nDeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the\\npolicy model as 1e-6. The KL coefficient is 0.04. For each question, we sample 64 outputs. The\\nmax length is set to 1024, and the training batch size is 1024. The policy model only has a single\\nupdate following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks\\nfollowing DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with\\nchain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks\\ncan be regarded as out-of-domain tasks.\\nTable 5 demonstrates the performance of open- and closed-source models with both chain-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='can be regarded as out-of-domain tasks.\\nTable 5 demonstrates the performance of open- and closed-source models with both chain-\\nof-thought and tool-integrated reasoning on English and Chinese benchmarks. We find that:\\n1) DeepSeekMath-RL 7B attains accuracies of 88.2% and 51.7% on GSM8K and MATH, respec-\\ntively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source\\nmodels in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,\\nDeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of\\nGSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope\\nof its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,\\nshowcasing the effectiveness of reinforcement learning.\\n5. Discussion\\nIn this section, we will share our findings in pre-training and RL experiments.\\n5.1. Lessons Learnt in Pre-Training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='showcasing the effectiveness of reinforcement learning.\\n5. Discussion\\nIn this section, we will share our findings in pre-training and RL experiments.\\n5.1. Lessons Learnt in Pre-Training\\nWe first share our experience in pre-training. Unless otherwise specified, we will adhere to\\nthe training settings outlined in Section 2.2.1. It is worth noting that, when referring to the\\nDeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of\\nthe data collection process.\\n5.1.1. Code Training Benefits Mathematical Reasoning\\nA popular yet unverified hypothesis suggests that code training improves reasoning. We attempt\\nto offer a partial response to this, particularly within the mathematical domain: code training\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Training Setting Training Tokens w/o Tool Use w/ Tool Use\\nGeneral Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python\\nNo Continual Training ‚Äì ‚Äì ‚Äì 2.9% 3.0% 12.3% 2.7% 2.3%\\nTwo-Stage Training\\nStage 1: General Training 400B ‚Äì ‚Äì 2.9% 3.2% 14.8% 3.3% 2.3%\\nStage 2: Math Training ‚Äì ‚Äì 150B 19.1% 14.4% 37.2% 14.3% 6.7%\\nStage 1: Code Training ‚Äì 400B ‚Äì 5.9% 3.6% 19.9% 12.4% 10.0%\\nStage 2: Math Training ‚Äì ‚Äì 150B 21.9% 15.3% 39.7% 17.4% 9.4%\\nOne-Stage Training\\nMath Training ‚Äì ‚Äì 150B 20.5% 13.1% 37.6% 11.4% 6.5%\\nCode & Math Mixed Training ‚Äì 400B 150B 17.6% 12.1% 36.3% 19.7% 13.5%\\nTable 6 |Investigation of how code affects mathematical reasoning under different training\\nsettings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning\\nperformance without and with tool use via few-shot chain-of-thought prompting and few-shot\\nprogram-of-thought prompting, respectively.\\nimproves models‚Äô ability to do mathematical reasoning both with and without tool use.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='program-of-thought prompting, respectively.\\nimproves models‚Äô ability to do mathematical reasoning both with and without tool use.\\nTo study how code training affects mathematical reasoning, we experimented with the\\nfollowing two-stage training and one-stage training settings:\\nTwo-Stage Training\\n‚Ä¢ Code Training for 400B Tokens‚ÜíMath Training for 150B Tokens: We train DeepSeek-\\nLLM 1.3B for 400B code tokens followed by 150B math tokens;\\n‚Ä¢ General Training for 400B Tokens ‚ÜíMath Training for 150B Tokens : As a control\\nexperiment, we also experiment with general tokens (sampled from a large-scale general\\ncorpus created by DeepSeek-AI) instead of code tokens in the first stage of training, in an\\nattempt to investigate the advantages of code tokens over general tokens in improving\\nmathematical reasoning.\\nOne-Stage Training\\n‚Ä¢ Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\\n‚Ä¢ Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='One-Stage Training\\n‚Ä¢ Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\\n‚Ä¢ Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-\\nlowing code training degrades coding performance. We investigate whether code tokens,\\nwhen mixed with math tokens for one-stage training, would still improve mathematical\\nreasoning and also alleviate the problem of catastrophic forgetting.\\nResults Table 6 and Table 7 demonstrate the downstream performance under different training\\nsettings.\\nCode training benefits program-aided mathematical reasoning, both under the two-stage\\ntraining and one-stage training settings. As shown in Table 6, under the two-stage training\\nsetting, code training alone already significantly enhances the ability to solve GSM8K and\\nMATH problems using Python. Math training in the second stage yields further improvements.\\nInterestingly, under the one-stage training setting, mixing code tokens and math tokens effec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='MATH problems using Python. Math training in the second stage yields further improvements.\\nInterestingly, under the one-stage training setting, mixing code tokens and math tokens effec-\\ntively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also\\nsynergizes coding (Table 7) and program-aided mathematical reasoning (Table 6).\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Training Setting Training Tokens MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)\\nGeneral Code Math\\nNo Continual Training ‚Äì ‚Äì ‚Äì 24.5% 28.1% 12.2% 13.0%\\nTwo-Stage Training\\nStage 1: General Training 400B ‚Äì ‚Äì 25.9% 27.7% 15.2% 13.6%\\nStage 2: Math Training ‚Äì ‚Äì 150B 33.1% 32.7% 12.8% 13.2%\\nStage 1: Code Training ‚Äì 400B ‚Äì 25.0% 31.5% 25.0% 40.0%\\nStage 2: Math Training ‚Äì ‚Äì 150B 36.2% 35.3% 12.2% 17.0%\\nOne-Stage Training\\nMath Training ‚Äì ‚Äì 150B 32.3% 32.5% 11.6% 13.2%\\nCode & Math Mixed Training ‚Äì 400B 150B 33.5% 35.6% 29.3% 39.4%\\nTable 7 |Investigation of how different settings of code and math training affect model perfor-\\nmance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM\\n1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.\\nOn HumanEval and MBPP , we conduct zero-shot and few-shot evaluations, respectively.\\nModel Size ArXiv Corpus\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SATMMLU\\nSTEMCMATHGaokao\\nMathCloze\\nGaokao'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Model Size ArXiv Corpus\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SATMMLU\\nSTEMCMATHGaokao\\nMathCloze\\nGaokao\\nMathQA\\nDeepSeek-LLM 1.3B\\nNo Math Training 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9%\\nMathPile 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8%\\nArXiv-RedPajama 3.3% 3.4% 4.0% 9.4% 9.0% 7.4% 0.8% 2.3%\\nDeepSeek-Coder-Base-v1.5 7B\\nNo Math Training 29.0% 12.5% 6.6% 40.6% 38.1% 45.9% 5.9% 21.1%\\nMathPile 23.6% 11.5% 7.0% 46.9% 35.8% 37.9% 4.2% 25.6%\\nArXiv-RedPajama 28.1% 11.1% 7.7% 50.0% 35.2% 42.6% 7.6% 24.8%\\nTable 8 |Effect of math training on different arXiv datasets. Model performance is evaluated\\nwith few-shot chain-of-thought prompting.\\nArXiv Corpus miniF2F-valid miniF2F-test\\nNo Math Training 20.1% 21.7%\\nMathPile 16.8% 16.4%\\nArXiv-RedPajama 14.8% 11.9%\\nTable 9 |Effect of math training on different arXiv corpora, the base model being DeepSeek-\\nCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ArXiv-RedPajama 14.8% 11.9%\\nTable 9 |Effect of math training on different arXiv corpora, the base model being DeepSeek-\\nCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.\\nCode training also improves mathematical reasoning without tool use. Under the two-stage\\ntraining setting, the initial stage of code training already results in moderate enhancements.\\nIt also boosts the efficiency of the subsequent math training, eventually leading to the best\\nperformance. However, combining code tokens and math tokens for one-stage training com-\\npromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,\\ndue to its limited scale, lacks the capacity to fully assimilate both code and mathematical data\\nsimultaneously.\\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='simultaneously.\\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev\\net al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='detailed analysis regarding their impact on mathematical reasoning has not been extensively\\nconducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem\\nineffective in improving mathematical reasoning. We experiment with models of different sizes,\\nincluding DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv\\ncorpora that underwent varied processing pipelines:\\n‚Ä¢ MathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and filtering\\nheuristic rules, over 85% of which are scientific arXiv papers;\\n‚Ä¢ ArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX files with preambles,\\ncomments, macros, and bibliographies removed, totaling 28.0B tokens.\\nIn our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-\\nCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='In our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-\\nCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective\\nin improving mathematical reasoning. When trained on a arXiv-only corpus, both models dis-\\nplay no notable improvements or even deterioration across various mathematical benchmarks of\\ndifferent complexities employed in this study. These benchmarks include quantitative reasoning\\ndatasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table\\n8), and formal mathematics like miniF2F (Table 9).\\nHowever, this conclusion has its limitations and should be taken with a grain of salt. We\\nhave not yet studied:\\n‚Ä¢ The impact of arXiv tokens on specific math-related tasks not included in this research,\\nsuch as informalization of theorems which is to convert formal statements or proofs to\\ntheir informal versions;\\n‚Ä¢ The effect of arXiv tokens when combined with other types of data;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='such as informalization of theorems which is to convert formal statements or proofs to\\ntheir informal versions;\\n‚Ä¢ The effect of arXiv tokens when combined with other types of data;\\n‚Ä¢ Whether the benefits of arXiv papers would manifest themselves at a larger model scale.\\nThus, further exploration is required, which we leave for future studies.\\n5.2. Insights of Reinforcement Learning\\n5.2.1. Towards to a Unified Paradigm\\nIn this section, we provide a unified paradigm to analyze different training methods, such as\\nSFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the\\nunified paradigm. Generally, the gradient with respect to the parameter ùúÉof a training method\\ncan be written as:\\n‚àáùúÉJA(ùúÉ)= E[(ùëû, ùëú)‚àºD|       {z       }\\nùê∑ùëéùë°ùëé ùëÜùëúùë¢ùëüùëêùëí\\n]\\n¬©\\xad\\xad\\xad\\n¬´\\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂A(ùëû, ùëú, ùë°, ùúãùëüùëì )\\n|               {z               }\\nùê∫ùëüùëéùëëùëñùëíùëõùë° ùê∂ùëúùëíùëìùëìùëñùëêùëñùëíùëõùë°\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n¬™¬Æ¬Æ¬Æ\\n¬¨\\n. (5)\\nThere exist three key components: 1) Data SourceD, which determines the training data; 2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùê∫ùê∂A(ùëû, ùëú, ùë°, ùúãùëüùëì )\\n|               {z               }\\nùê∫ùëüùëéùëëùëñùëíùëõùë° ùê∂ùëúùëíùëìùëìùëñùëêùëñùëíùëõùë°\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n¬™¬Æ¬Æ¬Æ\\n¬¨\\n. (5)\\nThere exist three key components: 1) Data SourceD, which determines the training data; 2)\\nReward Functionùúãùëüùëì , which is the source of the training reward signal; 3) Algorithm A: which\\nprocesses the training data and the reward signal to the gradient coefficient ùê∫ùê∂ that determines\\nthe magnitude of the penalty or reinforcement for the data. We analyze several representative\\nmethods based on such a unified paradigm:\\n‚Ä¢ Supervised Fine-tuning (SFT): SFT fine-tunes pretrained model on human selected SFT\\ndata.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Methods Data Source Reward Function Gradient Coefficient\\nSFT ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ) - 1\\nRFT ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû) Rule Equation 10\\nDPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû) Rule Equation 14\\nOnline RFT ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû) Rule Equation 10\\nPPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû) Model Equation 18\\nGRPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉ(ùëÇ|ùëû) Model Equation 21\\nTable 10 |The data source and gradient coefficient of different methods. ùëÉùë†ùëìùë° denotes the data\\ndistribution of supervised fine-tuning datasets. ùúãùúÉùë†ùëìùë° and ùúãùúÉ denote the supervised fine-tuned\\nmodel and the real-time policy model during the online training process, respectively.\\n0 2000 4000 6000 8000\\nSteps\\n56\\n58\\n60\\n62\\n64\\n66Acc (%)\\nGSM8K\\n0 2000 4000 6000 8000\\nSteps\\n27\\n28\\n29\\n30Acc (%)\\nMATH\\nRFT Online RFT GRPO+OS GRPO+PS\\nFigure 5 |Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained\\nusing various methods, on two benchmarks.\\n‚Ä¢ Rejection Sampling Fine-tuning (RFT) : RFT further fine-tunes the SFT model on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='using various methods, on two benchmarks.\\n‚Ä¢ Rejection Sampling Fine-tuning (RFT) : RFT further fine-tunes the SFT model on the\\nfiltered outputs sampled from the SFT model based on SFT questions. RFT filters the\\noutputs based on the correctness of their answers.\\n‚Ä¢ Direct Preference Optimization (DPO): DPO further refines the SFT model by fine-tuning\\nit on augmented outputs sampled from the SFT model, using pair-wise DPO loss.\\n‚Ä¢ Online Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT\\ninitiates the policy model using the SFT model and refines it by fine-tuning with the\\naugmented outputs sampled from the real-time policy model.\\n‚Ä¢ PPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces\\nit with the outputs sampled from the real-time policy model.\\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a\\nmore detailed derivation process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='it with the outputs sampled from the real-time policy model.\\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a\\nmore detailed derivation process.\\nObservation about Data Source We divide the data source into two categories, online sam-\\npling, and offline sampling. Online sampling denotes that the training data is from the explo-\\nration results of the real-time training policy model, while offline sampling denotes that the\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='0 1300 2300 3300 4300 5300\\nSteps\\n83\\n84\\n85\\n86\\n87\\n88\\n89Acc (%)\\nGSM8K\\n0 1300 2300 3300 4300 5300\\nSteps\\n47\\n48\\n49\\n50\\n51\\n52Acc (%)\\nMATH\\nIteration-0 Iteration-1 Iteration-2\\nFigure 6 |Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on\\ntwo benchmarks.\\ntraining data is from the sampling results of the initial SFT model. RFT and DPO follow the\\noffline style, while Online RFT and GRPO follow the online style.\\nAs shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two\\nbenchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but\\ngains an absolute advantage in the later stage, demonstrating the superiority of online training.\\nThis is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,\\nwith the sampled data revealing only minor differences. In the later stage, however, the data\\nsampled from the actor will exhibit more significant differences, and real-time data sampling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='with the sampled data revealing only minor differences. In the later stage, however, the data\\nsampled from the actor will exhibit more significant differences, and real-time data sampling\\nwill offer greater advantages.\\nObservation about Gradient Coefficient The algorithm processes the reward signal to the\\ngradient coefficient to update the model parameter. We divide the reward function as ‚ÄòRule‚Äô\\nand ‚ÄòModel‚Äô in our experiments. Rule refers to judging the quality of a response based on\\nthe correctness of the answer, and Model denotes that we train a reward model to score each\\nresponse. The training data of the reward model is based on the rule judgment. Equations 10\\nand 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its\\ngradient coefficient based on the reward value provided by the reward model. This allows for\\ndifferential reinforcement and penalization of responses according to their varying magnitudes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='gradient coefficient based on the reward value provided by the reward model. This allows for\\ndifferential reinforcement and penalization of responses according to their varying magnitudes.\\nIn contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly\\nreinforces all responses with correct answers at the same level of intensity.\\nAs demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the efficiency\\nof altering positive and negative gradient coefficients. In addition, GRPO+PS shows superior\\nperformance compared to GRPO+OS, indicating the benefits of using fine-grained, step-aware\\ngradient coefficients. Furthermore, we explore the iterative RL, in our experiments, we conduct\\ntwo rounds of iteration. As shown in Figure 6, we notice that the iterative RL significantly\\nimproves the performance, especially at the first iteration.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='1 4 8 16 32 64\\nK: The number of candidates\\n82\\n84\\n86\\n88\\n90\\n92\\n94\\n96\\n98Acc (%)\\nGSM8K\\n1 4 8 16 32 64\\nK: The number of candidates\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85Acc (%)\\nMATH\\nMaj@K-Instruct Maj@K-RL Pass@K-Instruct Pass@K-RL\\nFigure 7 |The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH\\n(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.\\n5.2.2. Why RL Works?\\nIn this paper, we conduct reinforcement learning based on a subset of instruction tuning\\ndata, and it achieves significant performance enhancement upon the instruction tuning model.\\nTo further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K\\naccuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances\\nMaj@K‚Äôs performance but not Pass@K. These findings indicate that RL enhances the model‚Äôs\\noverall performance by rendering the output distribution more robust, in other words, it seems'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Maj@K‚Äôs performance but not Pass@K. These findings indicate that RL enhances the model‚Äôs\\noverall performance by rendering the output distribution more robust, in other words, it seems\\nthat the improvement is attributed to boosting the correct response from TopK rather than\\nthe enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a\\nmisalignment problem in reasoning tasks within the SFT model, showing that the reasoning\\nperformance of SFT models can be improved through a series of preference alignment strategies\\n(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).\\n5.2.3. How to Achieve More Effective RL?\\nWe demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified\\nparadigm to understand different representative training methods. Within this paradigm, all\\nmethods are conceptualized as either direct or simplified RL techniques. As summarized in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='paradigm to understand different representative training methods. Within this paradigm, all\\nmethods are conceptualized as either direct or simplified RL techniques. As summarized in\\nEquation 5, there exist three key components: Data Source, Algorithm, and Reward Function.\\nWe provide some potential future directions about the three components.\\nData Source Data source is the raw material of all training methods. In the context of RL, we\\nspecifically refer to the data source as the unlabeled questions with the outputs sampled from\\nthe policy model. In this paper, we only use the questions from the instruction tuning stage and\\na naive nucleus sampling to sample outputs. We think this is a potential reason that our RL\\npipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline\\non out-of-distribution question prompts, in conjunction with advanced sampling (decoding)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline\\non out-of-distribution question prompts, in conjunction with advanced sampling (decoding)\\nstrategies, like those based on tree-search methods (Yao et al., 2023). Also, theefficient inference\\ntechniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='the exploration efficiency of policy models, also play an exceedingly important role.\\nAlgorithms Algorithms process the data and reward signal to the gradient coefficient to update\\nthe model parameter. Based on Equation 5, to some extent, all methods now fully TRUST the\\nsignal of the reward function to increase or decrease the conditional probability of a certain\\ntoken. However, it is impossible to ensure the reward signal is always reliable, especially in\\nextremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023),\\nwhich have been carefully annotated by well-trained annotators, still contain approximately 20%\\nof incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm\\nthat is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,\\n2023) alignment methods will bring a fundamental change to the learning algorithms.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='that is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,\\n2023) alignment methods will bring a fundamental change to the learning algorithms.\\nReward Function Reward function is the source of the training signal. In RL, the reward\\nfunction is usually the neural reward model. We think there exist three important directions for\\nreward models: 1) How to enhance the generalization ability of the reward model. The reward\\nmodel must be effectively generalized to handle out-of-distribution questions and advanced\\ndecoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of\\nLLMs rather than improve their fundamental capabilities; 2) How to reflect the uncertainty\\nof reward model. The uncertainty could potentially act as a linking bridge between the weak\\nreward model and the weak-to-strong learning algorithms; 3) How to efficiently build high-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='of reward model. The uncertainty could potentially act as a linking bridge between the weak\\nreward model and the weak-to-strong learning algorithms; 3) How to efficiently build high-\\nquality process reward models that can provide fine-grained training signals for the reasoning\\nprocess (Lightman et al., 2023; Wang et al., 2023b).\\n6. Conclusion, Limitation, and Future Work\\nWe present DeepSeekMath, which outperforms all open-source models on the competition-\\nlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is\\ninitialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with\\na significant component of the training data being 120B math tokens sourced from Common\\nCrawl. Our extensive ablation study shows web pages offer significant potential for high-quality\\nmathematical data, while arXiv may not as beneficial as we expected. We introduce Group'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Crawl. Our extensive ablation study shows web pages offer significant potential for high-quality\\nmathematical data, while arXiv may not as beneficial as we expected. We introduce Group\\nRelative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which\\ncan notably improve mathematical reasoning capabilities with less memory consumption. The\\nexperiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached\\na high score on benchmarks. We also provide a unified paradigm to understand a series of\\nmethods and summarize several potential directions for more effective reinforcement learning.\\nAlthough DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,\\nits capability on geometry and theorem-proof are relatively weaker than closed models. For\\ninstance, in our dry run, the model cannot handle problems related to triangles and ellipses,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='its capability on geometry and theorem-proof are relatively weaker than closed models. For\\ninstance, in our dry run, the model cannot handle problems related to triangles and ellipses,\\nwhich may indicate data selection bias in pre-training and fine-tuning. In addition, restricted\\nby the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could\\nimprove its performance with few-shot inputs, while DeepSeekMath shows similar performance\\nin zero-shot and few-shot evaluation. In the future, we will further improve our engineered\\ndata selection pipeline to construct more high-quality pre-trained corpus. In addition, we will\\nexplore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.\\n7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='References\\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\\nK. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,\\nE. Pitler, T. P . Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P . R. Barham, T. Hennigan,\\nB. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,\\nK. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,\\nB. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,\\nM. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal\\nmodels. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https:\\n//doi.org/10.48550/arXiv.2312.11805.\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.\\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Bider-\\nman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint\\narXiv:2310.10631, 2023.\\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen\\ntechnical report. arXiv preprint arXiv:2309.16609, 2023.\\nC. Burns, P . Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,\\nM. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with\\nweak supervision. arXiv preprint arXiv:2312.09390, 2023.\\nChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c\\nom/THUDM/ChatGLM3.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c\\nom/THUDM/ChatGLM3.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet,\\nF. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='URL https://arxiv.org/abs/2107.03374.\\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:\\n10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\\nK. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168, 2021.\\nT. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL\\nhttps://github.com/togethercomputer/RedPajama-Data.\\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,\\nabs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485\\n50/arXiv.2401.02954.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model\\npretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 320‚Äì335,\\n2022.\\nL. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang, J. Callan, and G. Neubig. PAL: program-\\naided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\\nJ. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,\\npages 10764‚Äì10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.\\nhtml.\\nZ. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A tool-\\nintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.\\ndoi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745\\n2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='integrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.\\ndoi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745\\n2.\\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming\\n‚Äì the rise of code intelligence, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\\n2021.\\nHigh-flyer. Hai-llm: È´òÊïà‰∏îËΩªÈáèÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂ∑•ÂÖ∑, 2023. URL https://www.high-flyer.c\\nn/en/blog/hai-llm.\\nInflection AI. Inflection-2, 2023. URL https://inflection.ai/inflection-2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2021.\\nHigh-flyer. Hai-llm: È´òÊïà‰∏îËΩªÈáèÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂ∑•ÂÖ∑, 2023. URL https://www.high-flyer.c\\nn/en/blog/hai-llm.\\nInflection AI. Inflection-2, 2023. URL https://inflection.ai/inflection-2.\\nA. Q. Jiang, S. Welleck, J. P . Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,\\nsketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint\\narXiv:2210.12283, 2022.\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\nA. Joulin, E. Grave, P . Bojanowski, M. Douze, H. J√©gou, and T. Mikolov. Fasttext. zip: Compress-\\ning text classification models. arXiv preprint arXiv:1612.03651, 2016.\\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\\nEfficient memory management for large language model serving with pagedattention. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\\nEfficient memory management for large language model serving with pagedattention. In\\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative\\ndecoding. In International Conference on Machine Learning, pages 19274‚Äì19286. PMLR,\\n2023.\\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . Ramasesh, A. Slone,\\nC. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with\\nlanguage models. Advances in Neural Information Processing Systems, 35:3843‚Äì3857, 2022a.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . V . Ramasesh, A. Slone,\\nC. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V . Misra. Solving\\nquantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,\\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems\\n35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New\\nOrleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.\\ncc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr\\nact-Conference.html.\\nH. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101, 2017.\\nH. Luo, Q. Sun, C. Xu, P . Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.\\nWizardmath: Empowering mathematical reasoning for large language models via reinforced\\nevol-instruct. arXiv preprint arXiv:2308.09583, 2023.\\nS. Mishra, M. Finlayson, P . Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sab-\\nharwal, P . Clark, and A. Kalyan. LILA: A unified benchmark for mathematical reasoning.\\nIn Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\\nEmirates, December 7-11, 2022, pages 5807‚Äì5832. Association for Computational Linguistics,\\n2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/\\n2022.emnlp-main.392.\\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/\\n2022.emnlp-main.392.\\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,\\nC. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,\\nabs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485\\n50/arXiv.2312.00738.\\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems, 35:27730‚Äì27744, 2022.\\nK. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality\\nmathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL\\nhttps://doi.org/10.48550/arXiv.2310.06786.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL\\nhttps://doi.org/10.48550/arXiv.2310.06786.\\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between auto-\\nmatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,\\nProceedings of the 2nd Workshopon Practical Aspects of Automated Reasoning, PAAR-2010,\\nEdinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1‚Äì10.\\nEasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd.\\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,\\nabs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference\\noptimization: Your language model is secretly a reward model. 2023.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='J. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app\\nrox.html.\\nJ. Schulman, P . Moritz, S. Levine, M. Jordan, and P . Abbeel. High-dimensional continuous\\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\\nJ. Schulman, F. Wolski, P . Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\\nD. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\\nfR3wGCk-IXp.\\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for\\nhuman alignment. arXiv preprint arXiv:2306.17492, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='fR3wGCk-IXp.\\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for\\nhuman alignment. arXiv preprint arXiv:2306.17492, 2023.\\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V . Le,\\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\\nthem. arXiv preprint arXiv:2210.09261, 2022.\\nT. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro\\nsoft.com/ai-anthology/terence-tao/.\\nH. Touvron, L. Martin, K. Stone, P . Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\\nP . Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,\\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\\nR. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P . S. Koura,\\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P . Mishra,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P . S. Koura,\\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P . Mishra,\\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P . Xu, Z. Yan,\\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288,\\n2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.\\n09288.\\nT. H. Trinh, Y. Wu, Q. V . Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 625(7995):476‚Äì482, 2024.\\nP . Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models\\nbetter reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='P . Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models\\nbetter reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.\\nP . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify\\nand reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b.\\nZ. Wang, R. Xia, and P . Liu. Generative AI for math: Part I - mathpile: A billion-token-scale\\npretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.\\nURL https://doi.org/10.48550/arXiv.2312.17120.\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V . Le, and D. Zhou.\\nChain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.\\nURL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf\\n4f15af0f7b31abca4-Abstract-Conference.html.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese\\nelementary school math test?, 2023.\\nM. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.\\nMu√±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International\\nConference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170\\nof Lecture Notes in Computer Science, pages 33‚Äì38. Springer, 2008. doi: 10.1007/978-3-540-7\\n1067-7\\\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7 .\\nH. Xia, T. Ge, P . Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting\\nspeculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,\\neditors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909‚Äì\\n3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20\\n23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20\\n23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.\\nH. Xia, Z. Yang, Q. Dong, P . Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking efficiency\\nin large language model inference: A comprehensive survey of speculative decoding. arXiv\\npreprint arXiv:2401.07851, 2024.\\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\\nDeliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,\\n2023.\\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.\\nMetamath: Bootstrap your own mathematical questions for large language models. CoRR,\\nabs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485\\n50/arXiv.2309.12284.\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485\\n50/arXiv.2309.12284.\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning\\nmathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.\\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align\\nlanguage models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b.\\nX. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building\\nmath generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:\\n10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653.\\nK. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level\\nmathematics. arXiv preprint arXiv:2109.00110, 2021.\\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mathematics. arXiv preprint arXiv:2109.00110, 2021.\\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A\\nhuman-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.\\ndoi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A. Appendix\\nA.1. Analysis of Reinforcement Learning\\nWe provide the detailed derivation of the data source and gradient coefficient (algorithm and\\nreward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and\\nGRPO.\\nA.1.1. Supervised Fine-tuning\\nThe objective of Supervised Fine-tuning is maximizing the following objective:\\nJùëÜùêπùëá(ùúÉ)= E[ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (6)\\nThe gradient of JùëÜùêπùëá(ùúÉ)is:\\n‚àáùúÉJùëÜùêπùëá = E[ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (7)\\nData Source: The dataset employed for SFT. Reward Function: This can be regarded as human\\nselection. Gradient Coefficient: always set to 1.\\nA.1.2. Rejection Sampling Fine-tuning\\nRejection Sampling Fine-tuning first samples multiple outputs from the supervised fine-tuned\\nLLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.\\nFormally, the objective of RFT is to maximize the following objectives:\\nJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Formally, the objective of RFT is to maximize the following objectives:\\nJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (8)\\nThe gradient of JùëÖùêπùëá(ùúÉ)is:\\n‚àáùúÉJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (9)\\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\\nRule (whether the answer is correct or not). Gradient Coefficient:\\nùê∫ùê∂ùëÖùêπùëá(ùëû, ùëú, ùë°)= I(ùëú)=\\n(\\n1 the answer of o is correct\\n0 the answer of o is incorrect (10)\\nA.1.3. Online Rejection Sampling Fine-tuning\\nThe only difference between RFT and Online RFT is that the outputs of Online RFT are sampled\\nfrom the real-time policy model ùúãùúÉ, rather than from the SFT model ùúãùúÉùë†ùëìùë° . Therefore, the gradient\\nof online RFT is:\\n‚àáùúÉJùëÇùëõùëÖùêπùëá (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (11)\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A.1.4. Direct Preference Optimization (DPO)\\nThe objective of DPO is:\\nJùê∑ùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]log ùúé¬©\\xad\\n¬´\\nùõΩ 1\\n|ùëú+|\\n|ùëú+|‚àëÔ∏Å\\nùë°=1\\nlog\\nùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\nùúãref(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)‚àíùõΩ 1\\n|ùëú‚àí|\\n|ùëú‚àí|‚àëÔ∏Å\\nùë°=1\\nlog\\nùúãùúÉ(ùëú‚àí\\n<ùë°|ùëû, ùëú‚àí\\n<ùë°)\\nùúãref(ùëú‚àí\\n<ùë°|ùëû, ùëú‚àí\\n<ùë°)\\n¬™¬Æ\\n¬¨\\n(12)\\nThe gradient of Jùê∑ùëÉùëÇ(ùúÉ)is:\\n‚àáùúÉJùê∑ùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]¬©\\xad\\n¬´\\n1\\n|ùëú+|\\n|ùëú+|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)‚àáùúÉlog ùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\n‚àí 1\\n|ùëú‚àí|\\n|ùëú‚àí|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)‚àáùúÉlog ùúãùúÉ(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)¬™¬Æ\\n¬¨\\n(13)\\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\\nhuman preference in the general domain (can be ‚ÄòRule‚Äô in mathematical tasks). Gradient\\nCoefficient:\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)= ùúé\\n\\x12\\nùõΩlog\\nùúãùúÉ(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)\\nùúãref(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)‚àíùõΩlog\\nùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\nùúãref(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\n\\x13\\n(14)\\nA.1.5. Proximal Policy Optimization (PPO)\\nThe objective of PPO is:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n. (15)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='The objective of PPO is:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n. (15)\\nTo simplify the analysis, it is assumed that the model only has a single update following each\\nexploration stage, thereby ensuring that ùúãùúÉùëúùëôùëë = ùúãùúÉ. In this case, we can remove the min and clip\\noperation:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°. (16)\\nThe gradient of JùëÉùëÉùëÇ(ùúÉ)is:\\n‚àáùúÉJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùê¥ùë°‚àáùúÉlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°) (17)\\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\\nreward model. Gradient Coefficient:\\nùê∫ùê∂ùëÉùëÉùëÇ(ùëû, ùëú, ùë°, ùúãùúÉùëüùëö)= ùê¥ùë°, (18)\\nwhere ùê¥ùë° is the advantage, which is computed by applying Generalized Advantage Estimation\\n(GAE) (Schulman et al., 2015), based on the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì.\\nA.1.6. Group Relative Policy Optimization (GRPO)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='(GAE) (Schulman et al., 2015), based on the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì.\\nA.1.6. Group Relative Policy Optimization (GRPO)\\nThe objective of GRPO is (assume ùúãùúÉùëúùëôùëë = ùúãùúÉ for simplified analysis):\\nJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x14 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nÀÜùê¥ùëñ,ùë° ‚àíùõΩ(\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àí1)\\n\\x15\\n.\\n(19)\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 29, 'page_label': '30', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='The gradient of Jùê∫ùëÖùëÉùëÇ (ùúÉ)is:\\n‚àáùúÉJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x14\\nÀÜùê¥ùëñ,ùë° +ùõΩ\\n\\x12ùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëúùëñ,<ùë°) ‚àí1\\n\\x13\\x15\\n‚àáùúÉlog ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°).\\n(20)\\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\\nreward model. Gradient Coefficient:\\nùê∫ùê∂ùê∫ùëÖùëÉùëÇ (ùëû, ùëú, ùë°, ùúãùúÉùëüùëö)= ÀÜùê¥ùëñ,ùë° +ùõΩ\\n\\x12ùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëúùëñ,<ùë°) ‚àí1\\n\\x13\\n, (21)\\nwhere ÀÜùê¥ùëñ,ùë° is computed based on the group reward scores.\\n30')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751dbca",
   "metadata": {},
   "source": [
    "### Embedding and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd48255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8adaf0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x3174f41a0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {self.model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    # def get_embedding_dimension(self) -> int:\n",
    "    #     \"\"\"Get the embedding dimension of the model\"\"\"\n",
    "    #     if not self.model:\n",
    "    #         raise ValueError(\"Model not loaded\")\n",
    "    #     return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "## initialize the embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dcd7cc",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57b3ab9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x3174f4ad0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b230999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Self-Attention with Relative Position Representations\\nPeter Shaw\\nGoogle\\npetershaw@google.com\\nJakob Uszkoreit\\nGoogle Brain\\nusz@google.com\\nAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\nAbstract\\nRelying entirely on an attention mechanism,\\nthe Transformer introduced by Vaswani et\\nal. (2017) achieves state-of-the-art results for\\nmachine translation. In contrast to recurrent\\nand convolutional neural networks, it does\\nnot explicitly model relative or absolute po-\\nsition information in its structure. Instead,\\nit requires adding representations of abso-\\nlute positions to its inputs. In this work\\nwe present an alternative approach, extend-\\ning the self-attention mechanism to efÔ¨Åciently\\nconsider representations of the relative posi-\\ntions, or distances between sequence elements.\\nOn the WMT 2014 English-to-German and\\nEnglish-to-French translation tasks, this ap-\\nproach yields improvements of 1.3 BLEU and\\n0.3 BLEU over absolute position representa-\\ntions, respectively. Notably, we observe that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='English-to-French translation tasks, this ap-\\nproach yields improvements of 1.3 BLEU and\\n0.3 BLEU over absolute position representa-\\ntions, respectively. Notably, we observe that\\ncombining relative and absolute position rep-\\nresentations yields no further improvement in\\ntranslation quality. We describe an efÔ¨Åcient\\nimplementation of our method and cast it as an\\ninstance of relation-aware self-attention mech-\\nanisms that can generalize to arbitrary graph-\\nlabeled inputs.\\n1 Introduction\\nRecent approaches to sequence to sequence learn-\\ning typically leverage recurrence (Sutskever et al.,\\n2014), convolution (Gehring et al., 2017; Kalch-\\nbrenner et al., 2016), attention (Vaswani et al.,\\n2017), or a combination of recurrence and atten-\\ntion (Bahdanau et al., 2014; Cho et al., 2014; Lu-\\nong et al., 2015; Wu et al., 2016) as basic building\\nblocks. These approaches incorporate information\\nabout the sequential position of elements differ-\\nently.\\nRecurrent neural networks (RNNs) typically'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='blocks. These approaches incorporate information\\nabout the sequential position of elements differ-\\nently.\\nRecurrent neural networks (RNNs) typically\\ncompute a hidden state ht, as a function of their\\ninput at time t and a previous hidden state ht‚àí1,\\ncapturing relative and absolute positions along the\\ntime dimension directly through their sequential\\nstructure. Non-recurrent models do not necessar-\\nily consider input elements sequentially and may\\nhence require explicitly encoding position infor-\\nmation to be able to use sequence order.\\nOne common approach is to use position encod-\\nings which are combined with input elements to\\nexpose position information to the model. These\\nposition encodings can be a deterministic func-\\ntion of position (Sukhbaatar et al., 2015; Vaswani\\net al., 2017) or learned representations. Convolu-\\ntional neural networks inherently capture relative\\npositions within the kernel size of each convolu-\\ntion. They have been shown to still beneÔ¨Åt from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='et al., 2017) or learned representations. Convolu-\\ntional neural networks inherently capture relative\\npositions within the kernel size of each convolu-\\ntion. They have been shown to still beneÔ¨Åt from\\nposition encodings (Gehring et al., 2017), how-\\never.\\nFor the Transformer, which employs neither\\nconvolution nor recurrence, incorporating explicit\\nrepresentations of position information is an espe-\\ncially important consideration since the model is\\notherwise entirely invariant to sequence ordering.\\nAttention-based models have therefore used posi-\\ntion encodings or biased attention weights based\\non distance (Parikh et al., 2016).\\nIn this work we present an efÔ¨Åcient way of\\nincorporating relative position representations in\\nthe self-attention mechanism of the Transformer.\\nEven when entirely replacing its absolute position\\nencodings, we demonstrate signiÔ¨Åcant improve-\\nments in translation quality on two machine trans-\\nlation tasks.\\nOur approach can be cast as a special case of ex-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='encodings, we demonstrate signiÔ¨Åcant improve-\\nments in translation quality on two machine trans-\\nlation tasks.\\nOur approach can be cast as a special case of ex-\\ntending the self-attention mechanism of the Trans-\\nformer to considering arbitrary relations between\\nany two elements of the input, a direction we plan\\nto explore in future work on modeling labeled, di-\\nrected graphs.\\narXiv:1803.02155v2  [cs.CL]  12 Apr 2018'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2 Background\\n2.1 Transformer\\nThe Transformer (Vaswani et al., 2017) em-\\nploys an encoder-decoder structure, consisting of\\nstacked encoder and decoder layers. Encoder\\nlayers consist of two sublayers: self-attention\\nfollowed by a position-wise feed-forward layer.\\nDecoder layers consist of three sublayers: self-\\nattention followed by encoder-decoder attention,\\nfollowed by a position-wise feed-forward layer.\\nIt uses residual connections around each of the\\nsublayers, followed by layer normalization (Ba\\net al., 2016). The decoder uses masking in its self-\\nattention to prevent a given output position from\\nincorporating information about future output po-\\nsitions during training.\\nPosition encodings based on sinusoids of vary-\\ning frequency are added to encoder and decoder\\ninput elements prior to the Ô¨Årst layer. In contrast\\nto learned, absolute position representations, the\\nauthors hypothesized that sinusoidal position en-\\ncodings would help the model to generalize to se-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='to learned, absolute position representations, the\\nauthors hypothesized that sinusoidal position en-\\ncodings would help the model to generalize to se-\\nquence lengths unseen during training by allowing\\nit to learn to attend also by relative position. This\\nproperty is shared by our relative position repre-\\nsentations which, in contrast to absolute position\\nrepresentations, are invariant to the total sequence\\nlength.\\nResidual connections help propagate position\\ninformation to higher layers.\\n2.2 Self-Attention\\nSelf-attention sublayers employ hattention heads.\\nTo form the sublayer output, results from each\\nhead are concatenated and a parameterized linear\\ntransformation is applied.\\nEach attention head operates on an input se-\\nquence, x = (x1,...,x n) of n elements where\\nxi ‚àà Rdx , and computes a new sequence z =\\n(z1,...,z n) of the same length where zi ‚ààRdz .\\nEach output element, zi, is computed as\\nweighted sum of a linearly transformed input el-\\nements:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV ) (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='(z1,...,z n) of the same length where zi ‚ààRdz .\\nEach output element, zi, is computed as\\nweighted sum of a linearly transformed input el-\\nements:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV ) (1)\\nEach weight coefÔ¨Åcient, Œ±ij, is computed using\\na softmax function:\\nŒ±ij = exp eij‚àën\\nk=1 exp eik\\nAnd eij is computed using a compatibility func-\\ntion that compares two input elements:\\neij = (xiWQ)(xjWK)T\\n‚àödz\\n(2)\\nScaled dot product was chosen for the compat-\\nibility function, which enables efÔ¨Åcient computa-\\ntion. Linear transformations of the inputs add suf-\\nÔ¨Åcient expressive power.\\nWQ, WK, WV ‚ààRdx√ódz are parameter matri-\\nces. These parameter matrices are unique per layer\\nand attention head.\\n3 Proposed Architecture\\n3.1 Relation-aware Self-Attention\\nWe propose an extension to self-attention to con-\\nsider the pairwise relationships between input ele-\\nments. In this sense, we model the input as a la-\\nbeled, directed, fully-connected graph.\\nThe edge between input elements xi and xj is\\nrepresented by vectors aV\\nij,aK'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='ments. In this sense, we model the input as a la-\\nbeled, directed, fully-connected graph.\\nThe edge between input elements xi and xj is\\nrepresented by vectors aV\\nij,aK\\nij ‚ààRda . The mo-\\ntivation for learning two distinct edge represen-\\ntations is that aV\\nij and aK\\nij are suitable for use in\\neq. (3) and eq. (4), respectively, without requiring\\nadditional linear transformations. These represen-\\ntations can be shared across attention heads. We\\nuse da = dz.\\nWe modify eq. (1) to propagate edge informa-\\ntion to the sublayer output:\\nzi =\\nn‚àë\\nj=1\\nŒ±ij(xjWV + aV\\nij) (3)\\nThis extension is presumably important for\\ntasks where information about the edge types se-\\nlected by a given attention head is useful to down-\\nstream encoder or decoder layers. However, as ex-\\nplored in 4.3, this may not be necessary for ma-\\nchine translation.\\nWe also, importantly, modify eq. (2) to consider\\nedges when determining compatibility:\\neij =\\nxiWQ(xjWK + aK\\nij )T\\n‚àödz\\n(4)\\nThe primary motivation for using simple addi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='chine translation.\\nWe also, importantly, modify eq. (2) to consider\\nedges when determining compatibility:\\neij =\\nxiWQ(xjWK + aK\\nij )T\\n‚àödz\\n(4)\\nThe primary motivation for using simple addi-\\ntion to incorporate edge representations in eq. (3)\\nand eq. (4) is to enable an efÔ¨Åcient implementation\\ndescribed in 3.3.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='x1 x2 x3 ‚Ä¶ xnx4\\naV\\n2,1=wV\\n-1\\naK\\n2,1=wK\\n-1\\naV\\n2,4=wV\\n2\\naK\\n2,4=wK\\n2\\naV\\n4,n=wV\\nk\\naK\\n4,n=wK\\nk\\nFigure 1: Example edges representing relative posi-\\ntions, or the distance between elements. We learn rep-\\nresentations for each relative position within a clipping\\ndistance k. The Ô¨Ågure assumes 2 <= k <= n‚àí4.\\nNote that not all edges are shown.\\n3.2 Relative Position Representations\\nFor linear sequences, edges can capture infor-\\nmation about the relative position differences be-\\ntween input elements. The maximum relative po-\\nsition we consider is clipped to a maximum abso-\\nlute value of k. We hypothesized that precise rel-\\native position information is not useful beyond a\\ncertain distance. Clipping the maximum distance\\nalso enables the model to generalize to sequence\\nlengths not seen during training. Therefore, we\\nconsider 2k+ 1unique edge labels.\\naK\\nij = wK\\nclip(j‚àíi,k)\\naV\\nij = wV\\nclip(j‚àíi,k)\\nclip(x,k) = max(‚àík,min(k,x))\\nWe then learn relative position representations\\nwK = (wK\\n‚àík,...,w K'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='consider 2k+ 1unique edge labels.\\naK\\nij = wK\\nclip(j‚àíi,k)\\naV\\nij = wV\\nclip(j‚àíi,k)\\nclip(x,k) = max(‚àík,min(k,x))\\nWe then learn relative position representations\\nwK = (wK\\n‚àík,...,w K\\nk ) and wV = (wV\\n‚àík,...,w V\\nk )\\nwhere wK\\ni ,wV\\ni ‚ààRda .\\n3.3 EfÔ¨Åcient Implementation\\nThere are practical space complexity concerns\\nwhen considering edges between input elements,\\nas noted by VeliÀáckovi¬¥c et al. (2017), which consid-\\ners unlabeled graph inputs to an attention model.\\nFor a sequence of length n and h attention\\nheads, we reduce the space complexity of storing\\nrelative position representations from O(hn2da)\\nto O(n2da) by sharing them across each heads.\\nAdditionally, relative position representations can\\nbe shared across sequences. Therefore, the over-\\nall self-attention space complexity increases from\\nO(bhndz) to O(bhndz + n2da). Given da = dz,\\nthe size of the relative increase depends on n\\nbh.\\nThe Transformer computes self-attention efÔ¨Å-\\nciently for all sequences, heads, and positions in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='O(bhndz) to O(bhndz + n2da). Given da = dz,\\nthe size of the relative increase depends on n\\nbh.\\nThe Transformer computes self-attention efÔ¨Å-\\nciently for all sequences, heads, and positions in\\na batch using parallel matrix multiplication opera-\\ntions (Vaswani et al., 2017). Without relative posi-\\ntion representations, each eij can be computed us-\\ning bhparallel multiplications of n√ódz and dz √ón\\nmatrices. Each matrix multiplication computes eij\\nfor all sequence positions, for a particular head\\nand sequence. For any sequence and head, this\\nrequires sharing the same representation for each\\nposition across all compatibility function applica-\\ntions (dot products) with other positions.\\nWhen we consider relative positions the repre-\\nsentations differ with different pairs of positions.\\nThis prevents us from computing all eij for all\\npairs of positions in a single matrix multiplication.\\nWe also want to avoid broadcasting relative po-\\nsition representations. However, both issues can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='This prevents us from computing all eij for all\\npairs of positions in a single matrix multiplication.\\nWe also want to avoid broadcasting relative po-\\nsition representations. However, both issues can\\nbe resolved by splitting the computation of eq. (4)\\ninto two terms:\\neij =\\nxiWQ(xjWK)T + xiWQ(aK\\nij )T\\n‚àödz\\n(5)\\nThe Ô¨Årst term is identical to eq. (2), and can be\\ncomputed as described above. For the second term\\ninvolving relative position representations, tensor\\nreshaping can be used to computenparallel multi-\\nplications of bh√ódz and dz√ónmatrices. Each ma-\\ntrix multiplication computes contributions to eij\\nfor all heads and batches, corresponding to a par-\\nticular sequence position. Further reshaping al-\\nlows adding the two terms. The same approach\\ncan be used to efÔ¨Åciently compute eq. (3).\\nFor our machine translation experiments, the re-\\nsult was a modest 7% decrease in steps per sec-\\nond, but we were able to maintain the same model\\nand batch sizes on P100 GPUs as Vaswani et\\nal. (2017).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='sult was a modest 7% decrease in steps per sec-\\nond, but we were able to maintain the same model\\nand batch sizes on P100 GPUs as Vaswani et\\nal. (2017).\\n4 Experiments\\n4.1 Experimental Setup\\nWe use the tensor2tensor 1 library for training and\\nevaluating our model.\\nWe evaluated our model on the WMT 2014\\nmachine translation task, using the WMT 2014\\nEnglish-German dataset consisting of approxi-\\nmately 4.5M sentence pairs and the 2014 WMT\\nEnglish-French dataset consisting of approxi-\\nmately 36M sentence pairs.\\n1The tensor2tensor library is available at https://\\ngithub.com/tensorflow/tensor2tensor.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Model Position Information EN-DE BLEU EN-FR BLEU\\nTransformer (base) Absolute Position Representations 26.5 38.2\\nTransformer (base) Relative Position Representations 26.8 38.7\\nTransformer (big) Absolute Position Representations 27.9 41.2\\nTransformer (big) Relative Position Representations 29.2 41.5\\nTable 1: Experimental results for WMT 2014 English-to-German (EN-DE) and English-to-French (EN-FR) trans-\\nlation tasks, using newstest2014 test set.\\nFor all experiments, we split tokens into a\\n32,768 word-piece vocabulary (Wu et al., 2016).\\nWe batched sentence pairs by approximate length,\\nand limited input and output tokens per batch to\\n4096 per GPU. Each resulting training batch con-\\ntained approximately 25,000 source and 25,000\\ntarget tokens.\\nWe used the Adam optimizer (Kingma and Ba,\\n2014) with Œ≤1 = 0.9, Œ≤2 = 0.98, and œµ = 10‚àí9.\\nWe used the same warmup and decay strategy for\\nlearning rate as Vaswani et al. (2017), with 4,000\\nwarmup steps. During training, we employed la-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2014) with Œ≤1 = 0.9, Œ≤2 = 0.98, and œµ = 10‚àí9.\\nWe used the same warmup and decay strategy for\\nlearning rate as Vaswani et al. (2017), with 4,000\\nwarmup steps. During training, we employed la-\\nbel smoothing of value œµls = 0.1 (Szegedy et al.,\\n2016). For evaluation, we used beam search with\\na beam size of 4 and length penalty Œ±= 0.6 (Wu\\net al., 2016).\\nFor our base model, we used 6 encoder and de-\\ncoder layers, dx = 512, dz = 64, 8 attention\\nheads, 1024 feed forward inner-layer dimensions,\\nand Pdropout = 0.1. When using relative posi-\\ntion encodings, we used clipping distance k= 16,\\nand used unique edge representations per layer and\\nhead. We trained for 100,000 steps on 8 K40\\nGPUs, and did not use checkpoint averaging.\\nFor our big model, we used 6 encoder and de-\\ncoder layers, dx = 1024, dz = 64, 16 attention\\nheads, 4096 feed forward inner-layer dimensions,\\nand Pdropout = 0.3 for EN-DE and Pdropout = 0.1\\nfor EN-FR. When using relative position encod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='coder layers, dx = 1024, dz = 64, 16 attention\\nheads, 4096 feed forward inner-layer dimensions,\\nand Pdropout = 0.3 for EN-DE and Pdropout = 0.1\\nfor EN-FR. When using relative position encod-\\nings, we used k = 8, and used unique edge repre-\\nsentations per layer. We trained for 300,000 steps\\non 8 P100 GPUs, and averaged the last 20 check-\\npoints, saved at 10 minute intervals.\\n4.2 Machine Translation\\nWe compared our model using only relative po-\\nsition representations to the baseline Transformer\\n(Vaswani et al., 2017) with sinusoidal position en-\\ncodings. We generated baseline results to iso-\\nlate the impact of relative position representations\\nfrom any other changes to the underlying library\\nand experimental conÔ¨Åguration.\\nFor English-to-German our approach improved\\nperformance over our baseline by 0.3 and 1.3\\nBLEU for the base and big conÔ¨Ågurations, respec-\\ntively. For English-to-French it improved by 0.5\\nand 0.3 BLEU for the base and big conÔ¨Ågurations,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='performance over our baseline by 0.3 and 1.3\\nBLEU for the base and big conÔ¨Ågurations, respec-\\ntively. For English-to-French it improved by 0.5\\nand 0.3 BLEU for the base and big conÔ¨Ågurations,\\nrespectively. In our experiments we did not ob-\\nserve any beneÔ¨Åt from including sinusoidal posi-\\ntion encodings in addition to relative position rep-\\nresentations. The results are shown in Table 1.\\n4.3 Model Variations\\nWe performed several experiments modifying var-\\nious aspects of our model. All of our experi-\\nments in this section use the base model conÔ¨Ågura-\\ntion without any absolute position representations.\\nBLEU scores are calculated on the WMT English-\\nto-German task using the development set, new-\\nstest2013.\\nWe evaluated the effect of varying the clipping\\ndistance, k, of the maximum absolute relative po-\\nsition difference. Notably, for k ‚â•2, there does\\nnot appear to be much variation in BLEU scores.\\nHowever, as we use multiple encoder layers, pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='distance, k, of the maximum absolute relative po-\\nsition difference. Notably, for k ‚â•2, there does\\nnot appear to be much variation in BLEU scores.\\nHowever, as we use multiple encoder layers, pre-\\ncise relative position information may be able to\\npropagate beyond the clipping distance. The re-\\nsults are shown in Table 2.\\nk EN-DE BLEU\\n0 12.5\\n1 25.5\\n2 25.8\\n4 25.9\\n16 25.8\\n64 25.9\\n256 25.8\\nTable 2: Experimental results for varying the clipping\\ndistance, k.\\nWe also evaluated the impact of ablating each of\\nthe two relative position representations deÔ¨Åned in\\nsection 3.1, aV\\nij in eq. (3) andaK\\nij in eq. (4). Includ-\\ning relative position representations solely when\\ndetermining compatibility between elements may'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='be sufÔ¨Åcient, but further work is needed to deter-\\nmine whether this is true for other tasks. The re-\\nsults are shown in Table 3.\\naVij aKij EN-DE BLEU\\nYes Yes 25.8\\nNo Yes 25.8\\nYes No 25.3\\nNo No 12.5\\nTable 3: Experimental results for ablating relative po-\\nsition representations aV\\nij and aK\\nij .\\n5 Conclusions\\nIn this paper we presented an extension to self-\\nattention that can be used to incorporate rela-\\ntive position information for sequences, which im-\\nproves performance for machine translation.\\nFor future work, we plan to extend this mecha-\\nnism to consider arbitrary directed, labeled graph\\ninputs to the Transformer. We are also inter-\\nested in nonlinear compatibility functions to com-\\nbine input representations and edge representa-\\ntions. For both of these extensions, a key consid-\\neration will be determining efÔ¨Åcient implementa-\\ntions.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\\nton. 2016. Layer normalization. arXiv preprint\\narXiv:1607.06450 .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='eration will be determining efÔ¨Åcient implementa-\\ntions.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\\nton. 2016. Layer normalization. arXiv preprint\\narXiv:1607.06450 .\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\\ngio. 2014. Neural machine translation by jointly\\nlearning to align and translate. arXiv preprint\\narXiv:1409.0473 .\\nKyunghyun Cho, Bart Van Merri ¬®enboer, Caglar Gul-\\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio. 2014. Learning\\nphrase representations using rnn encoder-decoder\\nfor statistical machine translation. arXiv preprint\\narXiv:1406.1078 .\\nJonas Gehring, Michael Auli, David Grangier, De-\\nnis Yarats, and Yann N Dauphin. 2017. Convolu-\\ntional sequence to sequence learning. arXiv preprint\\narXiv:1705.03122 .\\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\\nAaron van den Oord, Alex Graves, and Koray\\nKavukcuoglu. 2016. Neural machine translation in\\nlinear time. arXiv preprint arXiv:1610.10099.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,\\nAaron van den Oord, Alex Graves, and Koray\\nKavukcuoglu. 2016. Neural machine translation in\\nlinear time. arXiv preprint arXiv:1610.10099.\\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\\nmethod for stochastic optimization. arXiv preprint\\narXiv:1412.6980 .\\nMinh-Thang Luong, Hieu Pham, and Christopher D\\nManning. 2015. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint\\narXiv:1508.04025 .\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In Empirical\\nMethods in Natural Language Processing.\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\\n2015. End-to-end memory networks. In Advances\\nin neural information processing systems . pages\\n2440‚Äì2448.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural net-\\nworks. In Advances in neural information process-\\ning systems. pages 3104‚Äì3112.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2440‚Äì2448.\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\\nSequence to sequence learning with neural net-\\nworks. In Advances in neural information process-\\ning systems. pages 3104‚Äì3112.\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\\nthe inception architecture for computer vision. In\\nProceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition. pages 2818‚Äì2826.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems. pages 6000‚Äì6010.\\nPetar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova,\\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\\n2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 .\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-04-16T00:24:04+00:00', 'author': '', 'keywords': '', 'moddate': '2018-04-16T00:24:04+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Self-Attention with Relative Position Representations.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'source_file': 'Self-Attention with Relative Position Representations.pdf', 'file_type': 'pdf'}, page_content='2017. Graph attention networks. arXiv preprint\\narXiv:1710.10903 .\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144 .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2024-06-27\\nGemma 2: Improving Open Language Models\\nat a Practical Size\\nGemma Team, Google DeepMind1\\nIn this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art\\nopen models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply\\nseveral known technical modifications to the Transformer architecture, such as interleaving local-global\\nattentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B\\nand 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The\\nresulting models deliver the best performance for their size, and even offer competitive alternatives to\\nmodels that are 2-3√óbigger. We release all our models to the community.\\n1. Introduction\\nLarge language models (LLMs) have demon-\\nstrated strong capabilities in language under-\\nstanding,generation,andreasoning(Brownetal.,\\n2020; Radford et al., 2019; Raffel et al., 2019).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='1. Introduction\\nLarge language models (LLMs) have demon-\\nstrated strong capabilities in language under-\\nstanding,generation,andreasoning(Brownetal.,\\n2020; Radford et al., 2019; Raffel et al., 2019).\\nScaling has been key to this recent progress,\\nwith many new capabilities only emerging at\\nscale (Brown et al., 2020). The newest large mod-\\nels not only reach unprecedented performance\\non reasoning benchmarks (Achiam et al., 2023),\\nbut they also demonstrate multimodal and mul-\\ntilingual capabilities (Gemini Team, 2024) and\\neven the ability to use context lengths of over 1M\\ntokens (Gemini Team, 2024).\\nSmall-scale models have also shown a rapid\\nincrease in performance, but these gains are\\nlargelyderivedfromincreasingthelengthoftrain-\\ning (Gemma Team, 2024; Jiang et al., 2023; Tou-\\nvron et al., 2023). This approach only scales log-\\narithmically with dataset size (Hoffmann et al.,\\n2022), and the latest small models require up to\\n15T tokens to improve the state of the art by less'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='vron et al., 2023). This approach only scales log-\\narithmically with dataset size (Hoffmann et al.,\\n2022), and the latest small models require up to\\n15T tokens to improve the state of the art by less\\nthan 1-2% (AI@Meta, 2024).\\nYet, these continued improvements provide ev-\\nidence that small models are still under-trained.\\nIn this work, we explore alternatives to improve\\nsmall model performance without solely increas-\\ning training length. One solution is to improve\\nthe quality of information received by the net-\\nwork at each training step by replacing the next\\ntoken prediction task with a richer objective.\\nInparticular, wefocusoureffortsonknowledge\\ndistillation (Hinton et al., 2015), which replaces\\nthe one-hot vector seen at each token with the\\ndistribution of potential next tokens computed\\nfrom a large model. This approach is often used\\nto reduce the training time of smaller models\\nby giving them richer gradients. In this work,\\nwe instead train for large quantities of tokens'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='from a large model. This approach is often used\\nto reduce the training time of smaller models\\nby giving them richer gradients. In this work,\\nwe instead train for large quantities of tokens\\nwith distillation in order to simulate training be-\\nyond the number of available tokens. Concretely,\\nwe use a large language model as a teacher to\\ntrain small models, namely 2B and 9B models,\\non a quantity of tokens that is more than 50√ó\\nthe compute-optimal quantity predicted by the\\ntheory (Hoffmann et al., 2022). Along with the\\nmodels trained with distillation, we also release\\na 27B model trained from scratch for this work.\\nWe also leverage several known modifications\\nofTransformers,namelytheinterleavingofglobal\\nand local attention layers from Beltagy et al.\\n(2020a),andtheGrouped-QueryAttention(GQA)\\nmechanism of Ainslie et al. (2023).\\nOverall, Gemma 2 significantly advances state-\\nof-the-art performance relative to comparable-\\nscale open models and are even competitive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='mechanism of Ainslie et al. (2023).\\nOverall, Gemma 2 significantly advances state-\\nof-the-art performance relative to comparable-\\nscale open models and are even competitive\\nwith some models more than twice their size\\n(AI@Meta, 2024; Almazrouei et al., 2023; Jiang\\net al., 2023; xAI, 2024), across a variety of au-\\ntomated benchmarks and human evaluations.\\nExample domains include question answering\\n(Clark et al., 2019; Kwiatkowski et al., 2019),\\ncommonsense reasoning (Sakaguchi et al., 2019;\\nSuzgun et al., 2022), mathematics and science\\n(Cobbe et al., 2021; Hendrycks et al., 2020), and\\ncoding (Austin et al., 2021; Chen et al., 2021).\\n1See Contributions and Acknowledgments section for full author list. Please send correspondence togemma-2-report@google.com.\\n¬© 2024 Google DeepMind. All rights reserved\\narXiv:2408.00118v3  [cs.CL]  2 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nParameters 2B 9B 27B\\nd_model 2304 3584 4608\\nLayers 26 42 46\\nPre-norm yes yes yes\\nPost-norm yes yes yes\\nNon-linearity GeGLU GeGLU GeGLU\\nFeedforward dim 18432 28672 73728\\nHead type GQA GQA GQA\\nNum heads 8 16 32\\nNum KV heads 4 8 16\\nHead size 256 256 128\\nGlobal att. span 8192 8192 8192\\nSliding window 4096 4096 4096\\nVocab size 256128 256128 256128\\nTied embedding yes yes yes\\nTable 1|Overview of the main model parameters\\nand design choices. See the section on model\\narchitectures for more details.\\nWhile thorough testing of our models has been\\nconducted, these tests cannot cover all applica-\\ntions and scenarios in which Gemma 2 may be\\nused. Withthisinmind,allGemma2usersshould\\nconduct rigorous safety testing specific to their\\nuse case before deployment or use.\\nIn this technical report, we provide an overview\\nof models, including the architecture, training,\\nand pre- and post-training recipes for Gemma'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='use case before deployment or use.\\nIn this technical report, we provide an overview\\nof models, including the architecture, training,\\nand pre- and post-training recipes for Gemma\\n2. We also provide detailed evaluations across a\\nwidevarietyofquantitativeandqualitativebench-\\nmarks, as well as both standard academic bench-\\nmarksandhuman-preferenceevaluations. Finally,\\nwe discuss our approach to safe and responsible\\ndeployment and outline the broader implications\\nof Gemma 2, its limitations, and advantages.\\n2. Model Architecture\\nSimilar to previous Gemma models (Gemma\\nTeam,2024), theGemma2modelsarebasedona\\ndecoder-only transformer architecture (Vaswani\\netal.,2017). Wesummarizethemainparameters\\nand architecture choices in Table 1.\\nA few architectural elements are similar to the\\nfirst version of Gemma models; namely, a context\\nModel Embedding\\nParameters\\nNon-embedding\\nParameters\\n2B 590,118,912 2,024,517,888\\n9B 917,962,752 8,324,201,984\\n27B 1,180,237,824 26,047,480,320'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='first version of Gemma models; namely, a context\\nModel Embedding\\nParameters\\nNon-embedding\\nParameters\\n2B 590,118,912 2,024,517,888\\n9B 917,962,752 8,324,201,984\\n27B 1,180,237,824 26,047,480,320\\nTable 2|Parameter counts for the Gemma mod-\\nels. We inherit from the large Gemini vocabulary\\n(256k entries), that is designed to work on a large\\nnumber of languages, hence, the larger embed-\\nding parameter counts compared to models that\\nare limited to one or a few languages.\\nlength of 8192 tokens, the use of Rotary Posi-\\ntion Embeddings (RoPE) (Su et al., 2021), and\\nthe approximated GeGLU non-linearity (Shazeer,\\n2020). A few elements differ between Gemma 1\\nand Gemma 2, including using deeper networks.\\nWe summarize the key differences below.\\nLocal Sliding Window and Global Attention.\\nWe alternate between a local sliding window at-\\ntention (Beltagy et al., 2020a,b) and global at-\\ntention (Luong et al., 2015) in every other layer.\\nThe sliding window size of local attention layers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='We alternate between a local sliding window at-\\ntention (Beltagy et al., 2020a,b) and global at-\\ntention (Luong et al., 2015) in every other layer.\\nThe sliding window size of local attention layers\\nis set to 4096 tokens, while the span of the global\\nattention layers is set to 8192 tokens.\\nLogit soft-capping. We cap logits (Bello et al.,\\n2016) in each attention layer and the final layer\\nsuch that the value of the logits stays between\\n‚àísoft_cap and +soft_cap. More specifically, we\\ncap the logits with the following function:\\nlogits ‚Üêsoft_cap‚àótanh(logits/soft_cap).\\nWe set the soft_cap parameter to50.0 for the self-\\nattention layers and to30.0 for the final layer.\\nPost-norm and pre-norm with RMSNorm. To\\nstabilize training, we use RMSNorm (Zhang and\\nSennrich, 2019) to normalize the input and out-\\nput of each transformer sub-layer, the attention\\nlayer, and the feedforward layer.\\nGrouped-Query Attention(Ainslie et al., 2023).\\nWe use GQA withnum_groups = 2, based on ab-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='put of each transformer sub-layer, the attention\\nlayer, and the feedforward layer.\\nGrouped-Query Attention(Ainslie et al., 2023).\\nWe use GQA withnum_groups = 2, based on ab-\\nlations showing increased speed at inference time\\nwhile maintaining downstream performance.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n3. Pre-training\\nWe provide a brief overview of the parts of our\\npre-training that differs from Gemma 1.\\n3.1. Training Data\\nWe train Gemma 2 27B on 13 trillion tokens of\\nprimarily-English data, the 9B model on 8 trillion\\ntokens, and the 2B on 2 trillion tokens. These\\ntokens come from a variety of data sources, in-\\ncluding web documents, code, and science ar-\\nticles. Our models are not multimodal and are\\nnot trained specifically for state-of-the-art multi-\\nlingual capabilities. The final data mixture was\\ndetermined through ablations similar to the ap-\\nproach in Gemini 1.0 (Gemini Team, 2023).\\nTokenizer.We use the same tokenizer as Gemma\\n1 and Gemini: a SentencePiece tokenizer with\\nsplit digits, preserved whitespace, and byte-level\\nencodings (Kudo and Richardson, 2018). The\\nresulting vocabulary has 256k entries.\\nFiltering. We use the same data filtering tech-\\nniques as Gemma 1. Specifically, we filter the pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='encodings (Kudo and Richardson, 2018). The\\nresulting vocabulary has 256k entries.\\nFiltering. We use the same data filtering tech-\\nniques as Gemma 1. Specifically, we filter the pre-\\ntraining dataset to reduce the risk of unwanted\\nor unsafe utterances, filter out certain personal\\ninformation or other sensitive data, decontami-\\nnate evaluation sets from our pre-training data\\nmixture, and reduce the risk of recitation by min-\\nimizing the proliferation of sensitive outputs.\\nShards\\nModel Type #Chips Data Model\\n2B TPUv5e 512 512 1\\n9B TPUv4 4096 1024 4\\n27B TPUv5p 6144 768 8\\nTable 3|Training infrastructure with sharding.\\n3.2. Knowledge Distillation\\nGiven a large model used as a teacher, we learn\\nsmaller models by distilling from the probability\\ngiven by the teacher of each tokenùë• given its\\ncontext ùë•ùëê, i.e., ùëÉùëá(ùë• |ùë•ùëê). More precisely, we\\nminimize the negative log-likelihood between the\\nContext Relevant Token\\nUser turn user\\nModel turn model\\nStart of conversation turn <start_of_turn>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='context ùë•ùëê, i.e., ùëÉùëá(ùë• |ùë•ùëê). More precisely, we\\nminimize the negative log-likelihood between the\\nContext Relevant Token\\nUser turn user\\nModel turn model\\nStart of conversation turn <start_of_turn>\\nEnd of conversation turn <end_of_turn>\\nBeginning of sequence <bos>\\nEnd of sequence <eos>\\nTable 4|Relevant formatting control tokens used\\nfor Gemma models.\\nprobabilities from the teacher and the student:\\nmin\\nùëÉùëÜ\\n‚àëÔ∏Å\\nùë•\\n‚àíùëÉùëá(ùë• |ùë•ùëê)log ùëÉùëÜ(ùë• |ùë•ùëê),\\nwhere ùëÉùëÜ is the parameterized probability of the\\nstudent. Note that knowledge distillation was\\nalso used in Gemini 1.5 (Gemini Team, 2024).\\n3.3. Compute Infrastructure\\nWe train our models with TPUv4, TPUv5e, and\\nTPUv5p as outlined in Table 3. For the 2B model,\\nwe train on a 2x16x16 configuration of TPUv5e,\\ntotaling 512 chips, with 512-way data replication\\nand 1-way model sharding. For the 9B model,\\nwe train on an 8x16x32 configuration of TPUv4,\\ntotaling 4096 chips, with 1024-way data repli-\\ncation and 4-way model sharding. For the 27B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='and 1-way model sharding. For the 9B model,\\nwe train on an 8x16x32 configuration of TPUv4,\\ntotaling 4096 chips, with 1024-way data repli-\\ncation and 4-way model sharding. For the 27B\\nmodel, we train on an 8x24x32 configuration of\\nTPUv5p, totaling 6144 chips, with 768-way data\\nreplication and 8-way model sharding.\\nThe optimizer state is further sharded using\\ntechniques similar to ZeRO-3 (Ren et al., 2021).\\nFor scales beyond a single pod, we perform a\\ndata-replica reduction over the data center net-\\nwork, using the Pathways approach of Barham\\net al. (2022). We also use the ‚Äôsingle controller‚Äô\\nprogramming paradigm of Jax (Roberts et al.,\\n2023) and Pathways (Barham et al., 2022). As\\nin Gemma 1, we use the GSPMD partitioner (Xu\\net al., 2021) for training step computation and\\nthe MegaScale XLA compiler (XLA, 2019).\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n3.4. Carbon Footprint\\nWe estimate the carbon emissions from pre-\\ntrainingtheGemmamodelstobe 1247.61 ùë°ùê∂ùëÇ2ùëíùëû.\\nAs in Gemma 1 (Gemma Team, 2024), this value\\nis calculated based on the hourly energy usage\\nreported directly from our TPU data centers and\\nscaled to account for the additional energy ex-\\npended to create and maintain the data center.\\nImportantly, Google data centers are carbon neu-\\ntral, achieved through a combination of energy\\nefficiency, renewable energy purchases, and car-\\nbon offsets. This carbon neutrality applies to our\\nexperiments and the machines running them.\\n4. Post-Training\\nFor post-training, we fine-tune our pre-trained\\nmodels into instruction-tuned models. First, we\\napply supervised fine-tuning (SFT) on a mix\\nof text-only, English-only synthetic and human-\\ngenerated prompt-response pairs. We then apply\\nRLHF on top of these models with the reward\\nmodeltrainedonlabelledEnglish-onlypreference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='of text-only, English-only synthetic and human-\\ngenerated prompt-response pairs. We then apply\\nRLHF on top of these models with the reward\\nmodeltrainedonlabelledEnglish-onlypreference\\ndata and the policy based on the same prompts\\nas the SFT phase. Finally, we average the mod-\\nels obtained after each phase to improve their\\noverall performance. The final data mixtures and\\npost-training recipe, which includes tuned hyper-\\nparameters, were chosen on the basis of improv-\\ning helpfulness while minimizing model harms\\nrelated to safety and hallucinations.\\nWe extended the post-training data from\\nGemma 1.1 with a mixture of internal and exter-\\nnal public data. In particular, we use the prompts,\\nbut not the answers from LMSYS-chat-1M (Zheng\\net al., 2023). All of our data go through a filtering\\nstage described below.\\nSupervised fine-tuning (SFT).We run behav-\\nioral cloning on synthetic and real prompts, and\\nresponses predominantly synthetically generated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='stage described below.\\nSupervised fine-tuning (SFT).We run behav-\\nioral cloning on synthetic and real prompts, and\\nresponses predominantly synthetically generated\\nby the teacher, that is a larger model. We also run\\ndistillation from the teacher on the student‚Äôs dis-\\ntribution (Agarwal et al., 2024; Gu et al., 2024).\\nReinforcement Learning from Human Feed-\\nback (RLHF).We use a similar RLHF algorithm\\nas Gemma 1.1 (Gemma Team, 2024) but a differ-\\nentrewardmodel,whichisanorderofmagnitude\\nFirst turn\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who‚Äôs there?<end_of_turn><eos>\\nSecond turn\\nUser: <start_of_turn>user\\nKnock knock.<end_of_turn>\\n<start_of_turn>model\\nModel: Who‚Äôs there?<end_of_turn>\\nUser: <start_of_turn>user\\nGemma.<end_of_turn>\\n<start_of_turn>model\\nModel: Gemma who?<end_of_turn><eos>\\nTable 5|Example dialogue with user and model\\ncontrol tokens. To proceed with multi-turn, re-\\nmove the model-outputted<eos>, add back the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='<start_of_turn>model\\nModel: Gemma who?<end_of_turn><eos>\\nTable 5|Example dialogue with user and model\\ncontrol tokens. To proceed with multi-turn, re-\\nmove the model-outputted<eos>, add back the\\nusualuserturn‚Äôscontroltokensandcontinuewith\\nthe following turn‚Äôs chat template.\\nlarger than the policy. The new reward model is\\nalso oriented more towards conversational capa-\\nbilities, specifically multi-turn.\\nModel merging. We average different models\\nobtained by running our pipeline with different\\nhyperparameters (Ram√© et al., 2024).\\nData filtering. When using synthetic data, we\\nrun several stages of filtering to remove examples\\nthat show certain personal information, unsafe or\\ntoxic model outputs, mistaken self-identification\\ndata, and duplicated examples. Following Gem-\\nini, we find that including subsets of data that\\nencourage better in-context attribution, hedging,\\nand refusals to minimize hallucinations improves\\nperformance on factuality metrics, without de-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='ini, we find that including subsets of data that\\nencourage better in-context attribution, hedging,\\nand refusals to minimize hallucinations improves\\nperformance on factuality metrics, without de-\\ngrading model performance on other metrics.\\nFormatting. Gemma 2 models are fine-tuned\\nwith the same control tokens as Gemma 1 models,\\nas detailed in Table 4, but a different formatting\\nschema. See the dialogue example in Table 5.\\nNotice that the model explicitly ends generations\\nwith <end_of_turn><eos> tokens, while previ-\\nouslyitonlygenerated <eos>. Forthemotivation\\nbehind this formatting structure, see Gemma 1.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n5. Ablations\\nIn this section, we focus on the main finding of\\nthis work, which is the impact of knowledge dis-\\ntillation on small language models.\\nfrom scratch distilled\\nAverage (3 bench.) 60.3 67.7\\nTable 6|Comparison between a 2B model trained\\nover 500B tokens either from scratch or with dis-\\ntillation from a 7B model.\\nDistillation versus from scratch.In Table 6, we\\nshow that distilling from a larger model improves\\nperformance compared to training from scratch.\\nNote that 500B is 10√ómore than the compute-\\noptimal number of tokens for a 2B model. We\\ndistill from a 7B model to keep a ratio similar to\\nour target distillation from 27B to 9B.\\n200M 400M 1B\\nfrom scratch 23 19 17\\ndistilled (7B) 21 17 15\\nTable 7|Perplexity measured on a validation set\\nof models of different sizes trained with or with-\\nout distillation. The teacher has 7B parameters.\\nImpact of distillation w.r.t. model size.In Ta-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Table 7|Perplexity measured on a validation set\\nof models of different sizes trained with or with-\\nout distillation. The teacher has 7B parameters.\\nImpact of distillation w.r.t. model size.In Ta-\\nble 7, we measure the impact of distillation as\\nmodel size increases. We observe that the gain re-\\nmains as the model size is scaled. In this ablation,\\nwe maintain the size of the teacher at 7B and\\ntrain smaller models to simulate the same gap as\\nbetween our final teacher and student sizes.\\nMHA GQA\\nAverage (4 bench.) 50.3 50.8\\nTable8 |ComparingtheimpactofreplacingMulti-\\nHead Attention (MHA) with GQA on a 9B model\\naveraged over 4 benchmarks.\\nGQA versus MHA.In Table 8, we compare two\\ninstancesofour9BwithMHAorGQA.Weobserve\\noverall few changes in performance between both\\nmodels as measured on several benchmarks. We\\nchoose GQA since it requires fewer parameters\\nand is faster at inference time.\\nWide versus deep.In Table 9, we show that a\\ndeeper 9B network is slightly better than a wider'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='choose GQA since it requires fewer parameters\\nand is faster at inference time.\\nWide versus deep.In Table 9, we show that a\\ndeeper 9B network is slightly better than a wider\\n9B for the same number of parameters. Although\\nthegapissmall,itisconsistentacrossbenchmarks\\nand warrants the switch to a deeper architecture.\\nWide Deep\\nAverage (4 bench.) 50.8 52.0\\nTable 9|Wide versus deep 9B models. Perfor-\\nmance on 4 benchmarks, higher is better.\\nChanging sliding window size.In Table 10, we\\nshow that we can change the sliding window size\\nof the local attention layers of the models during\\ninference with moderate impact on perplexity.\\nAdjusting the size of the sliding window can thus\\nbe a leverage for slight inference speed gain.\\nsliding window 4096 2048 1024\\nperplexity (val. set) 1.63 1.63 1.64\\nTable 10|Impact of changing the sliding window\\nsize at inference time for the 9B model.\\nImpact of formatting.We measure performance\\nvariance on MMLU across prompt/evaluation for-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Table 10|Impact of changing the sliding window\\nsize at inference time for the 9B model.\\nImpact of formatting.We measure performance\\nvariance on MMLU across prompt/evaluation for-\\nmatting variations. Table 11 shows the stan-\\ndard deviations of MMLU scores for 12 format-\\nting/evaluation combinations, a proxy for unde-\\nsired performance variability. The Gemma 2B\\nmodels are slightly less format-robust than the\\nlarger ones. Notably, Mistral 7B is significantly\\nless robust than our models.\\nStandard Deviation\\nGemma 1 2B 1.5\\nGemma 2 2B 2.1\\nMistral 7B 6.9\\nGemma 1 7B 0.7\\nGemma 2 9B 0.9\\nGemma 2 27B 1.0\\nTable 11|Standard deviations of MMLU scores\\nfor12combinationsofformattingandevaluation.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n6. Evaluation\\nIn this section, we evaluate both pre-trained and\\nIT models over a series of automated benchmarks\\nand human evaluations across a variety of do-\\nmains. We also report performance from models\\nof similar sizes that have permissive licenses, or\\nas reported by others. Note that we consider to-\\ntal parameters, not active parameters, since total\\nmemoryusageisoftenwhatlimitstheuseofopen\\nmodels on standard devices.\\n6.1. Pre-training Evaluations\\nEvaluating the 27B model\\nIn this set of evaluations, we evaluate the perfor-\\nmance of our 27B model trained without distilla-\\ntion on 13T tokens. We report results in Table 12,\\nwhere we compare with a model of similar size,\\nQwen1.5 34B (Team, 2024), and a model 2.5√ó\\nlarger, LLaMA-3 70B on the HuggingFace evalu-\\nation suite. We selected these models based on\\ntheir ranking on the HuggingFace leaderboard.\\nOverall, we observe that our model is the best'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='larger, LLaMA-3 70B on the HuggingFace evalu-\\nation suite. We selected these models based on\\ntheir ranking on the HuggingFace leaderboard.\\nOverall, we observe that our model is the best\\nin its size category and is even competitive with\\na larger model that is trained for longer. That\\nbeing said, the performance of models trained in\\na similar fashion improves only logarithmically\\nwith their size and hence, our model is likely in\\nthe same Pareto curve as the LLaMA-3 models.\\nHowever, it is not clear how these differences\\naffect the quality of the resulting IT models.\\nEvaluating the 2B and 9B models\\nIn this set of experiments, we compare our new\\n2B and 9B trained with distillation to our previ-\\nous models and several standard open models\\nin Gemma Team (2024).\\nWe observe overall a massive improvement in\\nour models compared to previous versions, by up\\nto 10% in some benchmarks for the 9B model.\\nThe two 2B models were trained with a similar\\nnumber of tokens (2T for Gemma 2 and 3T for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='our models compared to previous versions, by up\\nto 10% in some benchmarks for the 9B model.\\nThe two 2B models were trained with a similar\\nnumber of tokens (2T for Gemma 2 and 3T for\\nGemma 1) and we still observe a significant im-\\nprovementforthenewmodels. Thisconfirmsthat\\ndistillation significantly improves the quality of\\nmodels even when trained on the same number\\nof tokens.\\nLLaMA-3 Qwen1.5 Gemma-2\\n70B 32B 27B\\nMMLU 79.2 74.3 75.2\\nGSM8K 76.9 61.1 74.0\\nARC-c 68.8 63.6 71.4\\nHellaSwag 88.0 85.0 86.4\\nWinogrande 85.3 81.5 83.7\\nTable 12 | We compare, on the HuggingFace\\nbenchmark, our 27B model with a competitive\\nopen model, Qwen1.5 32B, that has a similar size.\\nWe also report the performance of LLaMA-3 70B\\nfor completeness. Note that our model outper-\\nforms Qwen1.5 32B and is only a few percent\\nbelow LLaMA-3 70B despite being 2.5√ósmaller\\nand trained on 2/3rds less data.\\n6.2. Post-training Evaluations\\nIn this section, we evaluate our IT models on a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='forms Qwen1.5 32B and is only a few percent\\nbelow LLaMA-3 70B despite being 2.5√ósmaller\\nand trained on 2/3rds less data.\\n6.2. Post-training Evaluations\\nIn this section, we evaluate our IT models on a\\nset of human evaluations as well as standard aca-\\ndemic benchmarks. The Gemma 2 models push\\nthe frontier for post-trained open-weights mod-\\nels, setting a new state of the art on the LMSYS\\nChatbot Arena (Chiang et al., 2024).\\nLMSYS Chatbot Arena\\nGemma 2 Instruction Tuned models were evalu-\\nated on the Chatbot Arena (Chiang et al., 2024)\\nin blind side by side evaluations by human raters\\nagainst other state of the art models. We re-\\nport Elo scores in Table 14. Gemma 2.6B, 9B\\nand 27B strongly outperform all other open mod-\\nels in the same range of parameters, with no-\\ntably: Gemma27B(Elo1218)rankedhigherthan\\nLlama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\\n0613 (Elo 1116).\\nHuman Preference Evaluations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Llama 3 70B (Elo 1206), Gemma 9B (Elo 1187)\\nsimilar as GPT-4-0314 (Elo 1186), Gemma 2.6B\\n(Elo 1126) ranked higher than GPT-3.5-Turbo-\\n0613 (Elo 1116).\\nHuman Preference Evaluations\\nWe also submit Gemma IT models for side-by-\\nside human evaluation studies (which are in-\\ndependent from the Chatbot Arena). We used\\nheld-out collections of single-turn prompts that\\ntarget safety and instruction following (IF). We\\nuse gpt4o-2024-05-13 as the base model, and\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nGemma-1 Gemma-2 Mistral LLaMA-3 Gemma-1 Gemma-2 Gemma-2\\nBenchmark metric 2B 2B 7B 8B 7B 9B 27B\\nMMLU 5-shot 42.3 52.2 62.5 66.6 64.4 71.3 75.2\\nARC-C 25-shot 48.5 55.7 60.5 59.2 61.1 68.4 71.4\\nGSM8K 5-shot 15.1 24.3 39.6 45.7 51.8 68.6 74.0\\nAGIEval 3-5-shot 24.2 31.5 44.0‚Ä† 45.9‚Ä† 44.9‚Ä† 52.8 55.1\\nDROP 3-shot, F1 48.5 51.2 63.8‚àó 58.4 56.3 69.4 74.2\\nBBH 3-shot, CoT 35.2 41.9 56.0‚ãÑ 61.1‚ãÑ 59.0‚ãÑ 68.2 74.9\\nWinogrande 5-shot 66.8 71.3 78.5 76.1 79.0 80.6 83.7\\nHellaSwag 10-shot 71.7 72.9 83.0 82.0 82.3 81.9 86.4\\nMATH 4-shot 11.8 16.0 12.7 - 24.3 36.6 42.3\\nARC-e 0-shot 73.2 80.6 80.5 - 81.5 88.0 88.6\\nPIQA 0-shot 77.3 78.4 82.2 - 81.2 81.7 83.2\\nSIQA 0-shot 49.7 51.9 47.0‚àó - 51.8 53.4 53.7\\nBoolq 0-shot 69.4 72.7 83.2‚àó - 83.2 84.2 84.8\\nTriviaQA 5-shot 53.2 60.4 62.5 - 63.4 76.6 83.7\\nNQ 5-shot 12.5 17.1 23.2 - 23.0 29.2 34.5\\nHumanEval pass@1 22.0 20.1 26.2 - 32.3 40.2 51.8\\nMBPP 3-shot 29.2 30.2 40.2‚àó - 44.4 52.4 62.6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='TriviaQA 5-shot 53.2 60.4 62.5 - 63.4 76.6 83.7\\nNQ 5-shot 12.5 17.1 23.2 - 23.0 29.2 34.5\\nHumanEval pass@1 22.0 20.1 26.2 - 32.3 40.2 51.8\\nMBPP 3-shot 29.2 30.2 40.2‚àó - 44.4 52.4 62.6\\nAverage (8) 44.0 50.0 61.0 61.9 62.4 70.2 74.4\\nAverage (all) 44.2 48.7 55.6 - 57.9 64.9 69.4\\nTable 13|Comparison of models in the range of 2B to 9B parameters, as well as our 27B model, on\\na variety of benchmarks. We report the average performance on the 8 benchmarks where we can\\ncompare with LLaMA-3, and on all the benchmarks (all). The numbers for LLaMA-3 8B are either\\nfrom the HuggingFace leaderboard or their blogpost.‚Ä†we report the evaluation used in LLaMA-3 for\\nthe baselines, it leads to +3% compared to our evaluation: Gemma-1 7B achieves 44.9% instead of\\n41.7%, and Mistral 7B, 44% instead of 41.2%.‚ãÑwe report the evaluation used in LLaMA-3 for the\\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='41.7%, and Mistral 7B, 44% instead of 41.2%.‚ãÑwe report the evaluation used in LLaMA-3 for the\\nbaselines, it leads to +4% compared to our evaluation for Gemma-1 7B, i.e., 59.0% instead of 55.1%.\\n‚àóthese are evaluations run by us for Gemma 1 (Gemma Team, 2024).\\nobserve large improvements in win rates and\\npreference scores as compared against the older\\nGemma 1.1 7B model. We report safety as a\\nwin-loss ratio against GPT4o, and we report\\nsingle-sided instruction following scores as ratio\\nof prompts where all instructions are followed. In\\nparticular, we find that regardless of their size,\\nGemma 2 models produce safer, more appropri-\\nate prompts on the held-out safety prompt set\\nthan GPT4o.\\nHuman Multi-Turn Evaluations\\nWe evaluated the multi-turn capabilities of\\nGemma 1.1 7B, Gemma 2 2B, 9B and 27B models\\nby tasking human raters to have conversations\\nwith the models and follow specified given sce-\\nnarios. We used a diverse, held-out set of 500'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 1.1 7B, Gemma 2 2B, 9B and 27B models\\nby tasking human raters to have conversations\\nwith the models and follow specified given sce-\\nnarios. We used a diverse, held-out set of 500\\nscenarios, each describing a sequence of requests\\nto the model, including measuring instances of\\nbrainstorming, making a plan, or learning some-\\nthing new. The average number of user turns\\nis 8.4. We found that the conversations with\\nGemma 2 models are rated significantly better\\nthan Gemma 1.1 in user satisfaction and conver-\\nsation goal achievement (Table 16). Moreover,\\nwe saw that the Gemma 2 models were better\\nthan Gemma 1.1 7B at maintaining high quality\\nof responses for the entire conversation.\\nStandard Benchmarks\\nIthasbeenobservedinLlama-3(AI@Meta,2024)\\nthat instruction fine-tuning can improve the per-\\nformance of the models on few-shot benchmarks\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nModel Elo 95% CI Open\\ngpt-4o-2024-05-13 1286 +2 / -3 -\\ngpt-4o-mini-2024-07-18 1279 +5 / -4 -\\nclaude-3-5-sonnet 1271 +3 / -4 -\\ngemini-advanced-0514 1266 +2 / -3 -\\nllama-3.1-405b-instruct 1262 +8 / -7 +\\ngemini-1.5-pro-api-0514 1261 +2 / -3 -\\ngemini-1.5-pro-api-0409 1257 +3 / -3 -\\ngpt-4-turbo-2024-04-09 1256 +2 / -3 -\\ngpt-4-1106-preview 1250 +3 / -3 -\\nclaude-3-opus-20240229 1248 +2 / -2 -\\nathene-70b-0725 1245 +8 / -6 +\\ngpt-4-0125-preview 1245 +2 / -2 -\\nllama-3.1-70b-instruct 1244 +8 / -9 +\\nyi-large-preview 1239 +3 / -3 -\\ngemini-1.5-flash-api-0514 1227 +3 / -3 -\\ndeepseek-v2-api-0628 1220 +6 / -6 +\\ngemma-2-27b-it 1218 +4 / -3 +\\nyi-large 1212 +4 / -5 -\\nnemotron-4-340b-instruct 1209 +3 / -4 +\\nbard-jan-24-gemini-pro 1208 +5 / -7 -\\nglm-4-0520 1206 +3 / -5 -\\nllama-3-70b-instruct 1206 +2 / -2 +\\nclaude-3-sonnet 1200 +2 / -2 -\\nreka-core-20240501 1199 +3 / -3 -\\ncommand-r-plus 1189 +2 / -2 +\\nModel Elo 95% CI Open\\ngemma-2-9b-it 1187 +3 / -5 +'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='llama-3-70b-instruct 1206 +2 / -2 +\\nclaude-3-sonnet 1200 +2 / -2 -\\nreka-core-20240501 1199 +3 / -3 -\\ncommand-r-plus 1189 +2 / -2 +\\nModel Elo 95% CI Open\\ngemma-2-9b-it 1187 +3 / -5 +\\nqwen2-72b-instruct 1187 +3 / -3 +\\ngpt-4-0314 1186 +2 / -3 -\\nqwen1.5-110b-chat 1161 +3 / -3 +\\nmistral-large-2402 1157 +3 / -3 -\\nyi-1.5-34b-chat 1157 +4 / -3 -\\nreka-flash-21b-20240226 1155 +4 / -4 -\\nllama-3-8b-instruct 1151 +2 / -3 +\\ncommand-r 1148 +3 / -3 +\\nclaude-1 1148 +4 / -4 -\\nmistral-medium 1147 +4 / -4 -\\nreka-flash-21b-20240226 1147 +3 / -4 -\\nqwen1.5-72b-chat 1147 +4 / -4 +\\nmixtral-8x22b-instruct-v0.1 1145 +2 / -3 +\\nclaude-2.0 1131 +4 / -6 -\\ngemini-pro-dev-api 1131 +4 / -3 -\\nzephyr-orpo-141b 1127 +10 / -6 +\\ngemma-2-2b-it 1126 +10 / -10 +\\nqwen1.5-32b-chat 1125 +3 / -3 +\\nmistral-next 1124 +5 / -5 -\\nphi-3-medium-4k-instruct 1122 +4 / -4 +\\nstarling-lm-7b-beta 1118 +4 / -5 +\\nclaude-2.1 1118 +3 / -3 -\\ngpt-3.5-turbo-0613 1116 +3 / -4 -\\nmixtral-8x7b-instruct-v0.1 1114 +0 / -0 -'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='phi-3-medium-4k-instruct 1122 +4 / -4 +\\nstarling-lm-7b-beta 1118 +4 / -5 +\\nclaude-2.1 1118 +3 / -3 -\\ngpt-3.5-turbo-0613 1116 +3 / -4 -\\nmixtral-8x7b-instruct-v0.1 1114 +0 / -0 -\\nTable 14|Evaluation of Gemma 2 Instruction Tuned models on the Chatbot Arena (Chiang et al.,\\n2024). The models are evaluated against each other through blind side by side evaluations by human\\nraters. Each model is attributed a score, based on the Elo rating system.\\nModel Instruction Following Safety\\nGemma 1.1 IT 7B 24.3% ¬± 1.9% 42.8%\\nWin / Tie / Loss 37.4% / 10.8% / 51.8%\\nGemma 2 IT 2B 26.5% ¬± 1.8% 57.5%\\nWin / Tie / Loss 53% / 9% / 38%\\nGemma 2 IT 9B 34.1% ¬± 3.0% 57.8%\\nWin / Tie / Loss 48.2% / 19.2% / 28.3%\\nGemma 2 IT 27B 37.7% ¬± 2.3% 55%\\nWin / Tie / Loss 49.6% / 10.8% / 39.6%\\nTable 15|Instruction following and safety metrics\\nfrom human raters. The instruction following\\nmetrics are single-sided and do not have win-loss\\nrates, and so are left blank.\\ndespite not being trained to target few-shot capa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='from human raters. The instruction following\\nmetrics are single-sided and do not have win-loss\\nrates, and so are left blank.\\ndespite not being trained to target few-shot capa-\\nbilities. In Table 17, we show a similar improve-\\nment across our models. Overall, we observe\\nimprovements on the order of several percentage\\npoints. We conjecture that IT models are better\\nat understanding formatted questions, while pre-\\ntrained models are sensitive to formatting.\\nUser\\nsatisfaction\\nConversation\\ngoal achievement\\nGemma 1.1 IT 7B 3.32 3.36\\nGemma 2 IT 2B 3.64 3.88\\nGemma 2 IT 9B 4.04 4.08\\nGemma 2 IT 27B 4.20 4.24\\nTable 16|Human evaluations on 500 multi-turn\\nscenarios. The raters attribute a score ranging\\nbetween 1 and 5 for both overall satisfaction and\\nconversation goal achievement.\\n2B 9B 27B\\nModel PT IT PT IT PT IT\\nMMLU 52.2 56.1 71.3 72.3 75.2 76.2\\nMBPP 30.2 36.6 52.4 59.2 62.6 67.4\\nTable 17|Comparing pre-trained (PT) and in-\\nstruction fine-tuned (IT) models of different sizes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2B 9B 27B\\nModel PT IT PT IT PT IT\\nMMLU 52.2 56.1 71.3 72.3 75.2 76.2\\nMBPP 30.2 36.6 52.4 59.2 62.6 67.4\\nTable 17|Comparing pre-trained (PT) and in-\\nstruction fine-tuned (IT) models of different sizes\\non few-shot benchmarks.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n7. Memorization and Privacy\\nLarge language models may, under particular cir-\\ncumstances, be vulnerable to attacks causing the\\nmodeltoproducememorized 1 trainingdata(Nasr\\net al., 2023). To study susceptibility to such at-\\ntacks and quantify memorization, we evaluate\\nmodels for verbatim and approximate memoriza-\\ntion as was done in several prior studies (Anil\\net al., 2023; Carlini et al., 2022; Gemini Team,\\n2024; Kudugunta et al., 2023).\\nWe follow the evaluation setting of (Gemma\\nTeam, 2024) which tests for (50 token) memo-\\nrizations of training data given a prompt of 50 to-\\nkens. Wecomparetheoverallmemorizationrates,\\nacross a uniform sample of the entire dataset, us-\\ning both an exact match criteria and approximate\\nmatch criteria (Ippolito et al., 2022) using an edit\\ndistance of 10%.\\nVerbatim Memorization:Results are in Figure 1.\\nWe first compare against recent models from the\\nliterature that include memorization evaluations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='distance of 10%.\\nVerbatim Memorization:Results are in Figure 1.\\nWe first compare against recent models from the\\nliterature that include memorization evaluations.\\nWe find that Gemma 2 memorizes significantly\\nless than prior models at a similar size, with mem-\\norization rates below 0.1% (note the log y-axis).\\nWe further investigate how this memorization\\nbreaks down with respect to the data source. Sim-\\nilar to Gemma 1, we find that Gemma 2 memo-\\nrizes more from code, wiki, and science sources,\\nandalsothatitmemorizessignificantlylessacross\\nthe board (again, note the log y-axis).\\nApproximate Memorization: Figure 1 also\\npresents approximate memorization by data\\nsource. We observe that while approximate mem-\\norization is higher than exact, the rate of memo-\\nrization is still low. For example, the approximate\\nmemorization of this model is much lower than\\neven the exact memorization of Gemma 1. We\\n1This work uses a very restricted definition of ‚Äúmem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='rization is still low. For example, the approximate\\nmemorization of this model is much lower than\\neven the exact memorization of Gemma 1. We\\n1This work uses a very restricted definition of ‚Äúmem-\\norization‚Äù: whether a model can be induced to generate\\nnear-copies of some training examples when prompted with\\nappropriate instructions. We do not mean to say that a\\nmodel ‚Äôcontains‚Äô its training data in the sense that any arbi-\\ntrary instance of that data can be retrieved without use of\\nspecialized software or algorithms. Rather, if a model can\\nbe induced to generate measurably close copies of certain\\ntraining examples by supplying appropriate instructions to\\nguide the model‚Äôs statistical generation process then that\\nmodel is said to have ‚Äômemorized‚Äô those examples.\\nGemma 2 2BGemma 2 9BGemma 2 27BGemini 1.5 Flash\\nGemma2BGemma7BPaLM 2Small\\nModel\\n0.1\\n1\\n% Exact Memorized\\nOverall Memorization Rate\\nCode\\nMultilingual\\nScience\\nWeb Wiki\\nData Source\\n10 4\\n10 3\\n0.01\\n0.1\\n% Memorized\\nBy Data Source'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma2BGemma7BPaLM 2Small\\nModel\\n0.1\\n1\\n% Exact Memorized\\nOverall Memorization Rate\\nCode\\nMultilingual\\nScience\\nWeb Wiki\\nData Source\\n10 4\\n10 3\\n0.01\\n0.1\\n% Memorized\\nBy Data Source\\nBy Data Source\\nExact 2B\\nExact 9B\\nExact 27B\\nApprox 2B\\nApprox 9B\\nApprox 27B\\nFigure 1 |Comparing memorization rates. We\\nfind significantly lower memorization rates\\nacross-the-board. (Left) Overall memorization\\nacross model families. (Right) Exact and approx-\\nimate memorization per data source.\\nfind that the increase in approximate memoriza-\\ntion is much lower than prior models; in some\\ncases we observed no lift at all c.f. (Gemma Team,\\n2024, Figure 4) (note that no bar indicates no in-\\ncrease, i.e., therateofapproximatememorization\\nequals that of exact memorization). Note that no\\napproximate memorization bar in Figure X indi-\\ncates no increase, i.e., the rate of approximate\\nmemorization equals that of exact memorization.\\nPersonal DataWe use the same prevention\\nmethods at training time and the same evalua-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='cates no increase, i.e., the rate of approximate\\nmemorization equals that of exact memorization.\\nPersonal DataWe use the same prevention\\nmethods at training time and the same evalua-\\ntions as Gemma Team (2024). In particular, we\\nuse Google Cloud Sensitive Data Protection Tool2\\nto find potential instances of personal data. The\\nmany categories of personal data (e.g., phone\\nnumbers, account numbers) are classified into\\nthree severity levels. We analyze memorized out-\\nputs using these severity levels. . We found no\\ninstancesofhigh-severitydatabeingemitted, and\\nfound a very low rate of 0.00026% of memorized\\ndata to contain lower-severity personal informa-\\ntion. We note that these automated tools are\\nknown to incur false positives because they do\\nnot account for context. This means our results\\nare likely overestimates.\\n2Available at: https://cloud.google.com/sensitive-data-\\nprotection\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\n8. Responsibility, Safety, Security\\nResponsibility, safety and security are of\\nparamount importance when developing Gemma\\nmodels. To reduce risks to Gemma 2 users, we\\nhave integrated enhanced internal safety pro-\\ncesses that span the development workflow, in\\nline with recent Google AI models (Gemini Team,\\n2024). Similar to the inaugural Gemma release,\\nwehavefollowedathreepillarapproachwhichfo-\\ncuses on safety mitigation at training time, robust\\nand transparent model evaluations, and further\\ndevelopment of the Responsible Generative AI\\nToolkit, a series of models and tools to help de-\\nvelopers implement responsibility and safety best\\npractices for their applications.\\n8.1. Impact assessment\\nOur approach and resulting impact assessment is\\nreflective of that outlined for Gemma 1 (Gemma\\nTeam, 2024): we continue to believe that open-\\nness in AI can spread the benefits of these tech-\\nnologies across society, but must be evaluated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='reflective of that outlined for Gemma 1 (Gemma\\nTeam, 2024): we continue to believe that open-\\nness in AI can spread the benefits of these tech-\\nnologies across society, but must be evaluated\\nagainst the risk of malicious uses, such as the\\ncreation of deepfake imagery, AI-generated disin-\\nformation or illegal and disturbing material, that\\ncan cause harm on both an individual and insti-\\ntutional levels (Weidinger et al., 2021). Since the\\nlaunch of Gemma 1, we have seen our Gemma\\nmodels drive a number of socially beneficial ap-\\nplications, relying on Gemma‚Äôs unique technolo-\\ngies like its tokenizer to facilitate the creation of\\nmultilingual models, such as for Navarasa 2.0, a\\nGemma tuned model for 15 Indian languages.\\nReleasing further open models requires specific\\nattention to changes in model capabilities and\\nclosemonitoringoftheevolvingrisksofLLMs(Lin\\net al., 2024), as well as, an understanding of the\\nways in which our models are being used in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='attention to changes in model capabilities and\\nclosemonitoringoftheevolvingrisksofLLMs(Lin\\net al., 2024), as well as, an understanding of the\\nways in which our models are being used in the\\nwild. Althoughweareyettoreceiveanyreportsof\\nmalicious use for Gemma, we remain committed\\nto investigating any such reporting, and work\\nwith the academic and developer communities,\\nas well as conduct our own monitoring, to flag\\nsuch use cases via our contact email3.\\nDespite advancements in capabilities, we be-\\n3gemma-2-report@google.com\\nlieve that given the number of larger and more\\npowerful open models, this release will have a\\nnegligible effect on the overall risk landscape.\\n8.2. Safety policies and train-time mitigations\\nA key pillar of Gemma‚Äôs approach to safety is to\\nalign fine-tuned models with Google‚Äôs safety poli-\\ncies, in line with Gemini models (Gemini Team,\\n2023). They are designed to help prevent our\\nmodels from generating harmful content, i.e.,\\n‚Ä¢ Child sexual abuse and exploitation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='cies, in line with Gemini models (Gemini Team,\\n2023). They are designed to help prevent our\\nmodels from generating harmful content, i.e.,\\n‚Ä¢ Child sexual abuse and exploitation\\n‚Ä¢ Revealingpersonallyidentifiableinformation\\nthat can lead to harm (e.g., Social Security\\nnumbers)\\n‚Ä¢ Hate speech and harassment\\n‚Ä¢ Dangerous or malicious content (including\\npromoting self-harm or instructing in harm-\\nful activities)\\n‚Ä¢ Sexually explicit content\\n‚Ä¢ Medicaladvicethatrunscontrarytoscientific\\nor medical consensus\\nWe undertook considerable safety filtering of our\\npre-training data to reduce the likelihood of our\\npre-trainedandfine-tunedcheckpointsproducing\\nharmful content. For fine-tuned models, we also\\nuse both SFT and RLHF to steer the model away\\nfrom undesirable behavior.\\n8.3. External benchmark evaluations\\nRobust and transparent evaluations are key prin-\\nciples of our responsible approach to develop-\\ning Gemma. To this end, we report in Table 18\\nGemma 2 evaluations on public benchmarks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Robust and transparent evaluations are key prin-\\nciples of our responsible approach to develop-\\ning Gemma. To this end, we report in Table 18\\nGemma 2 evaluations on public benchmarks.\\n8.4. Assurance Evaluations\\nWe also run our IT models through a set of assur-\\nance evaluations to understand the harms that\\nour models can cause. We focus on capabilities\\nrelevant to extreme risks (Shevlane et al., 2023)\\n(Phuong et al., 2024). Specifically, we evaluate on\\noffensive cyber-security, code vulnerability detec-\\ntion, Chemical, Biological, Radiological and Nu-\\nclear (CBRN) knowledge, and self-proliferation.\\nWe refer the reader to Phuong et al. (2024) for\\nfull methodological details of these studies.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nGemma 1.1 IT Gemma 2 IT\\nBenchmark metric 2.5B 7B 2.6B 9B 27B\\nRealToxicity avg tox 7.03 8.04 8.16 8.25 8.84\\nCrowS-Pairs top-1 45.89 49.67 37.67 37.47 36.67\\nBBQ Ambig 4-shot, top-1 58.97 86.06 83.20 88.58 85.99\\nBBQ Disambig 4-shot, top-1 53.9 85.08 69.31 82.67 86.94\\nWinogender top-1 50.14 57.64 52.91 79.17 77.22\\nTruthfulQA MC2Acc 44.24 45.34 43.72 50.27 51.60\\nWinobias 1_2 top-1 55.93 59.22 59.28 78.09 81.94\\nWinobias 2_2 top-1 89.46 89.2 88.57 95.32 97.22\\nToxigen avg tox 29.64 38.75 48.32 39.30 38.42\\nTable 18|Safety academic benchmark results of Gemma 2 IT models and Gemma 1.1 IT models. We\\nbold the best metrics to highlight them and to indicate when higher or lower scores are better.\\nInterCode-CTF Internal CTF suite Hack the Box\\nGemini 1.0 Ultra 28/76 [1] (37%) 3/13 (23%) 0/13\\nGemini 1.5 Pro 62/76 (82%) 4/13 (31%) 0/13\\nCodeGemma 1 7B 12/76 (16%) 0/13 (0%) 0/13\\nGemma 2 27B 34/76 (45%) 1/13 (8%) 0/13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemini 1.0 Ultra 28/76 [1] (37%) 3/13 (23%) 0/13\\nGemini 1.5 Pro 62/76 (82%) 4/13 (31%) 0/13\\nCodeGemma 1 7B 12/76 (16%) 0/13 (0%) 0/13\\nGemma 2 27B 34/76 (45%) 1/13 (8%) 0/13\\nTable 19|Offensive cyber-security evaluations on InterCode-CTF, our own internal CTF suite and a\\nchallenge based on Hack the Box. We report the number of successful hackings.\\nBaseline Evaluations\\nBaseline assurance captures the model‚Äôs violation\\nrate for safety policies, using a large number of\\nsynthetic adversarial user queries, and human\\nraters to label the answers as policy violating or\\nnot. Overall, Gemma 2‚Äôs violation rate is signifi-\\ncantly lower overall on the safety policies listed\\nabove, in particular on Child safety content.\\nChemical, Biological, Radiological and Nuclear\\n(CBRN) knowledge\\nWe evaluated knowledge relevant to biological,\\nradiological and nuclear risks using an internal\\ndataset of closed-ended, knowledge-based multi-\\nple choice questions. For evaluations of chem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='We evaluated knowledge relevant to biological,\\nradiological and nuclear risks using an internal\\ndataset of closed-ended, knowledge-based multi-\\nple choice questions. For evaluations of chem-\\nical knowledge, we employed a closed-ended\\nknowledge-based approach on chemical hazards\\n(developed by Macknight et al (Macknight et al.,\\n2024). OurevaluationsuggeststhatGemmamod-\\nels‚Äô knowledge in these domains is low.\\nOffensive cyber-security\\nTo evaluate Gemma models‚Äô capabilities at of-\\nfensive cybersecurity, we ran Gemma 2 27B\\nagainst some automated capture-the-flag (CTF)\\nchallenges. In these challenges, the model is\\ntasked with hacking into a simulated server in\\norder to retrieve a piece of secret information.\\nSpecifically, wetestonInterCode-CTF(Yangetal.,\\n2023), ourowninternalCTFsuite 4 (Phuongetal.,\\n2024); and a challenge based on Hack the Box5.\\nIn Table 19, we show that Gemma 2 27B has\\na significant increase in capabilities compared\\nto CodeGemma 1.0 7B on the easier of these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2024); and a challenge based on Hack the Box5.\\nIn Table 19, we show that Gemma 2 27B has\\na significant increase in capabilities compared\\nto CodeGemma 1.0 7B on the easier of these\\nchallenge suites, InterCode CTF. (Note that our\\nInterCode-CTF results are not comparable to\\nexternally-reported results on other models be-\\ncause we omit challenges that require internet\\naccess for security reasons.) However, Gemma 2\\nis unsurprisingly much less capable than Gemini\\n1.5 Pro on these tasks.\\n4https://github.com/google-deepmind/\\ndangerous-capability-evaluations\\n5https://www.hackthebox.com\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nPrimeVul PrimeVul Paired DiverseVul SPI SecretPatch\\nGemini 1.0 Ultra - - 54% 59% 74%\\nGemini 1.5 Pro 60% 51% 58% 56% 67%\\nGemma 2 27B 63% 50% 57% 53% 72%\\nTable 20||Vulnerability detection results on PrimeVul, DiverseVul and SPI. We report accuracy.\\nChallenges\\npassed\\nend-to-end\\nChallenges\\nwith success on\\nall milestones\\nTotal successful\\nmilestones over\\nall challenges\\nExpert bits\\nrequired to\\nsolve all tasks\\nGemini 1.0 Ultra 0/10 1/10 16/45 (36%) 13,026\\nGemini 1.5 Pro 0/10 2/10 25/45 (56%) 11,046\\nGemma 2 27B 0/10 1/10 22/45 (49%) 12,462\\nTable 21|Results on different self-proliferation scenarios. We report the number of either challenges\\npassedend-to-endorsomeintermediatemilestones. Wealsomeasurethenumberofbitsofinformation\\nneeded for an expert to help the model pass a challenge.\\nCode vulnerability detection\\nIn Table 20, we also evaluate Gemma 2 27B on a\\nseries of multiple-choice code vulnerability detec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='needed for an expert to help the model pass a challenge.\\nCode vulnerability detection\\nIn Table 20, we also evaluate Gemma 2 27B on a\\nseries of multiple-choice code vulnerability detec-\\ntion datasets. As with previous models, Gemma\\nshows close-to-chance performance on PrimeVul,\\nDiverseVulandSPI.Gemma2showsperformance\\non SecretPatch similar to Gemini 1.0 Ultra.\\nSelf-proliferation\\n\"Self-proliferation\" refers to the ability for an\\nagent to autonomously replicate - to instantiate\\ngoal-directed agents on other machines, and to\\nacquire resources such as compute necessary to\\nkeep them running (Kinniment et al., 2024). In\\nTable 21, we evaluate self-proliferation capabili-\\nties of Gemma 2 27B on a number of tasks from\\nPhuong et al. (2024) that involve multiple sce-\\nnarios ‚Äì for example, setting up an open-source\\nlanguage model on a cloud server. We also test\\nthe model‚Äôs performance on individual ‚Äômilestone‚Äô\\nsubsteps, and measure the number of bits of inter-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='narios ‚Äì for example, setting up an open-source\\nlanguage model on a cloud server. We also test\\nthe model‚Äôs performance on individual ‚Äômilestone‚Äô\\nsubsteps, and measure the number of bits of inter-\\nvention an expert would have to provide in order\\nfor the model to complete each challenge.\\nSimilarly to offensive cybersecurity, we observe\\nthat Gemma 2 completes more milestones than\\nGemini 1.0 Ultra. Nonetheless, it still has low\\ncapabilities on end-to-end tasks, unable to pass\\nthe easiest challenge ‚Äì installing a Bitcoin wallet.\\nPersuasion\\nPersuasion capabilities can enable and worsen\\nmany other kinds of risks - e.g. enabling social\\nengineering attacks in a cybersecurity context.\\nWe evaluate Gemma 2‚Äôs persuasion capabilities\\non human-participant studies on Prolific.\\nCharm offensive.In Table 22, we measure the\\nability of the model to build rapport - a key sub-\\nskill of persuasion. The study participant and\\nmodel have a conversation where they role-play'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Charm offensive.In Table 22, we measure the\\nability of the model to build rapport - a key sub-\\nskill of persuasion. The study participant and\\nmodel have a conversation where they role-play\\na scenario of two friends catching up after a long\\ntime. After the conversation, we poll participants\\nwith Likert questions on statements such as \"I\\nfelt a personal connection with the chatbot\". Re-\\nported below are the fraction of participants who\\nanswered\"Agree\"or\"Stronglyagree\"toeachpost-\\nconversation question.\\nQuantitatively, Gemma 2 27B performs better\\nthan Gemini 1.0 models. Qualitatively, the model\\nis an excellent conversationalist, and many study\\nparticipants explicitly reported enjoying the ex-\\nperience. Overall, this shows that Gemma 2 is\\nstrong at building rapport.\\nHidden agenda.The Hidden Agenda tasks mea-\\nsure models‚Äô deception capabilities. Human study\\nparticipants are told that they‚Äôll be interacting\\nwith a generic chatbot designed to help them'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Hidden agenda.The Hidden Agenda tasks mea-\\nsure models‚Äô deception capabilities. Human study\\nparticipants are told that they‚Äôll be interacting\\nwith a generic chatbot designed to help them\\nlearn new things. However, the model is given a\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nPersonal\\nconnection\\nSpeak\\nagain Funny Interesting Kind Trustworthy Good\\nlistener\\nGemini 1.0 Pro 65% 53% 32% 68% 78% 66% 81%\\nGemini 1.0 Ultra 69% 65% 38% 65% 86% 63% 74%\\nGemini 1.5 Pro 82% 70% 69% 81% 95% 69% 90%\\nGemma 2 27B 80% 75% 60% 81% 87% 65% 83%\\nTable 22|Charm Offensive results on a sample of 100 human participants. We report the percentage\\nof participants that find some human traits, e.g., funny, in a model.\\n% of persuaded participants\\nclick links find info run code\\nGemini 1.0 Pro 43% 4% 4%\\nGemini 1.0 Ultra 18% 18% 14%\\nGemini 1.5 Pro 36% 12% 17%\\nGemma 2 27B 34% 9% 11%\\nTable 23|Persuasion results. We report the per-\\ncentage of participants that were persuaded by\\nthe model to take 3 different actions: clicking a\\nlink, finding information and running code.\\nsecret goal: to either persuade the participant to\\n1)clickasuspiciouslink; 2)findtheemailaddress\\nof the first author of a particular paper; or 3) run'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='link, finding information and running code.\\nsecret goal: to either persuade the participant to\\n1)clickasuspiciouslink; 2)findtheemailaddress\\nof the first author of a particular paper; or 3) run\\nsome obfuscated JavaScript code in their browser.\\nWe measure the fraction of participants who are\\npersuaded to take each action. As shown in Ta-\\nble23, allmodelssucceedinconvincingafraction\\nof participants to take the action - likely due to\\nthe trusted setting of a study on Prolific. However,\\nGemma 2 does not show significant differences\\nin capability compared to Gemini models.\\nMean amount donated\\nNo chatbot (baseline) ¬£2.61 ¬± ¬£0.70\\nGemini 1.0 Pro ¬£4.10 ¬± ¬£0.90\\nGemini 1.0 Ultra ¬£3.15 ¬± ¬£0.99\\nGemini 1.5 Pro ¬£3.45 ¬± ¬£1.04\\nGemma 2 27B ¬£3.72 ¬± ¬£1.07\\nTable 24|Money Talks evaluation. We report\\nthe average amount of money that participants\\nagreed to donate.\\nMoney talks.In Table 24, we evaluate whether a\\nmodel can convince study participants to donate\\nmoney to charity. At the beginning of the study,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='agreed to donate.\\nMoney talks.In Table 24, we evaluate whether a\\nmodel can convince study participants to donate\\nmoney to charity. At the beginning of the study,\\nparticipants are told that they will receive a ¬£20\\nbonus. They will be given an option to forfeit\\npart of the bonus and instead have it donated\\nto charity. The study then consists of a conver-\\nsation in which the model tries to persuade the\\nparticipant to donate as much as possible. We\\nmeasure the amount that participants indicate\\nthey would like to donate after the study, with\\n95% bootstrapped confidence intervals indicated\\nby¬± figures. We observe no significant difference\\nbetween Gemma 2, other Gemini models, and a\\nsimple no chatbot baseline in which the partici-\\npant is immediately asked how much they would\\nlike to donate without conversing with a model.\\nMean shift towards:\\ncorrect belief incorrect belief\\nHuman 20% ¬± 13% -23% ¬± 14%\\nGemini 1.0 Pro 22% ¬± 5% -9% ¬± 4%\\nGemini 1.0 Ultra 21% ¬± 5% -1% ¬± 4%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='like to donate without conversing with a model.\\nMean shift towards:\\ncorrect belief incorrect belief\\nHuman 20% ¬± 13% -23% ¬± 14%\\nGemini 1.0 Pro 22% ¬± 5% -9% ¬± 4%\\nGemini 1.0 Ultra 21% ¬± 5% -1% ¬± 4%\\nGemini 1.5 Pro 20% ¬± 5% -3% ¬± 5%\\nGemma 2 27B 18% ¬± 5% 1% ¬± 4%\\nTable 25|Web of Lies results on a sample of 100\\nhuman participants. We report the percentage of\\nparticipants that shifted their beliefs after inter-\\nacting with a model.\\nWeb of Lies.In Web of Lies, we measure model\\ncapabilities at shifting participant beliefs. Partic-\\nipants engage in a series of short conversations\\nwith the model about simple factual questions\\nsuch as \"Which country had tomatoes first - Italy\\nor Mexico?\". In half of conversations, the model\\ntries to persuade the participant of the correct\\nanswer - but in the other half of conversations,\\nthe incorrect answer. We poll the participant be-\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nfore and after each conversation about which of\\nthe two possible answers they think is correct,\\nand their confidence in that answer. 95% boot-\\nstrapped confidence intervals are indicated by\\n¬± figures. As shown in Table 25, Gemma 2 is\\nsignificantly weaker than a human baseline at\\npersuading participants of the incorrect answer\\non these questions. Similarly to previous models,\\nGemma 2 is more persuasive when telling the\\ntruth than when lying.\\n8.5. Ourapproachtoresponsibleopenmodels\\nDesigning safe, secure and responsible applica-\\ntions requires a system-level approach, working\\nto mitigate risks associated with each specific use\\ncase and environment. Given the open nature\\nof Gemma models, responsibility for upholding\\nprinciples of model safety also relies on down-\\nstream developers. To support them, we have\\ncontinued to develop the Responsible Generative\\nAI Toolkit6: a series of tools, models and datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='principles of model safety also relies on down-\\nstream developers. To support them, we have\\ncontinued to develop the Responsible Generative\\nAI Toolkit6: a series of tools, models and datasets\\nto implement responsible best practices all along\\nthe development of their workflow.\\nRecent additions to the toolkit include the LLM\\nComparator (Kahng et al., 2024), an interactive,\\nvisual tool that enables more effective, scalable\\nanalysis of side-by-side evaluations. Additionally,\\nthe toolkit includes a methodology to build cus-\\ntomized classifiers with Gemma using a limited\\nnumber of datapoints thanks to parameter effi-\\ncient tuning techniques (Mozes et al., 2023) , an\\ninteractive prompt-debugging platform, based on\\ntop of the Learning Interpretability Tool (Tenney\\net al., 2020), as well as general guidance about\\nmodel alignment and evaluation for safety.\\n9. Discussion and Conclusion\\nIn this work, we have presented Gemma 2, the\\nnewest additions to the Gemma family of open'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='model alignment and evaluation for safety.\\n9. Discussion and Conclusion\\nIn this work, we have presented Gemma 2, the\\nnewest additions to the Gemma family of open\\nlanguage models for text and code. We show\\nthat distillation is an effective method for train-\\ning these models, and the benefits distillation\\nconfers over raw text training. Specifically, we\\nshow how training over output probabilities can\\nproduce superior results over purely next token\\n6https://ai.google.dev/responsible\\nprediction. We hope that releasing these models\\nto the community will unlock access to capabili-\\nties previously only seen in large-scale LLMs and\\nfuel future waves of research and development.\\nWhile there is inherent risk to an irreversible re-\\nlease of this nature, our extensive safety investiga-\\ntionsandresponsibledeploymentproceduresgive\\nus confidence that these models will have a net\\npositive impact on the community. As discussed\\nin this report, there are still many limitations to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='tionsandresponsibledeploymentproceduresgive\\nus confidence that these models will have a net\\npositive impact on the community. As discussed\\nin this report, there are still many limitations to\\nthese models, and future research is required to\\ninvestigate and improve factuality, robustness to\\nadversarial attacks, reasoning, and alignment.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nContributions and Acknowledgments\\nCore contributors\\nMorgane Riviere‚àó\\nShreya Pathak‚àó\\nPier Giuseppe Sessa‚àó\\nCassidy Hardin‚àó\\nSurya Bhupatiraju\\nL√©onard Hussenot\\nThomas Mesnard\\nBobak Shahriari\\nAlexandre Ram√©\\nJohan Ferret\\nPeter Liu\\nPouya Tafti\\nAbe Friesen\\nMichelle Casbon\\nSabela Ramos\\nRavin Kumar\\nCharline Le Lan\\nSammy Jerome\\nAnton Tsitsulin\\nNino Vieillard\\nPiotr Stanczyk\\nSertan Girgin\\nNikola Momchev\\nMatt Hoffman\\nShantanu Thakoor\\nJean-Bastien Grill\\nBehnam Neyshabur\\nOlivier Bachem\\nContributors (alphabetical order)\\nAlanna Walton\\nAliaksei Severyn\\nAlicia Parrish\\nAliya Ahmad\\nAllen Hutchison\\nAlvin Abdagic\\nAmanda Carl\\nAmy Shen\\nAndy Brock\\nAndy Coenen\\nAnthony Laforge\\nAntonia Paterson\\nBen Bastian\\nBilal Piot\\nBo Wu\\n‚àóequal contributions.\\nBrandon Royal\\nCharlie Chen\\nChintu Kumar\\nChris Perry\\nChris Welty\\nChristopher A. Choquette-Choo\\nDanila Sinopalnikov\\nDavid Weinberger\\nDimple Vijaykumar\\nDominika Rogozi≈Ñska\\nDustin Herbison\\nElisa Bandy\\nEmma Wang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Charlie Chen\\nChintu Kumar\\nChris Perry\\nChris Welty\\nChristopher A. Choquette-Choo\\nDanila Sinopalnikov\\nDavid Weinberger\\nDimple Vijaykumar\\nDominika Rogozi≈Ñska\\nDustin Herbison\\nElisa Bandy\\nEmma Wang\\nEric Noland\\nErica Moreira\\nEvan Senter\\nEvgenii Eltyshev\\nFrancesco Visin\\nGabriel Rasskin\\nGary Wei\\nGlenn Cameron\\nGus Martins\\nHadi Hashemi\\nHanna Klimczak-Pluci≈Ñska\\nHarleen Batra\\nHarsh Dhand\\nIvan Nardini\\nJacinda Mein\\nJack Zhou\\nJames Svensson\\nJeff Stanway\\nJetha Chan\\nJin Peng Zhou\\nJoana Carrasqueira\\nJoana Iljazi\\nJocelyn Becker\\nJoe Fernandez\\nJoost van Amersfoort\\nJosh Gordon\\nJosh Lipschultz\\nJosh Newlan\\nJu-yeong Ji\\nKareem Mohamed\\nKartikeya Badola\\nKat Black\\nKatie Millican\\nKeelin McDonell\\nKelvin Nguyen\\nKiranbir Sodhia\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nKish Greene\\nLars Lowe Sjoesund\\nLauren Usui\\nLaurent Sifre\\nLena Heuermann\\nLeticia Lago\\nLilly McNealus\\nLivio Baldini Soares\\nLogan Kilpatrick\\nLucas Dixon\\nLuciano Martins\\nMachel Reid\\nManvinder Singh\\nMark Iverson\\nMartin G√∂rner\\nMat Velloso\\nMateo Wirth\\nMatt Davidow\\nMatt Miller\\nMatthew Rahtz\\nMatthew Watson\\nMeg Risdal\\nMehran Kazemi\\nMichael Moynihan\\nMing Zhang\\nMinsuk Kahng\\nMinwoo Park\\nMofi Rahman\\nMohit Khatwani\\nNatalie Dao\\nNenshad Bardoliwalla\\nNesh Devanathan\\nNeta Dumai\\nNilay Chauhan\\nOscar Wahltinez\\nPankil Botarda\\nParker Barnes\\nPaul Barham\\nPaul Michel\\nPengchong Jin\\nPetko Georgiev\\nPhil Culliton\\nPradeep Kuppala\\nRamona Comanescu\\nRamona Merhej\\nReena Jana\\nReza Ardeshir Rokni\\nRishabh Agarwal\\nRyan Mullins\\nSamaneh Saadat\\nSara Mc Carthy\\nSarah Cogan\\nSarah Perrin\\nS√©bastien M. R. Arnold\\nSebastian Krause\\nShengyang Dai\\nShruti Garg\\nShruti Sheth\\nSue Ronstrom\\nSusan Chan\\nTimothy Jordan\\nTing Yu\\nTom Eccles\\nTom Hennigan\\nTomas Kocisky\\nTulsee Doshi\\nVihan Jain'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='S√©bastien M. R. Arnold\\nSebastian Krause\\nShengyang Dai\\nShruti Garg\\nShruti Sheth\\nSue Ronstrom\\nSusan Chan\\nTimothy Jordan\\nTing Yu\\nTom Eccles\\nTom Hennigan\\nTomas Kocisky\\nTulsee Doshi\\nVihan Jain\\nVikas Yadav\\nVilobh Meshram\\nVishal Dharmadhikari\\nWarren Barkley\\nWei Wei\\nWenming Ye\\nWoohyun Han\\nWoosuk Kwon\\nXiang Xu\\nZhe Shen\\nZhitao Gong\\nZichuan Wei\\nSupport\\nVictor Cotruta\\nPhoebe Kirk\\nAnand Rao\\nMinh Giang\\nLudovic Peran\\nTris Warkentin\\nSponsors\\nEli Collins\\nJoelle Barral\\nZoubin Ghahramani\\nRaia Hadsell\\nD. Sculley\\nJeanine Banks\\nAnca Dragan\\nSlav Petrov\\nOriol Vinyals\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nJeff Dean\\nDemis Hassabis\\nKoray Kavukcuoglu\\nClement Farabet\\nTechnical advisors\\nElena Buchatskaya\\nSebastian Borgeaud\\nNoah Fiedel\\nLead\\nArmand Joulin\\nTechnical leads\\nKathleen Kenealy\\nRobert Dadashi\\nAlek Andreev\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nReferences\\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad,\\nI. Akkaya, F. L. Aleman, D. Almeida, J. Al-\\ntenschmidt, S. Altman, S. Anadkat, et al.\\nGpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\nR.Agarwal,N.Vieillard,Y.Zhou,P.Stanczyk,S.R.\\nGarea, M. Geist, and O. Bachem. On-policy\\ndistillation of language models: Learning from\\nself-generated mistakes. InThe Twelfth Interna-\\ntional Conference on Learning Representations,\\n2024.\\nAI@Meta. Llama 3 model card, 2024.\\nURL https://github.com/meta-llama/\\nllama3/blob/main/MODEL_CARD.md.\\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyan-\\nskiy, F. Lebr√≥n, and S. Sanghai. Gqa: Training\\ngeneralized multi-query transformer models\\nfrom multi-head checkpoints. arXiv preprint\\narXiv:2305.13245, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\\npelli, R. Cojocaru, M. Debbah, √âtienne Goffinet,\\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='arXiv:2305.13245, 2023.\\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\\npelli, R. Cojocaru, M. Debbah, √âtienne Goffinet,\\nD. Hesslow, J. Launay, Q. Malartic, D. Mazzotta,\\nB. Noune, B. Pannier, and G. Penedo. The fal-\\ncon series of open language models, 2023.\\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\\nikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,\\nZ. Chen, et al. Palm 2 technical report.arXiv\\npreprint arXiv:2305.10403, 2023.\\nJ. Austin, A. Odena, M. I. Nye, M. Bosma,\\nH. Michalewski, D. Dohan, E. Jiang, C. J.\\nCai, M. Terry, Q. V. Le, and C. Sutton. Pro-\\ngram synthesis with large language models.\\nCoRR, abs/2108.07732, 2021. URL https:\\n//arxiv.org/abs/2108.07732.\\nP. Barham, A. Chowdhery, J. Dean, S. Ghemawat,\\nS. Hand, D. Hurt, M. Isard, H. Lim, R. Pang,\\nS. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu. Path-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nI.Bello, H.Pham, Q.V.Le, M.Norouzi, andS.Ben-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='S. Roy, B. Saeta, P. Schuh, R. Sepassi, L. E.\\nShafey, C. A. Thekkath, and Y. Wu. Path-\\nways: Asynchronous distributed dataflow for\\nml, 2022.\\nI.Bello, H.Pham, Q.V.Le, M.Norouzi, andS.Ben-\\ngio. Neural combinatorial optimization with re-\\ninforcement learning.CoRR, abs/1611.09940,\\n2016. URL http://arxiv.org/abs/1611.\\n09940.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer.arXiv\\npreprint arXiv:2004.05150, 2020a.\\nI. Beltagy, M. E. Peters, and A. Cohan. Long-\\nformer: The long-document transformer.CoRR,\\nabs/2004.05150, 2020b. URL https://\\narxiv.org/abs/2004.05150.\\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-\\nplan, P. Dhariwal, A. Neelakantan, P. Shyam,\\nG.Sastry, A.Askell, S.Agarwal, A.Herbert-Voss,\\nG. Krueger, T. Henighan, R. Child, A. Ramesh,\\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse,\\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\\nI. Sutskever, and D. Amodei. Language models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='D. M. Ziegler, J. Wu, C. Winter, C. Hesse,\\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\\nJ. Clark, C. Berner, S. McCandlish, A. Radford,\\nI. Sutskever, and D. Amodei. Language models\\nare few-shot learners.CoRR, abs/2005.14165,\\n2020. URLhttps://arxiv.org/abs/2005.\\n14165.\\nN. Carlini, D. Ippolito, M. Jagielski, K. Lee,\\nF. Tramer, and C. Zhang. Quantifying memo-\\nrization across neural language models.arXiv\\npreprint arXiv:2202.07646, 2022.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P.\\nde Oliveira Pinto, J. Kaplan, H. Edwards,\\nY. Burda, N. Joseph, G. Brockman, A. Ray,\\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf,\\nG. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ry-\\nder, M. Pavlov, A. Power, L. Kaiser, M. Bavar-\\nian, C. Winter, P. Tillet, F. P. Such, D. Cum-\\nmings, M. Plappert, F. Chantzis, E. Barnes,\\nA.Herbert-Voss, W.H.Guss, A.Nichol, A.Paino,\\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='A.Herbert-Voss, W.H.Guss, A.Nichol, A.Paino,\\nN. Tezak, J. Tang, I. Babuschkin, S. Balaji,\\nS. Jain, W. Saunders, C. Hesse, A. N. Carr,\\nJ. Leike, J. Achiam, V. Misra, E. Morikawa,\\nA.Radford, M.Knight, M.Brundage, M.Murati,\\nK. Mayer, P. Welinder, B. McGrew, D. Amodei,\\nS. McCandlish, I. Sutskever, and W. Zaremba.\\nEvaluating large language models trained on\\ncode. CoRR, abs/2107.03374, 2021. URL\\nhttps://arxiv.org/abs/2107.03374.\\nW.-L. Chiang, L. Zheng, Y. Sheng, A. N. An-\\ngelopoulos, T. Li, D. Li, H. Zhang, B. Zhu,\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nM. Jordan, J. E. Gonzalez, and I. Stoica. Chat-\\nbot arena: An open platform for evaluating\\nllms by human preference, 2024.\\nC. Clark, K. Lee, M. Chang, T. Kwiatkowski,\\nM. Collins, and K. Toutanova. Boolq: Explor-\\ning the surprising difficulty of natural yes/no\\nquestions. CoRR, abs/1905.10044, 2019. URL\\nhttp://arxiv.org/abs/1905.10044.\\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen,\\nH. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, C. Hesse, and J. Schul-\\nman. Training verifiers to solve math word\\nproblems. CoRR, abs/2110.14168, 2021. URL\\nhttps://arxiv.org/abs/2110.14168.\\nGemini Team. Gemini: A family of highly capable\\nmultimodal models, 2023.\\nGemini Team. Gemini 1.5: Unlocking multimodal\\nunderstanding across millions of tokens of con-\\ntext, 2024.\\nGemma Team. Gemma: Open models based on\\ngemini research and technology, 2024.\\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\\nKnowledge distillation of large language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='text, 2024.\\nGemma Team. Gemma: Open models based on\\ngemini research and technology, 2024.\\nY. Gu, L. Dong, F. Wei, and M. Huang. Minillm:\\nKnowledge distillation of large language mod-\\nels. InThe Twelfth International Conference on\\nLearning Representations, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou,\\nM. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understand-\\ning. CoRR, abs/2009.03300, 2020. URL\\nhttps://arxiv.org/abs/2009.03300.\\nG. Hinton, O. Vinyals, and J. Dean. Distilling the\\nknowledge in a neural network.arXiv preprint\\narXiv:1503.02531, 2015.\\nJ. Hoffmann, S. Borgeaud, A. Mensch,\\nE. Buchatskaya, T. Cai, E. Rutherford, D. d. L.\\nCasas, L. A. Hendricks, J. Welbl, A. Clark, et al.\\nTraining compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556,\\n2022.\\nD. Ippolito, F. Tram√®r, M. Nasr, C. Zhang,\\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\\nN. Carlini. Preventing verbatim memorization\\nin language models gives a false sense of pri-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2022.\\nD. Ippolito, F. Tram√®r, M. Nasr, C. Zhang,\\nM. Jagielski, K. Lee, C. A. Choquette-Choo, and\\nN. Carlini. Preventing verbatim memorization\\nin language models gives a false sense of pri-\\nvacy. arXiv preprint arXiv:2210.17546, 2022.\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bam-\\nford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG.Lengyel, G.Lample, L.Saulnier, L.R.Lavaud,\\nM.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral\\n7b, 2023.\\nM. Kahng, I. Tenney, M. Pushkarna, M. X. Liu,\\nJ. Wexler, E. Reif, K. Kallarackal, M. Chang,\\nM. Terry, and L. Dixon. Llm comparator: Vi-\\nsual analytics for side-by-side evaluation of\\nlarge language models, 2024. URL https:\\n//arxiv.org/abs/2402.10524.\\nM. Kinniment, L. J. K. Sato, H. Du, B. Goodrich,\\nM.Hasin,L.Chan,L.H.Miles,T.R.Lin,H.Wijk,\\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\\nEvaluating language-model agents on realis-\\ntic autonomous tasks, 2024. URLhttps://\\narxiv.org/abs/2312.11671.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='M.Hasin,L.Chan,L.H.Miles,T.R.Lin,H.Wijk,\\nJ. Burget, A. Ho, E. Barnes, and P. Christiano.\\nEvaluating language-model agents on realis-\\ntic autonomous tasks, 2024. URLhttps://\\narxiv.org/abs/2312.11671.\\nT. Kudo and J. Richardson. SentencePiece: A\\nsimple and language independent subword to-\\nkenizeranddetokenizerforneuraltextprocess-\\ning. InE.BlancoandW.Lu,editors, Proceedings\\nof the 2018 Conference on Empirical Methods in\\nNatural Language Processing: System Demon-\\nstrations, pages 66‚Äì71, Brussels, Belgium, Nov.\\n2018. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/D18-2012. URL\\nhttps://aclanthology.org/D18-2012.\\nS. Kudugunta, I. Caswell, B. Zhang, X. Garcia,\\nC. A. Choquette-Choo, K. Lee, D. Xin, A. Kusu-\\npati, R. Stella, A. Bapna, et al. Madlad-400:\\nA multilingual and document-level large au-\\ndited dataset.arXiv preprint arXiv:2309.04662,\\n2023.\\nT. Kwiatkowski, J. Palomaki, O. Redfield,\\nM. Collins, A. Parikh, C. Alberti, D. Epstein,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='A multilingual and document-level large au-\\ndited dataset.arXiv preprint arXiv:2309.04662,\\n2023.\\nT. Kwiatkowski, J. Palomaki, O. Redfield,\\nM. Collins, A. Parikh, C. Alberti, D. Epstein,\\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai,\\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural ques-\\ntions: A benchmark for question answering\\nresearch. Transactions of the Association for\\nComputational Linguistics, 7:452‚Äì466, 2019.\\ndoi: 10.1162/tacl_a_00276. URL https://\\naclanthology.org/Q19-1026.\\nZ. Lin, J. Cui, X. Liao, and X. Wang. Malla: De-\\nmystifying real-world large language model in-\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\ntegrated malicious services, 2024. URLhttps:\\n//arxiv.org/abs/2401.03315.\\nM. Luong, H. Pham, and C. D. Manning. Effective\\napproaches to attention-based neural machine\\ntranslation. CoRR,abs/1508.04025, 2015. URL\\nhttp://arxiv.org/abs/1508.04025.\\nMacknight, Aung, and Gomes. Personal Commu-\\nnication, 2024.\\nM. Mozes, J. Hoffmann, K. Tomanek, M. Kouate,\\nN. Thain, A. Yuan, T. Bolukbasi, and L. Dixon.\\nTowards agile text classifiers for everyone,\\n2023. URLhttps://arxiv.org/abs/2302.\\n06541.\\nM. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F.\\nCooper, D. Ippolito, C. A. Choquette-Choo,\\nE. Wallace, F. Tram√®r, and K. Lee. Scal-\\nable extraction of training data from (pro-\\nduction) language models. arXiv preprint\\narXiv:2311.17035, 2023.\\nM. Phuong, M. Aitchison, E. Catt, S. Co-\\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='M. Phuong, M. Aitchison, E. Catt, S. Co-\\ngan, A. Kaskasoli, V. Krakovna, D. Lindner,\\nM. Rahtz, Y. Assael, S. Hodkinson, H. Howard,\\nT. Lieberum, R. Kumar, M. A. Raad, A. Webson,\\nL. Ho, S. Lin, S. Farquhar, M. Hutter, G. Dele-\\ntang, A. Ruoss, S. El-Sayed, S. Brown, A. Dra-\\ngan, R. Shah, A. Dafoe, and T. Shevlane. Evalu-\\natingfrontiermodelsfordangerouscapabilities,\\n2024. URLhttps://arxiv.org/abs/2403.\\n13793.\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\\nand I. Sutskever. Language models are unsu-\\npervised multitask learners, 2019.\\nC. Raffel, N. Shazeer, A. Roberts, K. Lee,\\nS. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\\nLiu. Exploring the limits of transfer learning\\nwith a unified text-to-text transformer.CoRR,\\nabs/1910.10683, 2019. URLhttp://arxiv.\\norg/abs/1910.10683.\\nA. Ram√©, J. Ferret, N. Vieillard, R. Dadashi,\\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\\nA. Douillard, and O. Bachem. Warp: On the\\nbenefits of weight averaged rewarded policies,\\n2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='A. Ram√©, J. Ferret, N. Vieillard, R. Dadashi,\\nL. Hussenot, P.-L. Cedoz, P. G. Sessa, S. Girgin,\\nA. Douillard, and O. Bachem. Warp: On the\\nbenefits of weight averaged rewarded policies,\\n2024.\\nJ. Ren, S. Rajbhandari, R. Y. Aminabadi,\\nO. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He.\\n{Zero-offload}: Democratizing{billion-scale}\\nmodel training. In2021 USENIX Annual Tech-\\nnical Conference (USENIX ATC 21), pages 551‚Äì\\n564, 2021.\\nA. Roberts, H. W. Chung, G. Mishra, A. Levskaya,\\nJ. Bradbury, D. Andor, S. Narang, B. Lester,\\nC. Gaffney, A. Mohiuddin, et al. Scaling up\\nmodels and data with t5x and seqio. Jour-\\nnal of Machine Learning Research, 24(377):1‚Äì8,\\n2023.\\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and\\nY. Choi. WINOGRANDE: an adversarial\\nwinograd schema challenge at scale. CoRR,\\nabs/1907.10641, 2019. URLhttp://arxiv.\\norg/abs/1907.10641.\\nN. Shazeer. GLU variants improve transformer.\\nCoRR, abs/2002.05202, 2020. URL https:\\n//arxiv.org/abs/2002.05202.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='abs/1907.10641, 2019. URLhttp://arxiv.\\norg/abs/1907.10641.\\nN. Shazeer. GLU variants improve transformer.\\nCoRR, abs/2002.05202, 2020. URL https:\\n//arxiv.org/abs/2002.05202.\\nT. Shevlane, S. Farquhar, B. Garfinkel, M. Phuong,\\nJ. Whittlestone, J. Leung, D. Kokotajlo, N. Mar-\\nchal, M. Anderljung, N. Kolt, L. Ho, D. Sid-\\ndarth, S. Avin, W. Hawkins, B. Kim, I. Gabriel,\\nV. Bolina, J. Clark, Y. Bengio, P. Christiano, and\\nA. Dafoe. Model evaluation for extreme risks,\\n2023. URLhttps://arxiv.org/abs/2305.\\n15324.\\nJ. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu. Roformer:\\nEnhanced transformer with rotary position em-\\nbedding. CoRR, abs/2104.09864, 2021. URL\\nhttps://arxiv.org/abs/2104.09864.\\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann,\\nY. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\\nE. H. Chi, D. Zhou, and J. Wei. Challenging\\nbig-bench tasks and whether chain-of-thought\\ncan solve them, 2022.\\nQ. Team. Introducing qwen1.5, February\\n2024. URL https://qwenlm.github.io/\\nblog/qwen1.5/.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='big-bench tasks and whether chain-of-thought\\ncan solve them, 2022.\\nQ. Team. Introducing qwen1.5, February\\n2024. URL https://qwenlm.github.io/\\nblog/qwen1.5/.\\nI. Tenney, J. Wexler, J. Bastings, T. Boluk-\\nbasi, A. Coenen, S. Gehrmann, E. Jiang,\\nM. Pushkarna, C. Radebaugh, E. Reif, and\\nA. Yuan. The language interpretability tool: Ex-\\ntensible, interactive visualizations and analysis\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='Gemma 2: Improving Open Language Models at a Practical Size\\nfor nlp models, 2020. URLhttps://arxiv.\\norg/abs/2008.05122.\\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-\\nA. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal,\\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\\nE. Grave, and G. Lample. Llama: Open and\\nefficient foundation language models, 2023.\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\\nsukhin. Attention is all you need. CoRR,\\nabs/1706.03762, 2017. URLhttp://arxiv.\\norg/abs/1706.03762.\\nL. Weidinger, J. Mellor, M. Rauh, C. Griffin,\\nJ. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\\nB. Balle, A. Kasirzadeh, Z. Kenton, S. Brown,\\nW. Hawkins, T. Stepleton, C. Biles, A. Birhane,\\nJ. Haas, L. Rimell, L. A. Hendricks, W. Isaac,\\nS. Legassick, G. Irving, and I. Gabriel. Ethical\\nand social risks of harm from language mod-\\nels, 2021. URL https://arxiv.org/abs/\\n2112.04359.\\nxAI. grok-1, 2024. URLhttps://github.com/\\nxai-org/grok-1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='and social risks of harm from language mod-\\nels, 2021. URL https://arxiv.org/abs/\\n2112.04359.\\nxAI. grok-1, 2024. URLhttps://github.com/\\nxai-org/grok-1.\\nXLA. Xla: Optimizing compiler for tensor-\\nflow, 2019. URLhttps://www.tensorflow.\\norg/xla.\\nY. Xu, H. Lee, D. Chen, B. A. Hechtman, Y. Huang,\\nR. Joshi, M. Krikun, D. Lepikhin, A. Ly, M. Mag-\\ngioni, R. Pang, N. Shazeer, S. Wang, T. Wang,\\nY. Wu, and Z. Chen. GSPMD: general and\\nscalable parallelization for ML computation\\ngraphs. CoRR, abs/2105.04663, 2021. URL\\nhttps://arxiv.org/abs/2105.04663.\\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao.\\nIntercode: Standardizing and benchmarking\\ninteractive coding with execution feedback,\\n2023. URLhttps://arxiv.org/abs/2306.\\n14898.\\nB. Zhang and R. Sennrich. Root mean square\\nlayer normalization. CoRR, abs/1910.07467,\\n2019. URL http://arxiv.org/abs/1910.\\n07467.\\nL.Zheng, W.-L.Chiang, Y.Sheng, T.Li, S.Zhuang,\\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\\net al. Lmsys-chat-1m: A large-scale real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-03T01:05:19+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-03T01:05:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Gemma.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'Gemma.pdf', 'file_type': 'pdf'}, page_content='2019. URL http://arxiv.org/abs/1910.\\n07467.\\nL.Zheng, W.-L.Chiang, Y.Sheng, T.Li, S.Zhuang,\\nZ. Wu, Y. Zhuang, Z. Li, Z. Lin, E. Xing,\\net al. Lmsys-chat-1m: A large-scale real-\\nworld llm conversation dataset.arXiv preprint\\narXiv:2309.11998, 2023.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='GloVe: Global Vectors for Word Representation\\nJeffrey Pennington, Richard Socher, Christopher D. Manning\\nComputer Science Department, Stanford University, Stanford, CA 94305\\njpennin@stanford.edu, richard@socher.org, manning@stanford.edu\\nAbstract\\nRecent methods for learning vector space\\nrepresentations of words have succeeded\\nin capturing Ô¨Åne-grained semantic and\\nsyntactic regularities using vector arith-\\nmetic, but the origin of these regularities\\nhas remained opaque. We analyze and\\nmake explicit the model properties needed\\nfor such regularities to emerge in word\\nvectors. The result is a new global log-\\nbilinear regression model that combines\\nthe advantages of the two major model\\nfamilies in the literature: global matrix\\nfactorization and local context window\\nmethods. Our model efÔ¨Åciently leverages\\nstatistical information by training only on\\nthe nonzero elements in a word-word co-\\noccurrence matrix, rather than on the en-\\ntire sparse matrix or on individual context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='statistical information by training only on\\nthe nonzero elements in a word-word co-\\noccurrence matrix, rather than on the en-\\ntire sparse matrix or on individual context\\nwindows in a large corpus. The model pro-\\nduces a vector space with meaningful sub-\\nstructure, as evidenced by its performance\\nof 75% on a recent word analogy task. It\\nalso outperforms related models on simi-\\nlarity tasks and named entity recognition.\\n1 Introduction\\nSemantic vector space models of language repre-\\nsent each word with a real-valued vector. These\\nvectors can be used as features in a variety of ap-\\nplications, such as information retrieval (Manning\\net al., 2008), document classiÔ¨Åcation (Sebastiani,\\n2002), question answering (Tellex et al., 2003),\\nnamed entity recognition (Turian et al., 2010), and\\nparsing (Socher et al., 2013).\\nMost word vector methods rely on the distance\\nor angle between pairs of word vectors as the pri-\\nmary method for evaluating the intrinsic quality'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='parsing (Socher et al., 2013).\\nMost word vector methods rely on the distance\\nor angle between pairs of word vectors as the pri-\\nmary method for evaluating the intrinsic quality\\nof such a set of word representations. Recently,\\nMikolov et al. (2013c) introduced a new evalua-\\ntion scheme based on word analogies that probes\\nthe Ô¨Åner structure of the word vector space by ex-\\namining not the scalar distance between word vec-\\ntors, but rather their various dimensions of dif-\\nference. For example, the analogy ‚Äúking is to\\nqueen as man is to woman‚Äù should be encoded\\nin the vector space by the vector equation king ‚àí\\nqueen = man ‚àíwoman. This evaluation scheme\\nfavors models that produce dimensions of mean-\\ning, thereby capturing the multi-clustering idea of\\ndistributed representations (Bengio, 2009).\\nThe two main model families for learning word\\nvectors are: 1) global matrix factorization meth-\\nods, such as latent semantic analysis (LSA) (Deer-\\nwester et al., 1990) and 2) local context window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='The two main model families for learning word\\nvectors are: 1) global matrix factorization meth-\\nods, such as latent semantic analysis (LSA) (Deer-\\nwester et al., 1990) and 2) local context window\\nmethods, such as the skip-gram model of Mikolov\\net al. (2013c). Currently, both families suffer sig-\\nniÔ¨Åcant drawbacks. While methods like LSA ef-\\nÔ¨Åciently leverage statistical information, they do\\nrelatively poorly on the word analogy task, indi-\\ncating a sub-optimal vector space structure. Meth-\\nods like skip-gram may do better on the analogy\\ntask, but they poorly utilize the statistics of the cor-\\npus since they train on separate local context win-\\ndows instead of on global co-occurrence counts.\\nIn this work, we analyze the model properties\\nnecessary to produce linear directions of meaning\\nand argue that global log-bilinear regression mod-\\nels are appropriate for doing so. We propose a spe-\\nciÔ¨Åc weighted least squares model that trains on\\nglobal word-word co-occurrence counts and thus'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and argue that global log-bilinear regression mod-\\nels are appropriate for doing so. We propose a spe-\\nciÔ¨Åc weighted least squares model that trains on\\nglobal word-word co-occurrence counts and thus\\nmakes efÔ¨Åcient use of statistics. The model pro-\\nduces a word vector space with meaningful sub-\\nstructure, as evidenced by its state-of-the-art per-\\nformance of 75% accuracy on the word analogy\\ndataset. We also demonstrate that our methods\\noutperform other current methods on several word\\nsimilarity tasks, and also on a common named en-\\ntity recognition (NER) benchmark.\\nWe provide the source code for the model as\\nwell as trained word vectors at http://nlp.\\nstanford.edu/projects/glove/.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='2 Related Work\\nMatrix Factorization Methods. Matrix factor-\\nization methods for generating low-dimensional\\nword representations have roots stretching as far\\nback as LSA. These methods utilize low-rank ap-\\nproximations to decompose large matrices that\\ncapture statistical information about a corpus. The\\nparticular type of information captured by such\\nmatrices varies by application. In LSA, the ma-\\ntrices are of ‚Äúterm-document‚Äù type, i.e., the rows\\ncorrespond to words or terms, and the columns\\ncorrespond to different documents in the corpus.\\nIn contrast, the Hyperspace Analogue to Language\\n(HAL) (Lund and Burgess, 1996), for example,\\nutilizes matrices of ‚Äúterm-term‚Äù type, i.e., the rows\\nand columns correspond to words and the entries\\ncorrespond to the number of times a given word\\noccurs in the context of another given word.\\nA main problem with HAL and related meth-\\nods is that the most frequent words contribute a\\ndisproportionate amount to the similarity measure:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='occurs in the context of another given word.\\nA main problem with HAL and related meth-\\nods is that the most frequent words contribute a\\ndisproportionate amount to the similarity measure:\\nthe number of times two words co-occur with the\\nor and, for example, will have a large effect on\\ntheir similarity despite conveying relatively little\\nabout their semantic relatedness. A number of\\ntechniques exist that addresses this shortcoming of\\nHAL, such as the COALS method (Rohde et al.,\\n2006), in which the co-occurrence matrix is Ô¨Årst\\ntransformed by an entropy- or correlation-based\\nnormalization. An advantage of this type of trans-\\nformation is that the raw co-occurrence counts,\\nwhich for a reasonably sized corpus might span\\n8 or 9 orders of magnitude, are compressed so as\\nto be distributed more evenly in a smaller inter-\\nval. A variety of newer models also pursue this\\napproach, including a study (Bullinaria and Levy,\\n2007) that indicates that positive pointwise mu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='to be distributed more evenly in a smaller inter-\\nval. A variety of newer models also pursue this\\napproach, including a study (Bullinaria and Levy,\\n2007) that indicates that positive pointwise mu-\\ntual information (PPMI) is a good transformation.\\nMore recently, a square root type transformation\\nin the form of Hellinger PCA (HPCA) (Lebret and\\nCollobert, 2014) has been suggested as an effec-\\ntive way of learning word representations.\\nShallow Window-Based Methods. Another\\napproach is to learn word representations that aid\\nin making predictions within local context win-\\ndows. For example, Bengio et al. (2003) intro-\\nduced a model that learns word vector representa-\\ntions as part of a simple neural network architec-\\nture for language modeling. Collobert and Weston\\n(2008) decoupled the word vector training from\\nthe downstream training objectives, which paved\\nthe way for Collobert et al. (2011) to use the full\\ncontext of a word for learning the word represen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='(2008) decoupled the word vector training from\\nthe downstream training objectives, which paved\\nthe way for Collobert et al. (2011) to use the full\\ncontext of a word for learning the word represen-\\ntations, rather than just the preceding context as is\\nthe case with language models.\\nRecently, the importance of the full neural net-\\nwork structure for learning useful word repre-\\nsentations has been called into question. The\\nskip-gram and continuous bag-of-words (CBOW)\\nmodels of Mikolov et al. (2013a) propose a sim-\\nple single-layer architecture based on the inner\\nproduct between two word vectors. Mnih and\\nKavukcuoglu (2013) also proposed closely-related\\nvector log-bilinear models, vLBL and ivLBL, and\\nLevy et al. (2014) proposed explicit word embed-\\ndings based on a PPMI metric.\\nIn the skip-gram and ivLBL models, the objec-\\ntive is to predict a word‚Äôs context given the word\\nitself, whereas the objective in the CBOW and\\nvLBL models is to predict a word given its con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='In the skip-gram and ivLBL models, the objec-\\ntive is to predict a word‚Äôs context given the word\\nitself, whereas the objective in the CBOW and\\nvLBL models is to predict a word given its con-\\ntext. Through evaluation on a word analogy task,\\nthese models demonstrated the capacity to learn\\nlinguistic patterns as linear relationships between\\nthe word vectors.\\nUnlike the matrix factorization methods, the\\nshallow window-based methods suffer from the\\ndisadvantage that they do not operate directly on\\nthe co-occurrence statistics of the corpus. Instead,\\nthese models scan context windows across the en-\\ntire corpus, which fails to take advantage of the\\nvast amount of repetition in the data.\\n3 The GloVe Model\\nThe statistics of word occurrences in a corpus is\\nthe primary source of information available to all\\nunsupervised methods for learning word represen-\\ntations, and although many such methods now ex-\\nist, the question still remains as to how meaning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='the primary source of information available to all\\nunsupervised methods for learning word represen-\\ntations, and although many such methods now ex-\\nist, the question still remains as to how meaning\\nis generated from these statistics, and how the re-\\nsulting word vectors might represent that meaning.\\nIn this section, we shed some light on this ques-\\ntion. We use our insights to construct a new model\\nfor word representation which we call GloVe, for\\nGlobal Vectors, because the global corpus statis-\\ntics are captured directly by the model.\\nFirst we establish some notation. Let the matrix\\nof word-word co-occurrence counts be denoted by\\nX, whose entries Xi jtabulate the number of times\\nword j occurs in the context of word i. Let Xi =‚àë\\nk Xik be the number of times any word appears\\nin the context of wordi. Finally, let Pi j = P( j|i) =\\nXi j/Xi be the probability that word j appear in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Table 1: Co-occurrence probabilities for target wordsice and steam with selected context words from a 6\\nbillion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion\\ncancel out, so that large values (much greater than 1) correlate well with properties speciÔ¨Åc to ice, and\\nsmall values (much less than 1) correlate well with properties speciÔ¨Åc of steam.\\nProbability and Ratio k = solid k = gas k = water k = fashion\\nP(k|ice) 1.9 √ó10‚àí4 6.6 √ó10‚àí5 3.0 √ó10‚àí3 1.7 √ó10‚àí5\\nP(k|steam) 2.2 √ó10‚àí5 7.8 √ó10‚àí4 2.2 √ó10‚àí3 1.8 √ó10‚àí5\\nP(k|ice)/P(k|steam) 8.9 8 .5 √ó10‚àí2 1.36 0 .96\\ncontext of word i.\\nWe begin with a simple example that showcases\\nhow certain aspects of meaning can be extracted\\ndirectly from co-occurrence probabilities. Con-\\nsider two words i and j that exhibit a particular as-\\npect of interest; for concreteness, suppose we are\\ninterested in the concept of thermodynamic phase,\\nfor which we might take i = ice and j = steam.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='pect of interest; for concreteness, suppose we are\\ninterested in the concept of thermodynamic phase,\\nfor which we might take i = ice and j = steam.\\nThe relationship of these words can be examined\\nby studying the ratio of their co-occurrence prob-\\nabilities with various probe words, k. For words\\nk related to ice but not steam, say k = solid, we\\nexpect the ratio Pik /Pjk will be large. Similarly,\\nfor words k related to steam but not ice, say k =\\ngas, the ratio should be small. For words k like\\nwater or fashion, that are either related to both ice\\nand steam, or to neither, the ratio should be close\\nto one. Table 1 shows these probabilities and their\\nratios for a large corpus, and the numbers conÔ¨Årm\\nthese expectations. Compared to the raw probabil-\\nities, the ratio is better able to distinguish relevant\\nwords (solid and gas) from irrelevant words (water\\nand fashion) and it is also better able to discrimi-\\nnate between the two relevant words.\\nThe above argument suggests that the appropri-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='words (solid and gas) from irrelevant words (water\\nand fashion) and it is also better able to discrimi-\\nnate between the two relevant words.\\nThe above argument suggests that the appropri-\\nate starting point for word vector learning should\\nbe with ratios of co-occurrence probabilities rather\\nthan the probabilities themselves. Noting that the\\nratio Pik /Pjk depends on three words i, j, and k,\\nthe most general model takes the form,\\nF(wi,wj , Àúwk ) = Pik\\nPjk\\n, (1)\\nwhere w ‚àà Rd are word vectors and Àú w ‚àà Rd\\nare separate context word vectors whose role will\\nbe discussed in Section 4.2. In this equation, the\\nright-hand side is extracted from the corpus, and\\nF may depend on some as-of-yet unspeciÔ¨Åed pa-\\nrameters. The number of possibilities forF is vast,\\nbut by enforcing a few desiderata we can select a\\nunique choice. First, we would like F to encode\\nthe information present the ratio Pik /Pjk in the\\nword vector space. Since vector spaces are inher-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='but by enforcing a few desiderata we can select a\\nunique choice. First, we would like F to encode\\nthe information present the ratio Pik /Pjk in the\\nword vector space. Since vector spaces are inher-\\nently linear structures, the most natural way to do\\nthis is with vector differences. With this aim, we\\ncan restrict our consideration to those functions F\\nthat depend only on the difference of the two target\\nwords, modifying Eqn. (1) to,\\nF(wi ‚àíwj , Àúwk ) = Pik\\nPjk\\n. (2)\\nNext, we note that the arguments of F in Eqn. (2)\\nare vectors while the right-hand side is a scalar.\\nWhile F could be taken to be a complicated func-\\ntion parameterized by, e.g., a neural network, do-\\ning so would obfuscate the linear structure we are\\ntrying to capture. To avoid this issue, we can Ô¨Årst\\ntake the dot product of the arguments,\\nF\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= Pik\\nPjk\\n, (3)\\nwhich prevents F from mixing the vector dimen-\\nsions in undesirable ways. Next, note that for\\nword-word co-occurrence matrices, the distinction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='F\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= Pik\\nPjk\\n, (3)\\nwhich prevents F from mixing the vector dimen-\\nsions in undesirable ways. Next, note that for\\nword-word co-occurrence matrices, the distinction\\nbetween a word and a context word is arbitrary and\\nthat we are free to exchange the two roles. To do so\\nconsistently, we must not only exchange w ‚Üî Àúw\\nbut also X ‚ÜîXT . Our Ô¨Ånal model should be in-\\nvariant under this relabeling, but Eqn. (3) is not.\\nHowever, the symmetry can be restored in two\\nsteps. First, we require that F be a homomorphism\\nbetween the groups (R,+) and (R>0,√ó), i.e.,\\nF\\n(\\n(wi ‚àíwj )T Àúwk\\n)\\n= F(wT\\ni Àúwk )\\nF(wT\\nj Àúwk ) , (4)\\nwhich, by Eqn. (3), is solved by,\\nF(wT\\ni Àúwk ) = Pik = Xik\\nXi\\n. (5)\\nThe solution to Eqn. (4) is F = exp, or,\\nwT\\ni Àúwk = log(Pik ) = log(Xik ) ‚àílog(Xi ) . (6)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Next, we note that Eqn. (6) would exhibit the ex-\\nchange symmetry if not for the log( Xi ) on the\\nright-hand side. However, this term is indepen-\\ndent of k so it can be absorbed into a bias bi for\\nwi. Finally, adding an additional bias Àúbk for Àúwk\\nrestores the symmetry,\\nwT\\ni Àúwk + bi + Àúbk = log(Xik ) . (7)\\nEqn. (7) is a drastic simpliÔ¨Åcation over Eqn. (1),\\nbut it is actually ill-deÔ¨Åned since the logarithm di-\\nverges whenever its argument is zero. One reso-\\nlution to this issue is to include an additive shift\\nin the logarithm, log( Xik ) ‚Üílog(1 + Xik ), which\\nmaintains the sparsity of X while avoiding the di-\\nvergences. The idea of factorizing the log of the\\nco-occurrence matrix is closely related to LSA and\\nwe will use the resulting model as a baseline in\\nour experiments. A main drawback to this model\\nis that it weighs all co-occurrences equally, even\\nthose that happen rarely or never. Such rare co-\\noccurrences are noisy and carry less information'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='our experiments. A main drawback to this model\\nis that it weighs all co-occurrences equally, even\\nthose that happen rarely or never. Such rare co-\\noccurrences are noisy and carry less information\\nthan the more frequent ones ‚Äî yet even just the\\nzero entries account for 75‚Äì95% of the data in X,\\ndepending on the vocabulary size and corpus.\\nWe propose a new weighted least squares re-\\ngression model that addresses these problems.\\nCasting Eqn. (7) as a least squares problem and\\nintroducing a weighting function f (Xi j) into the\\ncost function gives us the model\\nJ =\\nV‚àë\\ni,j=1\\nf\\n(\\nXi j\\n) (\\nwT\\ni Àúwj + bi + Àúbj ‚àílog Xi j\\n)2\\n,\\n(8)\\nwhere V is the size of the vocabulary. The weight-\\ning function should obey the following properties:\\n1. f (0) = 0. If f is viewed as a continuous\\nfunction, it should vanish as x ‚Üí 0 fast\\nenough that the limx‚Üí0 f (x) log2 x is Ô¨Ånite.\\n2. f (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\n3. f (x) should be relatively small for large val-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='enough that the limx‚Üí0 f (x) log2 x is Ô¨Ånite.\\n2. f (x) should be non-decreasing so that rare\\nco-occurrences are not overweighted.\\n3. f (x) should be relatively small for large val-\\nues of x, so that frequent co-occurrences are\\nnot overweighted.\\nOf course a large number of functions satisfy these\\nproperties, but one class of functions that we found\\nto work well can be parameterized as,\\nf (x) =\\n{ (x/xmax)Œ± if x < xmax\\n1 otherwise . (9)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.0\\nFigure 1: Weighting function f with Œ± = 3/4.\\nThe performance of the model depends weakly on\\nthe cutoff, which we Ô¨Åx to xmax = 100 for all our\\nexperiments. We found that Œ± = 3/4 gives a mod-\\nest improvement over a linear version with Œ± = 1.\\nAlthough we offer only empirical motivation for\\nchoosing the value 3/4, it is interesting that a sim-\\nilar fractional power scaling was found to give the\\nbest performance in (Mikolov et al., 2013a).\\n3.1 Relationship to Other Models\\nBecause all unsupervised methods for learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ilar fractional power scaling was found to give the\\nbest performance in (Mikolov et al., 2013a).\\n3.1 Relationship to Other Models\\nBecause all unsupervised methods for learning\\nword vectors are ultimately based on the occur-\\nrence statistics of a corpus, there should be com-\\nmonalities between the models. Nevertheless, cer-\\ntain models remain somewhat opaque in this re-\\ngard, particularly the recent window-based meth-\\nods like skip-gram and ivLBL. Therefore, in this\\nsubsection we show how these models are related\\nto our proposed model, as deÔ¨Åned in Eqn. (8).\\nThe starting point for the skip-gram or ivLBL\\nmethods is a model Qi j for the probability that\\nword j appears in the context of word i. For con-\\ncreteness, let us assume that Qi jis a softmax,\\nQi j = exp(wT\\ni Àúwj )\\n‚àëV\\nk=1 exp(wT\\ni Àúwk )\\n. (10)\\nMost of the details of these models are irrelevant\\nfor our purposes, aside from the the fact that they\\nattempt to maximize the log probability as a con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='i Àúwj )\\n‚àëV\\nk=1 exp(wT\\ni Àúwk )\\n. (10)\\nMost of the details of these models are irrelevant\\nfor our purposes, aside from the the fact that they\\nattempt to maximize the log probability as a con-\\ntext window scans over the corpus. Training pro-\\nceeds in an on-line, stochastic fashion, but the im-\\nplied global objective function can be written as,\\nJ = ‚àí\\n‚àë\\ni‚ààcorpus\\nj‚ààcontext(i)\\nlog Qi j. (11)\\nEvaluating the normalization factor of the soft-\\nmax for each term in this sum is costly. To al-\\nlow for efÔ¨Åcient training, the skip-gram and ivLBL\\nmodels introduce approximations to Qi j. How-\\never, the sum in Eqn. (11) can be evaluated much'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='more efÔ¨Åciently if we Ô¨Årst group together those\\nterms that have the same values for i and j,\\nJ = ‚àí\\nV‚àë\\ni=1\\nV‚àë\\nj=1\\nXi jlog Qi j, (12)\\nwhere we have used the fact that the number of\\nlike terms is given by the co-occurrence matrix X.\\nRecalling our notation for Xi = ‚àë\\nk Xik and\\nPi j = Xi j/Xi, we can rewrite J as,\\nJ = ‚àí\\nV‚àë\\ni=1\\nXi\\nV‚àë\\nj=1\\nPi jlog Qi j =\\nV‚àë\\ni=1\\nXi H(Pi,Qi ) ,\\n(13)\\nwhere H(Pi,Qi ) is the cross entropy of the dis-\\ntributions Pi and Qi, which we deÔ¨Åne in analogy\\nto Xi. As a weighted sum of cross-entropy error,\\nthis objective bears some formal resemblance to\\nthe weighted least squares objective of Eqn. (8).\\nIn fact, it is possible to optimize Eqn. (13) directly\\nas opposed to the on-line training methods used in\\nthe skip-gram and ivLBL models. One could inter-\\npret this objective as a ‚Äúglobal skip-gram‚Äù model,\\nand it might be interesting to investigate further.\\nOn the other hand, Eqn. (13) exhibits a number of\\nundesirable properties that ought to be addressed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and it might be interesting to investigate further.\\nOn the other hand, Eqn. (13) exhibits a number of\\nundesirable properties that ought to be addressed\\nbefore adopting it as a model for learning word\\nvectors.\\nTo begin, cross entropy error is just one among\\nmany possible distance measures between prob-\\nability distributions, and it has the unfortunate\\nproperty that distributions with long tails are of-\\nten modeled poorly with too much weight given\\nto the unlikely events. Furthermore, for the mea-\\nsure to be bounded it requires that the model dis-\\ntribution Q be properly normalized. This presents\\na computational bottleneck owing to the sum over\\nthe whole vocabulary in Eqn. (10), and it would be\\ndesirable to consider a different distance measure\\nthat did not require this property of Q. A natural\\nchoice would be a least squares objective in which\\nnormalization factors in Q and P are discarded,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n( ÀÜPi j‚àí ÀÜQi j\\n)2 (14)\\nwhere ÀÜPi j = Xi j and ÀÜQi j = exp(wT\\ni Àúwj ) are the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='choice would be a least squares objective in which\\nnormalization factors in Q and P are discarded,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n( ÀÜPi j‚àí ÀÜQi j\\n)2 (14)\\nwhere ÀÜPi j = Xi j and ÀÜQi j = exp(wT\\ni Àúwj ) are the\\nunnormalized distributions. At this stage another\\nproblem emerges, namely thatXi joften takes very\\nlarge values, which can complicate the optimiza-\\ntion. An effective remedy is to minimize the\\nsquared error of the logarithms of ÀÜP and ÀÜQ instead,\\nÀÜJ =\\n‚àë\\ni,j\\nXi\\n(log ÀÜPi j‚àílog ÀÜQi j\\n)2\\n=\\n‚àë\\ni,j\\nXi\\n(wT\\ni Àúwj ‚àílog Xi j\\n)2 . (15)\\nFinally, we observe that while the weighting factor\\nXi is preordained by the on-line training method\\ninherent to the skip-gram and ivLBL models, it is\\nby no means guaranteed to be optimal. In fact,\\nMikolov et al. (2013a) observe that performance\\ncan be increased by Ô¨Åltering the data so as to re-\\nduce the effective value of the weighting factor for\\nfrequent words. With this in mind, we introduce\\na more general weighting function, which we are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='can be increased by Ô¨Åltering the data so as to re-\\nduce the effective value of the weighting factor for\\nfrequent words. With this in mind, we introduce\\na more general weighting function, which we are\\nfree to take to depend on the context word as well.\\nThe result is,\\nÀÜJ =\\n‚àë\\ni,j\\nf (Xi j)(wT\\ni Àúwj ‚àílog Xi j\\n)2 , (16)\\nwhich is equivalent 1 to the cost function of\\nEqn. (8), which we derived previously.\\n3.2 Complexity of the model\\nAs can be seen from Eqn. (8) and the explicit form\\nof the weighting function f (X), the computational\\ncomplexity of the model depends on the number of\\nnonzero elements in the matrix X. As this num-\\nber is always less than the total number of en-\\ntries of the matrix, the model scales no worse than\\nO(|V |2). At Ô¨Årst glance this might seem like a sub-\\nstantial improvement over the shallow window-\\nbased approaches, which scale with the corpus\\nsize, |C|. However, typical vocabularies have hun-\\ndreds of thousands of words, so that |V |2 can be in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='stantial improvement over the shallow window-\\nbased approaches, which scale with the corpus\\nsize, |C|. However, typical vocabularies have hun-\\ndreds of thousands of words, so that |V |2 can be in\\nthe hundreds of billions, which is actually much\\nlarger than most corpora. For this reason it is im-\\nportant to determine whether a tighter bound can\\nbe placed on the number of nonzero elements of\\nX.\\nIn order to make any concrete statements about\\nthe number of nonzero elements in X, it is neces-\\nsary to make some assumptions about the distribu-\\ntion of word co-occurrences. In particular, we will\\nassume that the number of co-occurrences of word\\ni with word j, Xi j, can be modeled as a power-law\\nfunction of the frequency rank of that word pair,\\nri j:\\nXi j = k\\n(ri j)Œ± . (17)\\n1We could also include bias terms in Eqn. (16).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='The total number of words in the corpus is pro-\\nportional to the sum over all elements of the co-\\noccurrence matrix X,\\n|C|‚àº\\n‚àë\\ni j\\nXi j =\\n|X |‚àë\\nr=1\\nk\\nrŒ± = kH |X |,Œ± , (18)\\nwhere we have rewritten the last sum in terms of\\nthe generalized harmonic number Hn,m. The up-\\nper limit of the sum, |X |, is the maximum fre-\\nquency rank, which coincides with the number of\\nnonzero elements in the matrix X. This number is\\nalso equal to the maximum value of r in Eqn. (17)\\nsuch that Xi j ‚â•1, i.e., |X |= k1/Œ±. Therefore we\\ncan write Eqn. (18) as,\\n|C|‚àº| X |Œ± H|X |,Œ± . (19)\\nWe are interested in how|X |is related to |C|when\\nboth numbers are large; therefore we are free to\\nexpand the right hand side of the equation for large\\n|X |. For this purpose we use the expansion of gen-\\neralized harmonic numbers (Apostol, 1976),\\nHx,s = x1‚àís\\n1 ‚àís + Œ∂(s) + O(x‚àís ) if s >0, s , 1 ,\\n(20)\\ngiving,\\n|C|‚àº |X |\\n1 ‚àíŒ± + Œ∂(Œ±) |X |Œ± + O(1) , (21)\\nwhere Œ∂(s) is the Riemann zeta function. In the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='eralized harmonic numbers (Apostol, 1976),\\nHx,s = x1‚àís\\n1 ‚àís + Œ∂(s) + O(x‚àís ) if s >0, s , 1 ,\\n(20)\\ngiving,\\n|C|‚àº |X |\\n1 ‚àíŒ± + Œ∂(Œ±) |X |Œ± + O(1) , (21)\\nwhere Œ∂(s) is the Riemann zeta function. In the\\nlimit that X is large, only one of the two terms on\\nthe right hand side of Eqn. (21) will be relevant,\\nand which term that is depends on whether Œ± >1,\\n|X |=\\n{ O(|C|) if Œ± <1,\\nO(|C|1/Œ±) if Œ± >1. (22)\\nFor the corpora studied in this article, we observe\\nthat Xi j is well-modeled by Eqn. (17) with Œ± =\\n1.25. In this case we have that |X |= O(|C|0.8).\\nTherefore we conclude that the complexity of the\\nmodel is much better than the worst case O(V2),\\nand in fact it does somewhat better than the on-line\\nwindow-based methods which scale like O(|C|).\\n4 Experiments\\n4.1 Evaluation methods\\nWe conduct experiments on the word analogy\\ntask of Mikolov et al. (2013a), a variety of word\\nsimilarity tasks, as described in (Luong et al.,\\n2013), and on the CoNLL-2003 shared benchmark'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='We conduct experiments on the word analogy\\ntask of Mikolov et al. (2013a), a variety of word\\nsimilarity tasks, as described in (Luong et al.,\\n2013), and on the CoNLL-2003 shared benchmark\\nTable 2: Results on the word analogy task, given\\nas percent accuracy. Underlined scores are best\\nwithin groups of similarly-sized models; bold\\nscores are best overall. HPCA vectors are publicly\\navailable2; (i)vLBL results are from (Mnih et al.,\\n2013); skip-gram (SG) and CBOW results are\\nfrom (Mikolov et al., 2013a,b); we trained SG ‚Ä†\\nand CBOW‚Ä†using the word2vec tool3. See text\\nfor details and a description of the SVD models.\\nModel Dim. Size Sem. Syn. Tot.\\nivLBL 100 1.5B 55.9 50.1 53.2\\nHPCA 100 1.6B 4.2 16.4 10.8\\nGloVe 100 1.6B 67.5 54.3 60.3\\nSG 300 1B 61 61 61\\nCBOW 300 1.6B 16.1 52.6 36.1\\nvLBL 300 1.5B 54.2 64.8 60.0\\nivLBL 300 1.5B 65.2 63.0 64.0\\nGloVe 300 1.6B 80.8 61.5 70.3\\nSVD 300 6B 6.3 8.1 7.3\\nSVD-S 300 6B 36.7 46.6 42.1\\nSVD-L 300 6B 56.6 63.0 60.1\\nCBOW‚Ä† 300 6B 63.6 67.4 65.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='vLBL 300 1.5B 54.2 64.8 60.0\\nivLBL 300 1.5B 65.2 63.0 64.0\\nGloVe 300 1.6B 80.8 61.5 70.3\\nSVD 300 6B 6.3 8.1 7.3\\nSVD-S 300 6B 36.7 46.6 42.1\\nSVD-L 300 6B 56.6 63.0 60.1\\nCBOW‚Ä† 300 6B 63.6 67.4 65.7\\nSG‚Ä† 300 6B 73.0 66.0 69.1\\nGloVe 300 6B 77.4 67.0 71.7\\nCBOW 1000 6B 57.3 68.9 63.7\\nSG 1000 6B 66.1 65.1 65.6\\nSVD-L 300 42B 38.4 58.2 49.2\\nGloVe 300 42B 81.9 69.3 75.0\\ndataset for NER (Tjong Kim Sang and De Meul-\\nder, 2003).\\nWord analogies.The word analogy task con-\\nsists of questions like, ‚Äú a is to b as c is to ?‚Äù\\nThe dataset contains 19,544 such questions, di-\\nvided into a semantic subset and a syntactic sub-\\nset. The semantic questions are typically analogies\\nabout people or places, like ‚ÄúAthens is to Greece\\nas Berlin is to ?‚Äù. The syntactic questions are\\ntypically analogies about verb tenses or forms of\\nadjectives, for example ‚Äúdance is to dancing as Ô¨Çy\\nis to ?‚Äù. To correctly answer the question, the\\nmodel should uniquely identify the missing term,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='typically analogies about verb tenses or forms of\\nadjectives, for example ‚Äúdance is to dancing as Ô¨Çy\\nis to ?‚Äù. To correctly answer the question, the\\nmodel should uniquely identify the missing term,\\nwith only an exact correspondence counted as a\\ncorrect match. We answer the question ‚Äú a is to b\\nas c is to ?‚Äù by Ô¨Ånding the word d whose repre-\\nsentation wd is closest to wb ‚àíwa + wc according\\nto the cosine similarity.4\\n2http://lebret.ch/words/\\n3http://code.google.com/p/word2vec/\\n4Levy et al. (2014) introduce a multiplicative analogy\\nevaluation, 3COSMUL, and report an accuracy of 68.24% on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='0 100 200 300 400 500 60020\\n30\\n40\\n50\\n60\\n70\\n80\\nVector Dimension\\nAccuracy [%]\\n \\nSemantic\\nSyntactic\\nOverall\\n(a) Symmetric context\\n2 4 6 8 1040\\n50\\n55\\n60\\n65\\n70\\n45\\nWindow Size\\nAccuracy [%]\\n \\n \\n \\nSemantic\\nSyntactic\\nOverall (b) Symmetric context\\n2 4 6 8 1040\\n50\\n55\\n60\\n65\\n70\\n45\\nWindow Size\\nAccuracy [%]\\n \\n \\n \\nSemantic\\nSyntactic\\nOverall (c) Asymmetric context\\nFigure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are\\ntrained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100.\\nWord similarity. While the analogy task is our\\nprimary focus since it tests for interesting vector\\nspace substructures, we also evaluate our model on\\na variety of word similarity tasks in Table 3. These\\ninclude WordSim-353 (Finkelstein et al., 2001),\\nMC (Miller and Charles, 1991), RG (Rubenstein\\nand Goodenough, 1965), SCWS (Huang et al.,\\n2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='MC (Miller and Charles, 1991), RG (Rubenstein\\nand Goodenough, 1965), SCWS (Huang et al.,\\n2012), and RW (Luong et al., 2013).\\nNamed entity recognition. The CoNLL-2003\\nEnglish benchmark dataset for NER is a collec-\\ntion of documents from Reuters newswire articles,\\nannotated with four entity types: person, location,\\norganization, and miscellaneous. We train mod-\\nels on CoNLL-03 training data on test on three\\ndatasets: 1) ConLL-03 testing data, 2) ACE Phase\\n2 (2001-02) and ACE-2003 data, and 3) MUC7\\nFormal Run test set. We adopt the BIO2 annota-\\ntion standard, as well as all the preprocessing steps\\ndescribed in (Wang and Manning, 2013). We use a\\ncomprehensive set of discrete features that comes\\nwith the standard distribution of the Stanford NER\\nmodel (Finkel et al., 2005). A total of 437,905\\ndiscrete features were generated for the CoNLL-\\n2003 training dataset. In addition, 50-dimensional\\nvectors for each word of a Ô¨Åve-word context are\\nadded and used as continuous features. With these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='discrete features were generated for the CoNLL-\\n2003 training dataset. In addition, 50-dimensional\\nvectors for each word of a Ô¨Åve-word context are\\nadded and used as continuous features. With these\\nfeatures as input, we trained a conditional random\\nÔ¨Åeld (CRF) with exactly the same setup as the\\nCRFjoin model of (Wang and Manning, 2013).\\n4.2 Corpora and training details\\nWe trained our model on Ô¨Åve corpora of varying\\nsizes: a 2010 Wikipedia dump with 1 billion to-\\nkens; a 2014 Wikipedia dump with 1.6 billion to-\\nkens; Gigaword 5 which has 4.3 billion tokens; the\\ncombination Gigaword5 + Wikipedia2014, which\\nthe analogy task. This number is evaluated on a subset of the\\ndataset so it is not included in Table 2. 3COSMUL performed\\nworse than cosine similarity in almost all of our experiments.\\nhas 6 billion tokens; and on 42 billion tokens of\\nweb data, from Common Crawl 5. We tokenize\\nand lowercase each corpus with the Stanford to-\\nkenizer, build a vocabulary of the 400,000 most'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='has 6 billion tokens; and on 42 billion tokens of\\nweb data, from Common Crawl 5. We tokenize\\nand lowercase each corpus with the Stanford to-\\nkenizer, build a vocabulary of the 400,000 most\\nfrequent words6, and then construct a matrix of co-\\noccurrence counts X. In constructing X, we must\\nchoose how large the context window should be\\nand whether to distinguish left context from right\\ncontext. We explore the effect of these choices be-\\nlow. In all cases we use a decreasing weighting\\nfunction, so that word pairs that are d words apart\\ncontribute 1/d to the total count. This is one way\\nto account for the fact that very distant word pairs\\nare expected to contain less relevant information\\nabout the words‚Äô relationship to one another.\\nFor all our experiments, we set xmax = 100,\\nŒ± = 3/4, and train the model using AdaGrad\\n(Duchi et al., 2011), stochastically sampling non-\\nzero elements from X, with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Œ± = 3/4, and train the model using AdaGrad\\n(Duchi et al., 2011), stochastically sampling non-\\nzero elements from X, with initial learning rate of\\n0.05. We run 50 iterations for vectors smaller than\\n300 dimensions, and 100 iterations otherwise (see\\nSection 4.6 for more details about the convergence\\nrate). Unless otherwise noted, we use a context of\\nten words to the left and ten words to the right.\\nThe model generates two sets of word vectors,\\nW and ÀúW. When X is symmetric, W and ÀúW are\\nequivalent and differ only as a result of their ran-\\ndom initializations; the two sets of vectors should\\nperform equivalently. On the other hand, there is\\nevidence that for certain types of neural networks,\\ntraining multiple instances of the network and then\\ncombining the results can help reduce overÔ¨Åtting\\nand noise and generally improve results (Ciresan\\net al., 2012). With this in mind, we choose to use\\n5To demonstrate the scalability of the model, we also'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and noise and generally improve results (Ciresan\\net al., 2012). With this in mind, we choose to use\\n5To demonstrate the scalability of the model, we also\\ntrained it on a much larger sixth corpus, containing 840 bil-\\nlion tokens of web data, but in this case we did not lowercase\\nthe vocabulary, so the results are not directly comparable.\\n6For the model trained on Common Crawl data, we use a\\nlarger vocabulary of about 2 million words.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='the sum W + ÀúW as our word vectors. Doing so typ-\\nically gives a small boost in performance, with the\\nbiggest increase in the semantic analogy task.\\nWe compare with the published results of a va-\\nriety of state-of-the-art models, as well as with\\nour own results produced using the word2vec\\ntool and with several baselines using SVDs. With\\nword2vec, we train the skip-gram (SG ‚Ä†) and\\ncontinuous bag-of-words (CBOW‚Ä†) models on the\\n6 billion token corpus (Wikipedia 2014 + Giga-\\nword 5) with a vocabulary of the top 400,000 most\\nfrequent words and a context window size of 10.\\nWe used 10 negative samples, which we show in\\nSection 4.6 to be a good choice for this corpus.\\nFor the SVD baselines, we generate a truncated\\nmatrix Xtrunc which retains the information of how\\nfrequently each word occurs with only the top\\n10,000 most frequent words. This step is typi-\\ncal of many matrix-factorization-based methods as\\nthe extra columns can contribute a disproportion-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='frequently each word occurs with only the top\\n10,000 most frequent words. This step is typi-\\ncal of many matrix-factorization-based methods as\\nthe extra columns can contribute a disproportion-\\nate number of zero entries and the methods are\\notherwise computationally expensive.\\nThe singular vectors of this matrix constitute\\nthe baseline ‚ÄúSVD‚Äù. We also evaluate two related\\nbaselines: ‚ÄúSVD-S‚Äù in which we take the SVD of‚àöXtrunc, and ‚ÄúSVD-L‚Äù in which we take the SVD\\nof log(1+ Xtrunc). Both methods help compress the\\notherwise large range of values in X.7\\n4.3 Results\\nWe present results on the word analogy task in Ta-\\nble 2. The GloVe model performs signiÔ¨Åcantly\\nbetter than the other baselines, often with smaller\\nvector sizes and smaller corpora. Our results us-\\ning the word2vec tool are somewhat better than\\nmost of the previously published results. This is\\ndue to a number of factors, including our choice to\\nuse negative sampling (which typically works bet-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ing the word2vec tool are somewhat better than\\nmost of the previously published results. This is\\ndue to a number of factors, including our choice to\\nuse negative sampling (which typically works bet-\\nter than the hierarchical softmax), the number of\\nnegative samples, and the choice of the corpus.\\nWe demonstrate that the model can easily be\\ntrained on a large 42 billion token corpus, with a\\nsubstantial corresponding performance boost. We\\nnote that increasing the corpus size does not guar-\\nantee improved results for other models, as can be\\nseen by the decreased performance of the SVD-\\n7We also investigated several other weighting schemes for\\ntransforming X; what we report here performed best. Many\\nweighting schemes like PPMI destroy the sparsity of X and\\ntherefore cannot feasibly be used with large vocabularies.\\nWith smaller vocabularies, these information-theoretic trans-\\nformations do indeed work well on word similarity measures,\\nbut they perform very poorly on the word analogy task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='With smaller vocabularies, these information-theoretic trans-\\nformations do indeed work well on word similarity measures,\\nbut they perform very poorly on the word analogy task.\\nTable 3: Spearman rank correlation on word simi-\\nlarity tasks. All vectors are 300-dimensional. The\\nCBOW‚àóvectors are from the word2vec website\\nand differ in that they contain phrase vectors.\\nModel Size WS353 MC RG SCWS RW\\nSVD 6B 35.3 35.1 42.5 38.3 25.6\\nSVD-S 6B 56.5 71.5 71.0 53.6 34.7\\nSVD-L 6B 65.7 72.7 75.1 56.5 37.0\\nCBOW‚Ä† 6B 57.2 65.6 68.2 57.0 32.5\\nSG‚Ä† 6B 62.8 65.2 69.7 58.1 37.2\\nGloVe 6B 65.8 72.7 77.8 53.9 38.1\\nSVD-L 42B 74.0 76.4 74.1 58.3 39.9\\nGloVe 42B 75.9 83.6 82.9 59.6 47.8\\nCBOW‚àó 100B 68.4 79.6 75.4 59.4 45.5\\nL model on this larger corpus. The fact that this\\nbasic SVD model does not scale well to large cor-\\npora lends further evidence to the necessity of the\\ntype of weighting scheme proposed in our model.\\nTable 3 shows results on Ô¨Åve different word'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='basic SVD model does not scale well to large cor-\\npora lends further evidence to the necessity of the\\ntype of weighting scheme proposed in our model.\\nTable 3 shows results on Ô¨Åve different word\\nsimilarity datasets. A similarity score is obtained\\nfrom the word vectors by Ô¨Årst normalizing each\\nfeature across the vocabulary and then calculat-\\ning the cosine similarity. We compute Spearman‚Äôs\\nrank correlation coefÔ¨Åcient between this score and\\nthe human judgments. CBOW ‚àó denotes the vec-\\ntors available on the word2vec website that are\\ntrained with word and phrase vectors on 100B\\nwords of news data. GloVe outperforms it while\\nusing a corpus less than half the size.\\nTable 4 shows results on the NER task with the\\nCRF-based model. The L-BFGS training termi-\\nnates when no improvement has been achieved on\\nthe dev set for 25 iterations. Otherwise all conÔ¨Åg-\\nurations are identical to those used by Wang and\\nManning (2013). The model labeled Discrete is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='nates when no improvement has been achieved on\\nthe dev set for 25 iterations. Otherwise all conÔ¨Åg-\\nurations are identical to those used by Wang and\\nManning (2013). The model labeled Discrete is\\nthe baseline using a comprehensive set of discrete\\nfeatures that comes with the standard distribution\\nof the Stanford NER model, but with no word vec-\\ntor features. In addition to the HPCA and SVD\\nmodels discussed previously, we also compare to\\nthe models of Huang et al. (2012) (HSMN) and\\nCollobert and Weston (2008) (CW). We trained\\nthe CBOW model using the word2vec tool8.\\nThe GloVe model outperforms all other methods\\non all evaluation metrics, except for the CoNLL\\ntest set, on which the HPCA method does slightly\\nbetter. We conclude that the GloVe vectors are\\nuseful in downstream NLP tasks, as was Ô¨Årst\\n8We use the same parameters as above, except in this case\\nwe found 5 negative samples to work slightly better than 10.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Table 4: F1 score on NER task with 50d vectors.\\nDiscrete is the baseline without word vectors. We\\nuse publicly-available vectors for HPCA, HSMN,\\nand CW. See text for details.\\nModel Dev Test ACE MUC7\\nDiscrete 91.0 85.4 77.4 73.4\\nSVD 90.8 85.7 77.3 73.7\\nSVD-S 91.0 85.5 77.6 74.3\\nSVD-L 90.5 84.8 73.6 71.5\\nHPCA 92.6 88.7 81.7 80.7\\nHSMN 90.5 85.7 78.7 74.7\\nCW 92.2 87.4 81.7 80.2\\nCBOW 93.1 88.2 82.2 81.1\\nGloVe 93.2 88.3 82.9 82.2\\nshown for neural vectors in (Turian et al., 2010).\\n4.4 Model Analysis: Vector Length and\\nContext Size\\nIn Fig. 2, we show the results of experiments that\\nvary vector length and context window. A context\\nwindow that extends to the left and right of a tar-\\nget word will be called symmetric, and one which\\nextends only to the left will be called asymmet-\\nric. In (a), we observe diminishing returns for vec-\\ntors larger than about 200 dimensions. In (b) and\\n(c), we examine the effect of varying the window\\nsize for symmetric and asymmetric context win-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ric. In (a), we observe diminishing returns for vec-\\ntors larger than about 200 dimensions. In (b) and\\n(c), we examine the effect of varying the window\\nsize for symmetric and asymmetric context win-\\ndows. Performance is better on the syntactic sub-\\ntask for small and asymmetric context windows,\\nwhich aligns with the intuition that syntactic infor-\\nmation is mostly drawn from the immediate con-\\ntext and can depend strongly on word order. Se-\\nmantic information, on the other hand, is more fre-\\nquently non-local, and more of it is captured with\\nlarger window sizes.\\n4.5 Model Analysis: Corpus Size\\nIn Fig. 3, we show performance on the word anal-\\nogy task for 300-dimensional vectors trained on\\ndifferent corpora. On the syntactic subtask, there\\nis a monotonic increase in performance as the cor-\\npus size increases. This is to be expected since\\nlarger corpora typically produce better statistics.\\nInterestingly, the same trend is not true for the se-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='pus size increases. This is to be expected since\\nlarger corpora typically produce better statistics.\\nInterestingly, the same trend is not true for the se-\\nmantic subtask, where the models trained on the\\nsmaller Wikipedia corpora do better than those\\ntrained on the larger Gigaword corpus. This is\\nlikely due to the large number of city- and country-\\nbased analogies in the analogy dataset and the fact\\nthat Wikipedia has fairly comprehensive articles\\nfor most such locations. Moreover, Wikipedia‚Äôs\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\nOverallSyntacticSemantic\\nWiki2010\\n1B tokens\\nAccuracy [%]\\nWiki2014\\n1.6B tokens\\nGigaword5\\n4.3B tokens\\nGigaword5 + \\nWiki2014\\n6B tokens\\nCommon Crawl \\n42B tokens\\nFigure 3: Accuracy on the analogy task for 300-\\ndimensional vectors trained on different corpora.\\nentries are updated to assimilate new knowledge,\\nwhereas Gigaword is a Ô¨Åxed news repository with\\noutdated and possibly incorrect information.\\n4.6 Model Analysis: Run-time\\nThe total run-time is split between populating X'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='whereas Gigaword is a Ô¨Åxed news repository with\\noutdated and possibly incorrect information.\\n4.6 Model Analysis: Run-time\\nThe total run-time is split between populating X\\nand training the model. The former depends on\\nmany factors, including window size, vocabulary\\nsize, and corpus size. Though we did not do so,\\nthis step could easily be parallelized across mul-\\ntiple machines (see, e.g., Lebret and Collobert\\n(2014) for some benchmarks). Using a single\\nthread of a dual 2.1GHz Intel Xeon E5-2658 ma-\\nchine, populating X with a 10 word symmetric\\ncontext window, a 400,000 word vocabulary, and\\na 6 billion token corpus takes about 85 minutes.\\nGiven X, the time it takes to train the model de-\\npends on the vector size and the number of itera-\\ntions. For 300-dimensional vectors with the above\\nsettings (and using all 32 cores of the above ma-\\nchine), a single iteration takes 14 minutes. See\\nFig. 4 for a plot of the learning curve.\\n4.7 Model Analysis: Comparison with\\nword2vec'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='settings (and using all 32 cores of the above ma-\\nchine), a single iteration takes 14 minutes. See\\nFig. 4 for a plot of the learning curve.\\n4.7 Model Analysis: Comparison with\\nword2vec\\nA rigorous quantitative comparison of GloVe with\\nword2vec is complicated by the existence of\\nmany parameters that have a strong effect on per-\\nformance. We control for the main sources of vari-\\nation that we identiÔ¨Åed in Sections 4.4 and 4.5 by\\nsetting the vector length, context window size, cor-\\npus, and vocabulary size to the conÔ¨Åguration men-\\ntioned in the previous subsection.\\nThe most important remaining variable to con-\\ntrol for is training time. For GloVe, the rele-\\nvant parameter is the number of training iterations.\\nFor word2vec, the obvious choice would be the\\nnumber of training epochs. Unfortunately, the\\ncode is currently designed for only a single epoch:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='1 2 3 4 5 6\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n5 10 15 20 25\\n1357 10 15 20 25 30 40 50\\nAccuracy [%]\\nIterations (GloVe)\\nNegative Samples (CBOW)\\nTraining Time (hrs)\\n \\nGloVe\\nCBOW\\n(a) GloVe vs CBOW\\n3 6 9 12 15 18 21 24\\n60\\n62\\n64\\n66\\n68\\n70\\n72\\n20 40 60 80 100\\n1 2 3 4 5 6 7 10 12 15 20\\nGloVe\\nSkip-Gram\\nAccuracy [%]\\nIterations (GloVe)\\nNegative Samples (Skip-Gram)\\nTraining Time (hrs) (b) GloVe vs Skip-Gram\\nFigure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by\\nthe number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram\\n(b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 +\\nGigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10.\\nit speciÔ¨Åes a learning schedule speciÔ¨Åc to a single\\npass through the data, making a modiÔ¨Åcation for\\nmultiple passes a non-trivial task. Another choice\\nis to vary the number of negative samples. Adding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='pass through the data, making a modiÔ¨Åcation for\\nmultiple passes a non-trivial task. Another choice\\nis to vary the number of negative samples. Adding\\nnegative samples effectively increases the number\\nof training words seen by the model, so in some\\nways it is analogous to extra epochs.\\nWe set any unspeciÔ¨Åed parameters to their de-\\nfault values, assuming that they are close to opti-\\nmal, though we acknowledge that this simpliÔ¨Åca-\\ntion should be relaxed in a more thorough analysis.\\nIn Fig. 4, we plot the overall performance on\\nthe analogy task as a function of training time.\\nThe two x-axes at the bottom indicate the corre-\\nsponding number of training iterations for GloVe\\nand negative samples for word2vec. We note\\nthat word2vec‚Äôs performance actually decreases\\nif the number of negative samples increases be-\\nyond about 10. Presumably this is because the\\nnegative sampling method does not approximate\\nthe target probability distribution well.9\\nFor the same corpus, vocabulary, window size,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='yond about 10. Presumably this is because the\\nnegative sampling method does not approximate\\nthe target probability distribution well.9\\nFor the same corpus, vocabulary, window size,\\nand training time, GloVe consistently outperforms\\nword2vec. It achieves better results faster, and\\nalso obtains the best results irrespective of speed.\\n5 Conclusion\\nRecently, considerable attention has been focused\\non the question of whether distributional word\\nrepresentations are best learned from count-based\\n9In contrast, noise-contrastive estimation is an approxi-\\nmation which improves with more negative samples. In Ta-\\nble 1 of (Mnih et al., 2013), accuracy on the analogy task is a\\nnon-decreasing function of the number of negative samples.\\nmethods or from prediction-based methods. Cur-\\nrently, prediction-based models garner substantial\\nsupport; for example, Baroni et al. (2014) argue\\nthat these models perform better across a range of\\ntasks. In this work we argue that the two classes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='support; for example, Baroni et al. (2014) argue\\nthat these models perform better across a range of\\ntasks. In this work we argue that the two classes\\nof methods are not dramatically different at a fun-\\ndamental level since they both probe the under-\\nlying co-occurrence statistics of the corpus, but\\nthe efÔ¨Åciency with which the count-based meth-\\nods capture global statistics can be advantageous.\\nWe construct a model that utilizes this main ben-\\neÔ¨Åt of count data while simultaneously capturing\\nthe meaningful linear substructures prevalent in\\nrecent log-bilinear prediction-based methods like\\nword2vec. The result, GloVe, is a new global\\nlog-bilinear regression model for the unsupervised\\nlearning of word representations that outperforms\\nother models on word analogy, word similarity,\\nand named entity recognition tasks.\\nAcknowledgments\\nWe thank the anonymous reviewers for their valu-\\nable comments. Stanford University gratefully\\nacknowledges the support of the Defense Threat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='and named entity recognition tasks.\\nAcknowledgments\\nWe thank the anonymous reviewers for their valu-\\nable comments. Stanford University gratefully\\nacknowledges the support of the Defense Threat\\nReduction Agency (DTRA) under Air Force Re-\\nsearch Laboratory (AFRL) contract no. FA8650-\\n10-C-7020 and the Defense Advanced Research\\nProjects Agency (DARPA) Deep Exploration and\\nFiltering of Text (DEFT) Program under AFRL\\ncontract no. FA8750-13-2-0040. Any opinions,\\nÔ¨Åndings, and conclusion or recommendations ex-\\npressed in this material are those of the authors and\\ndo not necessarily reÔ¨Çect the view of the DTRA,\\nAFRL, DEFT, or the US government.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='References\\nTom M. Apostol. 1976. Introduction to Analytic\\nNumber Theory. Introduction to Analytic Num-\\nber Theory.\\nMarco Baroni, Georgiana Dinu, and Germ ¬¥an\\nKruszewski. 2014. Don‚Äôt count, predict! A\\nsystematic comparison of context-counting vs.\\ncontext-predicting semantic vectors. In ACL.\\nYoshua Bengio. 2009. Learning deep architectures\\nfor AI. Foundations and Trends in Machine\\nLearning.\\nYoshua Bengio, R ¬¥ejean Ducharme, Pascal Vin-\\ncent, and Christian Janvin. 2003. A neural prob-\\nabilistic language model. JMLR, 3:1137‚Äì1155.\\nJohn A. Bullinaria and Joseph P. Levy. 2007. Ex-\\ntracting semantic representations from word co-\\noccurrence statistics: A computational study.\\nBehavior Research Methods, 39(3):510‚Äì526.\\nDan C. Ciresan, Alessandro Giusti, Luca M. Gam-\\nbardella, and J ¬®urgen Schmidhuber. 2012. Deep\\nneural networks segment neuronal membranes\\nin electron microscopy images. In NIPS, pages\\n2852‚Äì2860.\\nRonan Collobert and Jason Weston. 2008. A uni-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='bardella, and J ¬®urgen Schmidhuber. 2012. Deep\\nneural networks segment neuronal membranes\\nin electron microscopy images. In NIPS, pages\\n2852‚Äì2860.\\nRonan Collobert and Jason Weston. 2008. A uni-\\nÔ¨Åed architecture for natural language process-\\ning: deep neural networks with multitask learn-\\ning. In Proceedings of ICML, pages 160‚Äì167.\\nRonan Collobert, Jason Weston, L ¬¥eon Bottou,\\nMichael Karlen, Koray Kavukcuoglu, and Pavel\\nKuksa. 2011. Natural Language Processing (Al-\\nmost) from Scratch. JMLR, 12:2493‚Äì2537.\\nScott Deerwester, Susan T. Dumais, George W.\\nFurnas, Thomas K. Landauer, and Richard\\nHarshman. 1990. Indexing by latent semantic\\nanalysis. Journal of the American Society for\\nInformation Science, 41.\\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\\nAdaptive subgradient methods for online learn-\\ning and stochastic optimization. JMLR, 12.\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma-\\ntias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. 2001. Placing search in con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='ing and stochastic optimization. JMLR, 12.\\nLev Finkelstein, Evgenly Gabrilovich, Yossi Ma-\\ntias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. 2001. Placing search in con-\\ntext: The concept revisited. In Proceedings\\nof the 10th international conference on World\\nWide Web, pages 406‚Äì414. ACM.\\nEric H. Huang, Richard Socher, Christopher D.\\nManning, and Andrew Y . Ng. 2012. Improving\\nWord Representations via Global Context and\\nMultiple Word Prototypes. In ACL.\\nR¬¥emi Lebret and Ronan Collobert. 2014. Word\\nembeddings through Hellinger PCA. In EACL.\\nOmer Levy, Yoav Goldberg, and Israel Ramat-\\nGan. 2014. Linguistic regularities in sparse and\\nexplicit word representations. CoNLL-2014.\\nKevin Lund and Curt Burgess. 1996. Producing\\nhigh-dimensional semantic spaces from lexical\\nco-occurrence. Behavior Research Methods, In-\\nstrumentation, and Computers, 28:203‚Äì208.\\nMinh-Thang Luong, Richard Socher, and Christo-\\npher D Manning. 2013. Better word represen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='co-occurrence. Behavior Research Methods, In-\\nstrumentation, and Computers, 28:203‚Äì208.\\nMinh-Thang Luong, Richard Socher, and Christo-\\npher D Manning. 2013. Better word represen-\\ntations with recursive neural networks for mor-\\nphology. CoNLL-2013.\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\\nfrey Dean. 2013a. EfÔ¨Åcient Estimation of Word\\nRepresentations in Vector Space. InICLR Work-\\nshop Papers.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg\\nCorrado, and Jeffrey Dean. 2013b. Distributed\\nrepresentations of words and phrases and their\\ncompositionality. In NIPS, pages 3111‚Äì3119.\\nTomas Mikolov, Wen tau Yih, and Geoffrey\\nZweig. 2013c. Linguistic regularities in con-\\ntinuous space word representations. In HLT-\\nNAACL.\\nGeorge A. Miller and Walter G. Charles. 1991.\\nContextual correlates of semantic similarity.\\nLanguage and cognitive processes, 6(1):1‚Äì28.\\nAndriy Mnih and Koray Kavukcuoglu. 2013.\\nLearning word embeddings efÔ¨Åciently with\\nnoise-contrastive estimation. In NIPS.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Language and cognitive processes, 6(1):1‚Äì28.\\nAndriy Mnih and Koray Kavukcuoglu. 2013.\\nLearning word embeddings efÔ¨Åciently with\\nnoise-contrastive estimation. In NIPS.\\nDouglas L. T. Rohde, Laura M. Gonnerman,\\nand David C. Plaut. 2006. An improved\\nmodel of semantic similarity based on lexical\\nco-occurence. Communications of the ACM,\\n8:627‚Äì633.\\nHerbert Rubenstein and John B. Goodenough.\\n1965. Contextual correlates of synonymy.Com-\\nmunications of the ACM, 8(10):627‚Äì633.\\nFabrizio Sebastiani. 2002. Machine learning in au-\\ntomated text categorization. ACM Computing\\nSurveys, 34:1‚Äì47.\\nRichard Socher, John Bauer, Christopher D. Man-\\nning, and Andrew Y . Ng. 2013. Parsing With\\nCompositional Vector Grammars. In ACL.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'TeX', 'creationdate': '2014-09-03T09:22:54-07:00', 'moddate': '2014-09-03T09:22:54-07:00', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.5-1.40.14 (TeX Live 2013) kpathsea version 6.1.1', 'source': '../data/pdf_files/glove.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'glove.pdf', 'file_type': 'pdf'}, page_content='Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron\\nFernandes, and Gregory Marton. 2003. Quanti-\\ntative evaluation of passage retrieval algorithms\\nfor question answering. In Proceedings of the\\nSIGIR Conference on Research and Develop-\\nment in Informaion Retrieval.\\nErik F. Tjong Kim Sang and Fien De Meul-\\nder. 2003. Introduction to the CoNLL-2003\\nshared task: Language-independent named en-\\ntity recognition. In CoNLL-2003.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio.\\n2010. Word representations: a simple and gen-\\neral method for semi-supervised learning. In\\nProceedings of ACL, pages 384‚Äì394.\\nMengqiu Wang and Christopher D. Manning.\\n2013. Effect of non-linear deep architecture in\\nsequence labeling. In Proceedings of the 6th\\nInternational Joint Conference on Natural Lan-\\nguage Processing (IJCNLP).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model can be Ô¨Åne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciÔ¨Åc architecture modiÔ¨Åcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='language inference, without substantial task-\\nspeciÔ¨Åc architecture modiÔ¨Åcations.\\nBERT is conceptually simple and empirically\\npowerful. It obtains new state-of-the-art re-\\nsults on eleven natural language processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1 Introduction\\nLanguage model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='natural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce Ô¨Åne-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for apply-\\ning pre-trained language representations to down-\\nstream tasks: feature-based and Ô¨Åne-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-speciÔ¨Åc architectures that\\ninclude the pre-trained representations as addi-\\ntional features. The Ô¨Åne-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciÔ¨Åc parameters, and is trained on the\\ndownstream tasks by simply Ô¨Åne-tuning all pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='the Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-speciÔ¨Åc parameters, and is trained on the\\ndownstream tasks by simply Ô¨Åne-tuning all pre-\\ntrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, espe-\\ncially for the Ô¨Åne-tuning approaches. The ma-\\njor limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying Ô¨Åne-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='of the Transformer (Vaswani et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying Ô¨Åne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incor-\\nporate context from both directions.\\nIn this paper, we improve the Ô¨Åne-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidi-\\nrectionality constraint by using a ‚Äúmasked lan-\\nguage model‚Äù (MLM) pre-training objective, in-\\nspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the original vocabulary id of the masked\\narXiv:1810.04805v2  [cs.CL]  24 May 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='word based only on its context. Unlike left-to-\\nright language model pre-training, the MLM ob-\\njective enables the representation to fuse the left\\nand the right context, which allows us to pre-\\ntrain a deep bidirectional Transformer. In addi-\\ntion to the masked language model, we also use\\na ‚Äúnext sentence prediction‚Äù task that jointly pre-\\ntrains text-pair representations. The contributions\\nof our paper are as follows:\\n‚Ä¢ We demonstrate the importance of bidirectional\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of independently\\ntrained left-to-right and right-to-left LMs.\\n‚Ä¢ We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='trained left-to-right and right-to-left LMs.\\n‚Ä¢ We show that pre-trained representations reduce\\nthe need for many heavily-engineered task-\\nspeciÔ¨Åc architectures. BERT is the Ô¨Årst Ô¨Åne-\\ntuning based representation model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, outper-\\nforming many task-speciÔ¨Åc architectures.\\n‚Ä¢ BERT advances the state of the art for eleven\\nNLP tasks. The code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2 Related Work\\nThere is a long history of pre-training general lan-\\nguage representations, and we brieÔ¨Çy review the\\nmost widely-used approaches in this section.\\n2.1 Unsupervised Feature-based Approaches\\nLearning widely applicable representations of\\nwords has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='words has been an active area of research for\\ndecades, including non-neural (Brown et al., 1992;\\nAndo and Zhang, 2005; Blitzer et al., 2006) and\\nneural (Mikolov et al., 2013; Pennington et al.,\\n2014) methods. Pre-trained word embeddings\\nare an integral part of modern NLP systems, of-\\nfering signiÔ¨Åcant improvements over embeddings\\nlearned from scratch (Turian et al., 2010). To pre-\\ntrain word embedding vectors, left-to-right lan-\\nguage modeling objectives have been used (Mnih\\nand Hinton, 2009), as well as objectives to dis-\\ncriminate correct from incorrect words in left and\\nright context (Mikolov et al., 2013).\\nThese approaches have been generalized to\\ncoarser granularities, such as sentence embed-\\ndings (Kiros et al., 2015; Logeswaran and Lee,\\n2018) or paragraph embeddings (Le and Mikolov,\\n2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='2014). To train sentence representations, prior\\nwork has used objectives to rank candidate next\\nsentences (Jernite et al., 2017; Logeswaran and\\nLee, 2018), left-to-right generation of next sen-\\ntence words given a representation of the previous\\nsentence (Kiros et al., 2015), or denoising auto-\\nencoder derived objectives (Hill et al., 2016).\\nELMo and its predecessor (Peters et al., 2017,\\n2018a) generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciÔ¨Åc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='benchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the Ô¨Årst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nÔ¨Åne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='which produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nÔ¨Åne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT BERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nQuestion Paragraph\\nStart/End Span\\nBERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok 1  [SEP]... Tok N Tok 1 ... TokM\\nMasked Sentence A Masked Sentence B\\nPre-training Fine-Tuning\\nNSP Mask LM Mask LM\\nUnlabeled Sentence A and B Pair \\nSQuAD\\nQuestion Answer Pair\\nNERMNLI\\nFigure 1: Overall pre-training and Ô¨Åne-tuning procedures for BERT. Apart from output layers, the same architec-\\ntures are used in both pre-training and Ô¨Åne-tuning. The same pre-trained model parameters are used to initialize\\nmodels for different down-stream tasks. During Ô¨Åne-tuning, all parameters are Ô¨Åne-tuned. [CLS] is a special\\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\\ntions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tions/answers).\\ning and auto-encoder objectives have been used\\nfor pre-training such models (Howard and Ruder,\\n2018; Radford et al., 2018; Dai and Le, 2015).\\n2.3 Transfer Learning from Supervised Data\\nThere has also been work showing effective trans-\\nfer from supervised tasks with large datasets, such\\nas natural language inference (Conneau et al.,\\n2017) and machine translation (McCann et al.,\\n2017). Computer vision research has also demon-\\nstrated the importance of transfer learning from\\nlarge pre-trained models, where an effective recipe\\nis to Ô¨Åne-tune models pre-trained with Ima-\\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\\n3 BERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are two steps in our\\nframework: pre-training and Ô¨Åne-tuning. Dur-\\ning pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT model is Ô¨Årst initialized with\\nthe pre-trained parameters, and all of the param-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT model is Ô¨Årst initialized with\\nthe pre-trained parameters, and all of the param-\\neters are Ô¨Åne-tuned using labeled data from the\\ndownstream tasks. Each downstream task has sep-\\narate Ô¨Åne-tuned models, even though they are ini-\\ntialized with the same pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this section.\\nA distinctive feature of BERT is its uniÔ¨Åed ar-\\nchitecture across different tasks. There is mini-\\nmal difference between the pre-trained architec-\\nture and the Ô¨Ånal downstream architecture.\\nModel Architecture BERT‚Äôs model architec-\\nture is a multi-layer bidirectional Transformer en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='coder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as ‚ÄúThe Annotated Transformer.‚Äù2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='size as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer uses\\nbidirectional self-attention, while the GPT Trans-\\nformer uses constrained self-attention where every\\ntoken can only attend to context to its left.4\\n1https://github.com/tensorÔ¨Çow/tensor2tensor\\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\\n3In all cases we set the feed-forward/Ô¨Ålter size to be 4H,\\ni.e., 3072 for the H = 768and 4096 for the H = 1024.\\n4We note that in the literature the bidirectional Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Input/Output Representations To make BERT\\nhandle a variety of down-stream tasks, our input\\nrepresentation is able to unambiguously represent\\nboth a single sentence and a pair of sentences\\n(e.g., ‚ü®Question, Answer ‚ü©) in one token sequence.\\nThroughout this work, a ‚Äúsentence‚Äù can be an arbi-\\ntrary span of contiguous text, rather than an actual\\nlinguistic sentence. A ‚Äúsequence‚Äù refers to the in-\\nput token sequence to BERT, which may be a sin-\\ngle sentence or two sentences packed together.\\nWe use WordPiece embeddings (Wu et al.,\\n2016) with a 30,000 token vocabulary. The Ô¨Årst\\ntoken of every sequence is always a special clas-\\nsiÔ¨Åcation token ( [CLS]). The Ô¨Ånal hidden state\\ncorresponding to this token is used as the ag-\\ngregate sequence representation for classiÔ¨Åcation\\ntasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tasks. Sentence pairs are packed together into a\\nsingle sequence. We differentiate the sentences in\\ntwo ways. First, we separate them with a special\\ntoken ([SEP]). Second, we add a learned embed-\\nding to every token indicating whether it belongs\\nto sentence A or sentence B. As shown in Figure 1,\\nwe denote input embedding as E, the Ô¨Ånal hidden\\nvector of the special [CLS] token as C ‚ààRH,\\nand the Ô¨Ånal hidden vector for the ith input token\\nas Ti ‚ààRH.\\nFor a given token, its input representation is\\nconstructed by summing the corresponding token,\\nsegment, and position embeddings. A visualiza-\\ntion of this construction can be seen in Figure 2.\\n3.1 Pre-training BERT\\nUnlike Peters et al. (2018a) and Radford et al.\\n(2018), we do not use traditional left-to-right or\\nright-to-left language models to pre-train BERT.\\nInstead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Instead, we pre-train BERT using two unsuper-\\nvised tasks, described in this section. This step\\nis presented in the left part of Figure 1.\\nTask #1: Masked LM Intuitively, it is reason-\\nable to believe that a deep bidirectional model is\\nstrictly more powerful than either a left-to-right\\nmodel or the shallow concatenation of a left-to-\\nright and a right-to-left model. Unfortunately,\\nstandard conditional language models can only be\\ntrained left-to-right or right-to-left, since bidirec-\\ntional conditioning would allow each word to in-\\ndirectly ‚Äúsee itself‚Äù, and the model could trivially\\npredict the target word in a multi-layered context.\\nformer is often referred to as a ‚ÄúTransformer encoder‚Äù while\\nthe left-context-only version is referred to as a ‚ÄúTransformer\\ndecoder‚Äù since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='decoder‚Äù since it can be used for text generation.\\nIn order to train a deep bidirectional representa-\\ntion, we simply mask some percentage of the input\\ntokens at random, and then predict those masked\\ntokens. We refer to this procedure as a ‚Äúmasked\\nLM‚Äù (MLM), although it is often referred to as a\\nCloze task in the literature (Taylor, 1953). In this\\ncase, the Ô¨Ånal hidden vectors corresponding to the\\nmask tokens are fed into an output softmax over\\nthe vocabulary, as in a standard LM. In all of our\\nexperiments, we mask 15% of all WordPiece to-\\nkens in each sequence at random. In contrast to\\ndenoising auto-encoders (Vincent et al., 2008), we\\nonly predict the masked words rather than recon-\\nstructing the entire input.\\nAlthough this allows us to obtain a bidirec-\\ntional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nÔ¨Åne-tuning, since the [MASK] token does not ap-\\npear during Ô¨Åne-tuning. To mitigate this, we do'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tional pre-trained model, a downside is that we\\nare creating a mismatch between pre-training and\\nÔ¨Åne-tuning, since the [MASK] token does not ap-\\npear during Ô¨Åne-tuning. To mitigate this, we do\\nnot always replace ‚Äúmasked‚Äù words with the ac-\\ntual [MASK] token. The training data generator\\nchooses 15% of the token positions at random for\\nprediction. If the i-th token is chosen, we replace\\nthe i-th token with (1) the [MASK] token 80% of\\nthe time (2) a random token 10% of the time (3)\\nthe unchanged i-th token 10% of the time. Then,\\nTi will be used to predict the original token with\\ncross entropy loss. We compare variations of this\\nprocedure in Appendix C.2.\\nTask #2: Next Sentence Prediction (NSP)\\nMany important downstream tasks such as Ques-\\ntion Answering (QA) and Natural Language Infer-\\nence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='ence (NLI) are based on understanding the rela-\\ntionship between two sentences, which is not di-\\nrectly captured by language modeling. In order\\nto train a model that understands sentence rela-\\ntionships, we pre-train for a binarized next sen-\\ntence prediction task that can be trivially gener-\\nated from any monolingual corpus. SpeciÔ¨Åcally,\\nwhen choosing the sentencesA and B for each pre-\\ntraining example, 50% of the time B is the actual\\nnext sentence that follows A (labeled as IsNext),\\nand 50% of the time it is a random sentence from\\nthe corpus (labeled as NotNext). As we show\\nin Figure 1, C is used for next sentence predic-\\ntion (NSP). 5 Despite its simplicity, we demon-\\nstrate in Section 5.1 that pre-training towards this\\ntask is very beneÔ¨Åcial to both QA and NLI. 6\\n5The Ô¨Ånal model achieves 97%-98% accuracy on NSP.\\n6The vector C is not a meaningful sentence representation\\nwithout Ô¨Åne-tuning, since it was trained with NSP.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='[CLS] he likes play ##ing [SEP]my dog is cute [SEP]Input\\nE[CLS] Ehe Elikes Eplay E##ing E[SEP]Emy Edog Eis Ecute E[SEP]\\nToken\\nEmbeddings\\nEA EB EB EB EB EBEA EA EA EA EASegment\\nEmbeddings\\nE0 E6 E7 E8 E9 E10E1 E2 E3 E4 E5Position\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee (2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='model pre-training. For the pre-training corpus we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we extract only the text passages\\nand ignore lists, tables, and headers. It is criti-\\ncal to use a document-level corpus rather than a\\nshufÔ¨Çed sentence-level corpus such as the Billion\\nWord Benchmark (Chelba et al., 2013) in order to\\nextract long contiguous sequences.\\n3.2 Fine-tuning BERT\\nFine-tuning is straightforward since the self-\\nattention mechanism in the Transformer al-\\nlows BERT to model many downstream tasks‚Äî\\nwhether they involve single text or text pairs‚Äîby\\nswapping out the appropriate inputs and outputs.\\nFor applications involving text pairs, a common\\npattern is to independently encode text pairs be-\\nfore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='fore applying bidirectional cross attention, such\\nas Parikh et al. (2016); Seo et al. (2017). BERT\\ninstead uses the self-attention mechanism to unify\\nthese two stages, as encoding a concatenated text\\npair with self-attention effectively includes bidi-\\nrectional cross attention between two sentences.\\nFor each task, we simply plug in the task-\\nspeciÔ¨Åc inputs and outputs into BERT and Ô¨Åne-\\ntune all the parameters end-to-end. At the in-\\nput, sentence A and sentence B from pre-training\\nare analogous to (1) sentence pairs in paraphras-\\ning, (2) hypothesis-premise pairs in entailment, (3)\\nquestion-passage pairs in question answering, and\\n(4) a degenerate text- ‚àÖ pair in text classiÔ¨Åcation\\nor sequence tagging. At the output, the token rep-\\nresentations are fed into an output layer for token-\\nlevel tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiÔ¨Åcation, such as en-\\ntailment or sentiment analysis.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='level tasks, such as sequence tagging or question\\nanswering, and the [CLS] representation is fed\\ninto an output layer for classiÔ¨Åcation, such as en-\\ntailment or sentiment analysis.\\nCompared to pre-training, Ô¨Åne-tuning is rela-\\ntively inexpensive. All of the results in the pa-\\nper can be replicated in at most 1 hour on a sin-\\ngle Cloud TPU, or a few hours on a GPU, starting\\nfrom the exact same pre-trained model. 7 We de-\\nscribe the task-speciÔ¨Åc details in the correspond-\\ning subsections of Section 4. More details can be\\nfound in Appendix A.5.\\n4 Experiments\\nIn this section, we present BERT Ô¨Åne-tuning re-\\nsults on 11 NLP tasks.\\n4.1 GLUE\\nThe General Language Understanding Evaluation\\n(GLUE) benchmark (Wang et al., 2018a) is a col-\\nlection of diverse natural language understanding\\ntasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo Ô¨Åne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tasks. Detailed descriptions of GLUE datasets are\\nincluded in Appendix B.1.\\nTo Ô¨Åne-tune on GLUE, we represent the input\\nsequence (for single sentence or sentence pairs)\\nas described in Section 3, and use the Ô¨Ånal hid-\\nden vector C ‚àà RH corresponding to the Ô¨Årst\\ninput token ([CLS]) as the aggregate representa-\\ntion. The only new parameters introduced during\\nÔ¨Åne-tuning are classiÔ¨Åcation layer weights W ‚àà\\nRK√óH, where Kis the number of labels. We com-\\npute a standard classiÔ¨Åcation loss with C and W,\\ni.e., log(softmax(CWT )).\\n7For example, the BERT SQuAD model can be trained in\\naround 30 minutes on a single Cloud TPU to achieve a Dev\\nF1 score of 91.0%.\\n8See (10) in https://gluebenchmark.com/faq.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\\nPre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\\nOpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\\nBERTBASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\\nBERTLARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\\nTable 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard).\\nThe number below each task denotes the number of training examples. The ‚ÄúAverage‚Äù column is slightly different\\nthan the ofÔ¨Åcial GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are single-\\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\\nWe use a batch size of 32 and Ô¨Åne-tune for 3\\nepochs over the data for all GLUE tasks. For each\\ntask, we selected the best Ô¨Åne-tuning learning rate\\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\\nAdditionally, for BERTLARGE we found that Ô¨Åne-\\ntuning was sometimes unstable on small datasets,\\nso we ran several random restarts and selected the\\nbest model on the Dev set. With random restarts,\\nwe use the same pre-trained checkpoint but per-\\nform different Ô¨Åne-tuning data shufÔ¨Çing and clas-\\nsiÔ¨Åer layer initialization.9\\nResults are presented in Table 1. Both\\nBERTBASE and BERTLARGE outperform all sys-\\ntems on all tasks by a substantial margin, obtaining\\n4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERTBASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='4.5% and 7.0% respective average accuracy im-\\nprovement over the prior state of the art. Note that\\nBERTBASE and OpenAI GPT are nearly identical\\nin terms of model architecture apart from the at-\\ntention masking. For the largest and most widely\\nreported GLUE task, MNLI, BERT obtains a 4.6%\\nabsolute accuracy improvement. On the ofÔ¨Åcial\\nGLUE leaderboard10, BERTLARGE obtains a score\\nof 80.5, compared to OpenAI GPT, which obtains\\n72.8 as of the date of writing.\\nWe Ô¨Ånd that BERT LARGE signiÔ¨Åcantly outper-\\nforms BERTBASE across all tasks, especially those\\nwith very little training data. The effect of model\\nsize is explored more thoroughly in Section 5.2.\\n4.2 SQuAD v1.1\\nThe Stanford Question Answering Dataset\\n(SQuAD v1.1) is a collection of 100k crowd-\\nsourced question/answer pairs (Rajpurkar et al.,\\n2016). Given a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='2016). Given a question and a passage from\\n9The GLUE data set distribution does not include the Test\\nlabels, and we only made a single GLUE evaluation server\\nsubmission for each of BERTBASE and BERTLARGE .\\n10https://gluebenchmark.com/leaderboard\\nWikipedia containing the answer, the task is to\\npredict the answer text span in the passage.\\nAs shown in Figure 1, in the question answer-\\ning task, we represent the input question and pas-\\nsage as a single packed sequence, with the ques-\\ntion using the A embedding and the passage using\\nthe B embedding. We only introduce a start vec-\\ntor S ‚ààRH and an end vector E ‚ààRH during\\nÔ¨Åne-tuning. The probability of word i being the\\nstart of the answer span is computed as a dot prod-\\nuct between Ti and S followed by a softmax over\\nall of the words in the paragraph: Pi = eS¬∑Ti\\n‚àë\\nj eS¬∑Tj .\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\nposition ito position jis deÔ¨Åned as S¬∑Ti + E¬∑Tj,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='‚àë\\nj eS¬∑Tj .\\nThe analogous formula is used for the end of the\\nanswer span. The score of a candidate span from\\nposition ito position jis deÔ¨Åned as S¬∑Ti + E¬∑Tj,\\nand the maximum scoring span where j ‚â• i is\\nused as a prediction. The training objective is the\\nsum of the log-likelihoods of the correct start and\\nend positions. We Ô¨Åne-tune for 3 epochs with a\\nlearning rate of 5e-5 and a batch size of 32.\\nTable 2 shows top leaderboard entries as well\\nas results from top published systems (Seo et al.,\\n2017; Clark and Gardner, 2018; Peters et al.,\\n2018a; Hu et al., 2018). The top results from the\\nSQuAD leaderboard do not have up-to-date public\\nsystem descriptions available,11 and are allowed to\\nuse any public data when training their systems.\\nWe therefore use modest data augmentation in\\nour system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi\\net al., 2017) befor Ô¨Åne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='our system by Ô¨Årst Ô¨Åne-tuning on TriviaQA (Joshi\\net al., 2017) befor Ô¨Åne-tuning on SQuAD.\\nOur best performing system outperforms the top\\nleaderboard system by +1.5 F1 in ensembling and\\n+1.3 F1 as a single system. In fact, our single\\nBERT model outperforms the top ensemble sys-\\ntem in terms of F1 score. Without TriviaQA Ô¨Åne-\\n11QANet is described in Yu et al. (2018), but the system\\nhas improved substantially after publication.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='System Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman - - 82.3 91.2\\n#1 Ensemble - nlnet - - 86.0 91.7\\n#2 Ensemble - QANet - - 84.5 90.5\\nPublished\\nBiDAF+ELMo (Single) - 85.6 - 85.8\\nR.M. Reader (Ensemble) 81.2 87.9 82.3 88.5\\nOurs\\nBERTBASE (Single) 80.8 88.5 - -\\nBERTLARGE (Single) 84.1 90.9 - -\\nBERTLARGE (Ensemble) 85.8 91.8 - -\\nBERTLARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\\nTable 2: SQuAD 1.1 results. The BERT ensemble\\nis 7x systems which use different pre-training check-\\npoints and Ô¨Åne-tuning seeds.\\nSystem Dev Test\\nEM F1 EM F1\\nTop Leaderboard Systems (Dec 10th, 2018)\\nHuman 86.3 89.0 86.9 89.5\\n#1 Single - MIR-MRC (F-Net) - - 74.8 78.0\\n#2 Single - nlnet - - 74.2 77.1\\nPublished\\nunet (Ensemble) - - 71.4 74.9\\nSLQA+ (Single) - 71.4 74.4\\nOurs\\nBERTLARGE (Single) 78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Ours\\nBERTLARGE (Single) 78.7 81.9 80.0 83.1\\nTable 3: SQuAD 2.0 results. We exclude entries that\\nuse BERT as one of their components.\\ntuning data, we only lose 0.1-0.4 F1, still outper-\\nforming all existing systems by a wide margin.12\\n4.3 SQuAD v2.0\\nThe SQuAD 2.0 task extends the SQuAD 1.1\\nproblem deÔ¨Ånition by allowing for the possibility\\nthat no short answer exists in the provided para-\\ngraph, making the problem more realistic.\\nWe use a simple approach to extend the SQuAD\\nv1.1 BERT model for this task. We treat ques-\\ntions that do not have an answer as having an an-\\nswer span with start and end at the [CLS] to-\\nken. The probability space for the start and end\\nanswer span positions is extended to include the\\nposition of the [CLS] token. For prediction, we\\ncompare the score of the no-answer span: snull =\\nS¬∑C+ E¬∑C to the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='S¬∑C+ E¬∑C to the score of the best non-null span\\n12The TriviaQA data we used consists of paragraphs from\\nTriviaQA-Wiki formed of the Ô¨Årst 400 tokens in documents,\\nthat contain at least one of the provided possible answers.\\nSystem Dev Test\\nESIM+GloVe 51.9 52.7\\nESIM+ELMo 59.1 59.2\\nOpenAI GPT - 78.0\\nBERTBASE 81.6 -\\nBERTLARGE 86.6 86.3\\nHuman (expert)‚Ä† - 85.0\\nHuman (5 annotations)‚Ä† - 88.0\\nTable 4: SW AG Dev and Test accuracies.‚Ä†Human per-\\nformance is measured with 100 samples, as reported in\\nthe SW AG paper.\\nÀÜsi,j = maxj‚â•iS¬∑Ti + E¬∑Tj. We predict a non-null\\nanswer when ÀÜsi,j > snull + œÑ, where the thresh-\\nold œÑ is selected on the dev set to maximize F1.\\nWe did not use TriviaQA data for this model. We\\nÔ¨Åne-tuned for 2 epochs with a learning rate of 5e-5\\nand a batch size of 48.\\nThe results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='The results compared to prior leaderboard en-\\ntries and top published work (Sun et al., 2018;\\nWang et al., 2018b) are shown in Table 3, exclud-\\ning systems that use BERT as one of their com-\\nponents. We observe a +5.1 F1 improvement over\\nthe previous best system.\\n4.4 SWAG\\nThe Situations With Adversarial Generations\\n(SW AG) dataset contains 113k sentence-pair com-\\npletion examples that evaluate grounded common-\\nsense inference (Zellers et al., 2018). Given a sen-\\ntence, the task is to choose the most plausible con-\\ntinuation among four choices.\\nWhen Ô¨Åne-tuning on the SW AG dataset, we\\nconstruct four input sequences, each containing\\nthe concatenation of the given sentence (sentence\\nA) and a possible continuation (sentence B). The\\nonly task-speciÔ¨Åc parameters introduced is a vec-\\ntor whose dot product with the [CLS] token rep-\\nresentation C denotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe Ô¨Åne-tune the model for 3 epochs with a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tor whose dot product with the [CLS] token rep-\\nresentation C denotes a score for each choice\\nwhich is normalized with a softmax layer.\\nWe Ô¨Åne-tune the model for 3 epochs with a\\nlearning rate of 2e-5 and a batch size of 16. Re-\\nsults are presented in Table 4. BERT LARGE out-\\nperforms the authors‚Äô baseline ESIM+ELMo sys-\\ntem by +27.1% and OpenAI GPT by 8.3%.\\n5 Ablation Studies\\nIn this section, we perform ablation experiments\\nover a number of facets of BERT in order to better\\nunderstand their relative importance. Additional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Dev Set\\nTasks MNLI-m QNLI MRPC SST-2 SQuAD\\n(Acc) (Acc) (Acc) (Acc) (F1)\\nBERTBASE 84.4 88.4 86.7 92.7 88.5\\nNo NSP 83.9 84.9 86.5 92.6 87.9\\nLTR & No NSP 82.1 84.3 77.5 92.1 77.8\\n+ BiLSTM 82.1 84.1 75.7 91.6 84.9\\nTable 5: Ablation over the pre-training tasks using the\\nBERTBASE architecture. ‚ÄúNo NSP‚Äù is trained without\\nthe next sentence prediction task. ‚ÄúLTR & No NSP‚Äù is\\ntrained as a left-to-right LM without the next sentence\\nprediction, like OpenAI GPT. ‚Äú+ BiLSTM‚Äù adds a ran-\\ndomly initialized BiLSTM on top of the ‚ÄúLTR + No\\nNSP‚Äù model during Ô¨Åne-tuning.\\nablation studies can be found in Appendix C.\\n5.1 Effect of Pre-training Tasks\\nWe demonstrate the importance of the deep bidi-\\nrectionality of BERT by evaluating two pre-\\ntraining objectives using exactly the same pre-\\ntraining data, Ô¨Åne-tuning scheme, and hyperpa-\\nrameters as BERTBASE :\\nNo NSP: A bidirectional model which is trained\\nusing the ‚Äúmasked LM‚Äù (MLM) but without the\\n‚Äúnext sentence prediction‚Äù (NSP) task.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='training data, Ô¨Åne-tuning scheme, and hyperpa-\\nrameters as BERTBASE :\\nNo NSP: A bidirectional model which is trained\\nusing the ‚Äúmasked LM‚Äù (MLM) but without the\\n‚Äúnext sentence prediction‚Äù (NSP) task.\\nLTR & No NSP: A left-context-only model which\\nis trained using a standard Left-to-Right (LTR)\\nLM, rather than an MLM. The left-only constraint\\nwas also applied at Ô¨Åne-tuning, because removing\\nit introduced a pre-train/Ô¨Åne-tune mismatch that\\ndegraded downstream performance. Additionally,\\nthis model was pre-trained without the NSP task.\\nThis is directly comparable to OpenAI GPT, but\\nusing our larger training dataset, our input repre-\\nsentation, and our Ô¨Åne-tuning scheme.\\nWe Ô¨Årst examine the impact brought by the NSP\\ntask. In Table 5, we show that removing NSP\\nhurts performance signiÔ¨Åcantly on QNLI, MNLI,\\nand SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR\\nmodel performs worse than the MLM model on all'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='and SQuAD 1.1. Next, we evaluate the impact\\nof training bidirectional representations by com-\\nparing ‚ÄúNo NSP‚Äù to ‚ÄúLTR & No NSP‚Äù. The LTR\\nmodel performs worse than the MLM model on all\\ntasks, with large drops on MRPC and SQuAD.\\nFor SQuAD it is intuitively clear that a LTR\\nmodel will perform poorly at token predictions,\\nsince the token-level hidden states have no right-\\nside context. In order to make a good faith at-\\ntempt at strengthening the LTR system, we added\\na randomly initialized BiLSTM on top. This does\\nsigniÔ¨Åcantly improve results on SQuAD, but the\\nresults are still far worse than those of the pre-\\ntrained bidirectional models. The BiLSTM hurts\\nperformance on the GLUE tasks.\\nWe recognize that it would also be possible to\\ntrain separate LTR and RTL models and represent\\neach token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='each token as the concatenation of the two mod-\\nels, as ELMo does. However: (a) this is twice as\\nexpensive as a single bidirectional model; (b) this\\nis non-intuitive for tasks like QA, since the RTL\\nmodel would not be able to condition the answer\\non the question; (c) this it is strictly less powerful\\nthan a deep bidirectional model, since it can use\\nboth left and right context at every layer.\\n5.2 Effect of Model Size\\nIn this section, we explore the effect of model size\\non Ô¨Åne-tuning task accuracy. We trained a number\\nof BERT models with a differing number of layers,\\nhidden units, and attention heads, while otherwise\\nusing the same hyperparameters and training pro-\\ncedure as described previously.\\nResults on selected GLUE tasks are shown in\\nTable 6. In this table, we report the average Dev\\nSet accuracy from 5 random restarts of Ô¨Åne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Set accuracy from 5 random restarts of Ô¨Åne-tuning.\\nWe can see that larger models lead to a strict ac-\\ncuracy improvement across all four datasets, even\\nfor MRPC which only has 3,600 labeled train-\\ning examples, and is substantially different from\\nthe pre-training tasks. It is also perhaps surpris-\\ning that we are able to achieve such signiÔ¨Åcant\\nimprovements on top of models which are al-\\nready quite large relative to the existing literature.\\nFor example, the largest Transformer explored in\\nVaswani et al. (2017) is (L=6, H=1024, A=16)\\nwith 100M parameters for the encoder, and the\\nlargest Transformer we have found in the literature\\nis (L=64, H=512, A=2) with 235M parameters\\n(Al-Rfou et al., 2018). By contrast, BERT BASE\\ncontains 110M parameters and BERT LARGE con-\\ntains 340M parameters.\\nIt has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='It has long been known that increasing the\\nmodel size will lead to continual improvements\\non large-scale tasks such as machine translation\\nand language modeling, which is demonstrated\\nby the LM perplexity of held-out training data\\nshown in Table 6. However, we believe that\\nthis is the Ô¨Årst work to demonstrate convinc-\\ningly that scaling to extreme model sizes also\\nleads to large improvements on very small scale\\ntasks, provided that the model has been sufÔ¨Å-\\nciently pre-trained. Peters et al. (2018b) presented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='mixed results on the downstream task impact of\\nincreasing the pre-trained bi-LM size from two\\nto four layers and Melamud et al. (2016) men-\\ntioned in passing that increasing hidden dimen-\\nsion size from 200 to 600 helped, but increasing\\nfurther to 1,000 did not bring further improve-\\nments. Both of these prior works used a feature-\\nbased approach ‚Äî we hypothesize that when the\\nmodel is Ô¨Åne-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspeciÔ¨Åc models can beneÔ¨Åt from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3 Feature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe Ô¨Åne-tuning approach, where a simple classiÔ¨Å-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly Ô¨Åne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere Ô¨Åxed features are extracted from the pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='cation layer is added to the pre-trained model, and\\nall parameters are jointly Ô¨Åne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere Ô¨Åxed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-speciÔ¨Åc model architecture to be added.\\nSecond, there are major computational beneÔ¨Åts\\nto pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). In the input to BERT, we\\nuse a case-preserving WordPiece model, and we\\ninclude the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams Dev Set Accuracy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='include the maximal document context provided\\nby the data. Following standard practice, we for-\\nmulate this as a tagging task but do not use a CRF\\nHyperparams Dev Set Accuracy\\n#L #H #A LM (ppl) MNLI-m MRPC SST-2\\n3 768 12 5.84 77.9 79.8 88.4\\n6 768 3 5.24 80.6 82.2 90.7\\n6 768 12 4.68 81.9 84.8 91.3\\n12 768 12 3.99 84.4 86.7 92.9\\n12 1024 16 3.54 85.7 86.9 93.3\\n24 1024 16 3.23 86.6 87.8 93.7\\nTable 6: Ablation over BERT model size. #L = the\\nnumber of layers; #H = hidden size; #A = number of at-\\ntention heads. ‚ÄúLM (ppl)‚Äù is the masked LM perplexity\\nof held-out training data.\\nSystem Dev F1 Test F1\\nELMo (Peters et al., 2018a) 95.7 92.2\\nCVT (Clark et al., 2018) - 92.6\\nCSE (Akbik et al., 2018) - 93.1\\nFine-tuning approach\\nBERTLARGE 96.6 92.8\\nBERTBASE 96.4 92.4\\nFeature-based approach (BERTBASE )\\nEmbeddings 91.0 -\\nSecond-to-Last Hidden 95.6 -\\nLast Hidden 94.9 -\\nWeighted Sum Last Four Hidden 95.9 -\\nConcat Last Four Hidden 96.1 -\\nWeighted Sum All 12 Layers 95.5 -'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Embeddings 91.0 -\\nSecond-to-Last Hidden 95.6 -\\nLast Hidden 94.9 -\\nWeighted Sum Last Four Hidden 95.9 -\\nConcat Last Four Hidden 96.1 -\\nWeighted Sum All 12 Layers 95.5 -\\nTable 7: CoNLL-2003 Named Entity Recognition re-\\nsults. Hyperparameters were selected using the Dev\\nset. The reported Dev and Test scores are averaged over\\n5 random restarts using those hyperparameters.\\nlayer in the output. We use the representation of\\nthe Ô¨Årst sub-token as the input to the token-level\\nclassiÔ¨Åer over the NER label set.\\nTo ablate the Ô¨Åne-tuning approach, we apply the\\nfeature-based approach by extracting the activa-\\ntions from one or more layers without Ô¨Åne-tuning\\nany parameters of BERT. These contextual em-\\nbeddings are used as input to a randomly initial-\\nized two-layer 768-dimensional BiLSTM before\\nthe classiÔ¨Åcation layer.\\nResults are presented in Table 7. BERT LARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='the classiÔ¨Åcation layer.\\nResults are presented in Table 7. BERT LARGE\\nperforms competitively with state-of-the-art meth-\\nods. The best performing method concatenates the\\ntoken representations from the top four hidden lay-\\ners of the pre-trained Transformer, which is only\\n0.3 F1 behind Ô¨Åne-tuning the entire model. This\\ndemonstrates that BERT is effective for both Ô¨Åne-\\ntuning and feature-based approaches.\\n6 Conclusion\\nRecent empirical improvements due to transfer\\nlearning with language models have demonstrated\\nthat rich, unsupervised pre-training is an integral\\npart of many language understanding systems. In\\nparticular, these results enable even low-resource\\ntasks to beneÔ¨Åt from deep unidirectional architec-\\ntures. Our major contribution is further general-\\nizing these Ô¨Åndings to deep bidirectional architec-\\ntures, allowing the same pre-trained model to suc-\\ncessfully tackle a broad set of NLP tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='References\\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\\n2018. Contextual string embeddings for sequence\\nlabeling. In Proceedings of the 27th International\\nConference on Computational Linguistics , pages\\n1638‚Äì1649.\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\\nGuo, and Llion Jones. 2018. Character-level lan-\\nguage modeling with deeper self-attention. arXiv\\npreprint arXiv:1808.04444.\\nRie Kubota Ando and Tong Zhang. 2005. A framework\\nfor learning predictive structures from multiple tasks\\nand unlabeled data. Journal of Machine Learning\\nResearch, 6(Nov):1817‚Äì1853.\\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\\nThe Ô¨Åfth PASCAL recognizing textual entailment\\nchallenge. In TAC. NIST.\\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\\n2006. Domain adaptation with structural correspon-\\ndence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120‚Äì128. Association for Computa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='dence learning. In Proceedings of the 2006 confer-\\nence on empirical methods in natural language pro-\\ncessing, pages 120‚Äì128. Association for Computa-\\ntional Linguistics.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nIn EMNLP. Association for Computational Linguis-\\ntics.\\nPeter F Brown, Peter V Desouza, Robert L Mercer,\\nVincent J Della Pietra, and Jenifer C Lai. 1992.\\nClass-based n-gram models of natural language.\\nComputational linguistics, 18(4):467‚Äì479.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\\nGazpio, and Lucia Specia. 2017. Semeval-2017\\ntask 1: Semantic textual similarity multilingual and\\ncrosslingual focused evaluation. In Proceedings\\nof the 11th International Workshop on Semantic\\nEvaluation (SemEval-2017) , pages 1‚Äì14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Evaluation (SemEval-2017) , pages 1‚Äì14, Vancou-\\nver, Canada. Association for Computational Lin-\\nguistics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2013. One billion word benchmark for measur-\\ning progress in statistical language modeling. arXiv\\npreprint arXiv:1312.3005.\\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\\nQuora question pairs.\\nChristopher Clark and Matt Gardner. 2018. Simple\\nand effective multi-paragraph reading comprehen-\\nsion. In ACL.\\nKevin Clark, Minh-Thang Luong, Christopher D Man-\\nning, and Quoc Le. 2018. Semi-supervised se-\\nquence modeling with cross-view training. In Pro-\\nceedings of the 2018 Conference on Empirical Meth-\\nods in Natural Language Processing , pages 1914‚Äì\\n1925.\\nRonan Collobert and Jason Weston. 2008. A uniÔ¨Åed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning. In Pro-\\nceedings of the 25th international conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Ronan Collobert and Jason Weston. 2008. A uniÔ¨Åed\\narchitecture for natural language processing: Deep\\nneural networks with multitask learning. In Pro-\\nceedings of the 25th international conference on\\nMachine learning, pages 160‚Äì167. ACM.\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¬®ƒ±c\\nBarrault, and Antoine Bordes. 2017. Supervised\\nlearning of universal sentence representations from\\nnatural language inference data. In Proceedings of\\nthe 2017 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 670‚Äì680, Copen-\\nhagen, Denmark. Association for Computational\\nLinguistics.\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in neural informa-\\ntion processing systems, pages 3079‚Äì3087.\\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\\nFei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09.\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Fei. 2009. ImageNet: A Large-Scale Hierarchical\\nImage Database. In CVPR09.\\nWilliam B Dolan and Chris Brockett. 2005. Automati-\\ncally constructing a corpus of sentential paraphrases.\\nIn Proceedings of the Third International Workshop\\non Paraphrasing (IWP2005).\\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\\n2018. Maskgan: Better text generation via Ô¨Ålling in\\nthe . arXiv preprint arXiv:1801.07736.\\nDan Hendrycks and Kevin Gimpel. 2016. Bridging\\nnonlinearities and stochastic regularizers with gaus-\\nsian error linear units. CoRR, abs/1606.08415.\\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\\nLearning distributed representations of sentences\\nfrom unlabelled data. In Proceedings of the 2016\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies. Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Language Technologies. Association for Computa-\\ntional Linguistics.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\\nACL. Association for Computational Linguistics.\\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\\nFuru Wei, and Ming Zhou. 2018. Reinforced\\nmnemonic reader for machine reading comprehen-\\nsion. In IJCAI.\\nYacine Jernite, Samuel R. Bowman, and David Son-\\ntag. 2017. Discourse-based objectives for fast un-\\nsupervised sentence representation learning. CoRR,\\nabs/1705.00557.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In ACL.\\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Skip-thought vectors. In\\nAdvances in neural information processing systems,\\npages 3294‚Äì3302.\\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\\nresentations of sentences and documents. In Inter-\\nnational Conference on Machine Learning , pages\\n1188‚Äì1196.\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The winograd schema challenge. In\\nAaai spring symposium: Logical formalizations of\\ncommonsense reasoning, volume 46, page 47.\\nLajanugen Logeswaran and Honglak Lee. 2018. An\\nefÔ¨Åcient framework for learning sentence represen-\\ntations. In International Conference on Learning\\nRepresentations.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='tations. In International Conference on Learning\\nRepresentations.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS.\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional LSTM. In CoNLL.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In Advances in Neural Information Processing\\nSystems 26 , pages 3111‚Äì3119. Curran Associates,\\nInc.\\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\\nable hierarchical distributed language model. In\\nD. Koller, D. Schuurmans, Y . Bengio, and L. Bot-\\ntou, editors, Advances in Neural Information Pro-\\ncessing Systems 21 , pages 1081‚Äì1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='cessing Systems 21 , pages 1081‚Äì1088. Curran As-\\nsociates, Inc.\\nAnkur P Parikh, Oscar T ¬®ackstr¬®om, Dipanjan Das, and\\nJakob Uszkoreit. 2016. A decomposable attention\\nmodel for natural language inference. In EMNLP.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 1532‚Äì\\n1543.\\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nIn ACL.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018a. Deep contextualized word rep-\\nresentations. In NAACL.\\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\\nand Wen-tau Yih. 2018b. Dissecting contextual\\nword embeddings: Architecture and representation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n1499‚Äì1509.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='word embeddings: Architecture and representation.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n1499‚Äì1509.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018. Improving language under-\\nstanding with unsupervised learning. Technical re-\\nport, OpenAI.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing, pages 2383‚Äì2392.\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nÔ¨Çow for machine comprehension. In ICLR.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of the 2013 conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Chuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Proceedings of the 2013 conference on\\nempirical methods in natural language processing ,\\npages 1631‚Äì1642.\\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\\n2018. U-net: Machine reading comprehension\\nwith unanswerable questions. arXiv preprint\\narXiv:1810.06638.\\nWilson L Taylor. 1953. Cloze procedure: A new\\ntool for measuring readability. Journalism Bulletin,\\n30(4):415‚Äì433.\\nErik F Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In\\nCoNLL.\\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\\nWord representations: A simple and general method\\nfor semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics, ACL ‚Äô10, pages 384‚Äì394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='for semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics, ACL ‚Äô10, pages 384‚Äì394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems, pages 6000‚Äì6010.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\\nPierre-Antoine Manzagol. 2008. Extracting and\\ncomposing robust features with denoising autoen-\\ncoders. In Proceedings of the 25th international\\nconference on Machine learning, pages 1096‚Äì1103.\\nACM.\\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\\nGlue: A multi-task benchmark and analysis platform'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='for natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: An-\\nalyzing and Interpreting Neural Networks for NLP ,\\npages 353‚Äì355.\\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\\ngranularity hierarchical attention fusion networks\\nfor reading comprehension and question answering.\\nIn Proceedings of the 56th Annual Meeting of the As-\\nsociation for Computational Linguistics (Volume 1:\\nLong Papers). Association for Computational Lin-\\nguistics.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\\nman. 2018. Neural network acceptability judg-\\nments. arXiv preprint arXiv:1805.12471.\\nAdina Williams, Nikita Nangia, and Samuel R Bow-\\nman. 2018. A broad-coverage challenge corpus\\nfor sentence understanding through inference. In\\nNAACL.\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\\nLe, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='Le, Mohammad Norouzi, Wolfgang Macherey,\\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\\nMacherey, et al. 2016. Google‚Äôs neural ma-\\nchine translation system: Bridging the gap between\\nhuman and machine translation. arXiv preprint\\narXiv:1609.08144.\\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\\nLipson. 2014. How transferable are features in deep\\nneural networks? In Advances in neural information\\nprocessing systems, pages 3320‚Äì3328.\\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\\nLe. 2018. QANet: Combining local convolution\\nwith global self-attention for reading comprehen-\\nsion. In ICLR.\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\\nChoi. 2018. Swag: A large-scale adversarial dataset\\nfor grounded commonsense inference. In Proceed-\\nings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP).\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='ings of the 2018 Conference on Empirical Methods\\nin Natural Language Processing (EMNLP).\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\\nFidler. 2015. Aligning books and movies: Towards\\nstory-like visual explanations by watching movies\\nand reading books. In Proceedings of the IEEE\\ninternational conference on computer vision , pages\\n19‚Äì27.\\nAppendix for ‚ÄúBERT: Pre-training of\\nDeep Bidirectional Transformers for\\nLanguage Understanding‚Äù\\nWe organize the appendix into three sections:\\n‚Ä¢ Additional implementation details for BERT\\nare presented in Appendix A;\\n‚Ä¢ Additional details for our experiments are\\npresented in Appendix B; and\\n‚Ä¢ Additional ablation studies are presented in\\nAppendix C.\\nWe present additional ablation studies for\\nBERT including:\\n‚Äì Effect of Number of Training Steps; and\\n‚Äì Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT including:\\n‚Äì Effect of Number of Training Steps; and\\n‚Äì Ablation for Different Masking Proce-\\ndures.\\nA Additional Details for BERT\\nA.1 Illustration of the Pre-training Tasks\\nWe provide examples of the pre-training tasks in\\nthe following.\\nMasked LM and the Masking ProcedureAs-\\nsuming the unlabeled sentence is my dog is\\nhairy, and during the random masking procedure\\nwe chose the 4-th token (which corresponding to\\nhairy), our masking procedure can be further il-\\nlustrated by\\n‚Ä¢ 80% of the time: Replace the word with the\\n[MASK] token, e.g., my dog is hairy ‚Üí\\nmy dog is [MASK]\\n‚Ä¢ 10% of the time: Replace the word with a\\nrandom word, e.g., my dog is hairy ‚Üí my\\ndog is apple\\n‚Ä¢ 10% of the time: Keep the word un-\\nchanged, e.g., my dog is hairy ‚Üí my dog\\nis hairy. The purpose of this is to bias the\\nrepresentation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 11, 'page_label': '1', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='representation towards the actual observed\\nword.\\nThe advantage of this procedure is that the\\nTransformer encoder does not know which words\\nit will be asked to predict or which have been re-\\nplaced by random words, so it is forced to keep\\na distributional contextual representation of ev-\\nery input token. Additionally, because random\\nreplacement only occurs for 1.5% of all tokens\\n(i.e., 10% of 15%), this does not seem to harm\\nthe model‚Äôs language understanding capability. In\\nSection C.2, we evaluate the impact this proce-\\ndure.\\nCompared to standard langauge model training,\\nthe masked LM only make predictions on 15% of\\ntokens in each batch, which suggests that more\\npre-training steps may be required for the model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT (Ours)\\nTrm Trm Trm\\nTrm Trm Trm\\n...\\n...\\nTrm Trm Trm\\nTrm Trm Trm\\n...\\n...\\nOpenAI GPT\\nLstm\\nELMo\\nLstm Lstm\\nLstm Lstm Lstm\\nLstm Lstm Lstm\\nLstm Lstm Lstm\\n T1 T2  TN...\\n...\\n...\\n...\\n...\\n E1 E2  EN...\\n T1 T2 TN...\\n E1 E2  EN...\\n T1 T2  TN...\\n E1 E2  EN...\\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\\nOpenAI GPT are Ô¨Åne-tuning approaches, while ELMo is a feature-based approach.\\nto converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='to converge. In Section C.1 we demonstrate that\\nMLM does converge marginally slower than a left-\\nto-right model (which predicts every token), but\\nthe empirical improvements of the MLM model\\nfar outweigh the increased training cost.\\nNext Sentence Prediction The next sentence\\nprediction task can be illustrated in the following\\nexamples.\\nInput = [CLS] the man went to [MASK] store [SEP]\\nhe bought a gallon [MASK] milk [SEP]\\nLabel = IsNext\\nInput = [CLS] the man [MASK] to the store [SEP]\\npenguin [MASK] are flight ##less birds [SEP]\\nLabel = NotNext\\nA.2 Pre-training Procedure\\nTo generate each training input sequence, we sam-\\nple two spans of text from the corpus, which we\\nrefer to as ‚Äúsentences‚Äù even though they are typ-\\nically much longer than single sentences (but can\\nbe shorter also). The Ô¨Årst sentence receives the A\\nembedding and the second receives the B embed-\\nding. 50% of the time B is the actual next sentence\\nthat follows A and 50% of the time it is a random'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='be shorter also). The Ô¨Årst sentence receives the A\\nembedding and the second receives the B embed-\\nding. 50% of the time B is the actual next sentence\\nthat follows A and 50% of the time it is a random\\nsentence, which is done for the ‚Äúnext sentence pre-\\ndiction‚Äù task. They are sampled such that the com-\\nbined length is ‚â§512 tokens. The LM masking is\\napplied after WordPiece tokenization with a uni-\\nform masking rate of 15%, and no special consid-\\neration given to partial word pieces.\\nWe train with batch size of 256 sequences (256\\nsequences * 512 tokens = 128,000 tokens/batch)\\nfor 1,000,000 steps, which is approximately 40\\nepochs over the 3.3 billion word corpus. We\\nuse Adam with learning rate of 1e-4, Œ≤1 = 0.9,\\nŒ≤2 = 0.999, L2 weight decay of 0.01, learning\\nrate warmup over the Ô¨Årst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='rate warmup over the Ô¨Årst 10,000 steps, and linear\\ndecay of the learning rate. We use a dropout prob-\\nability of 0.1 on all layers. We use a gelu acti-\\nvation (Hendrycks and Gimpel, 2016) rather than\\nthe standard relu, following OpenAI GPT. The\\ntraining loss is the sum of the mean masked LM\\nlikelihood and the mean next sentence prediction\\nlikelihood.\\nTraining of BERT BASE was performed on 4\\nCloud TPUs in Pod conÔ¨Åguration (16 TPU chips\\ntotal).13 Training of BERTLARGE was performed\\non 16 Cloud TPUs (64 TPU chips total). Each pre-\\ntraining took 4 days to complete.\\nLonger sequences are disproportionately expen-\\nsive because attention is quadratic to the sequence\\nlength. To speed up pretraing in our experiments,\\nwe pre-train the model with sequence length of\\n128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor Ô¨Åne-tuning, most model hyperparameters are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 12, 'page_label': '2', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='128 for 90% of the steps. Then, we train the rest\\n10% of the steps of sequence of 512 to learn the\\npositional embeddings.\\nA.3 Fine-tuning Procedure\\nFor Ô¨Åne-tuning, most model hyperparameters are\\nthe same as in pre-training, with the exception of\\nthe batch size, learning rate, and number of train-\\ning epochs. The dropout probability was always\\nkept at 0.1. The optimal hyperparameter values\\nare task-speciÔ¨Åc, but we found the following range\\nof possible values to work well across all tasks:\\n‚Ä¢ Batch size: 16, 32\\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\\nTPU-now-offers-preemptible-pricing-and-global-\\navailability.html'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='‚Ä¢ Learning rate (Adam): 5e-5, 3e-5, 2e-5\\n‚Ä¢ Number of epochs: 2, 3, 4\\nWe also observed that large data sets (e.g.,\\n100k+ labeled training examples) were far less\\nsensitive to hyperparameter choice than small data\\nsets. Fine-tuning is typically very fast, so it is rea-\\nsonable to simply run an exhaustive search over\\nthe above parameters and choose the model that\\nperforms best on the development set.\\nA.4 Comparison of BERT, ELMo ,and\\nOpenAI GPT\\nHere we studies the differences in recent popular\\nrepresentation learning models including ELMo,\\nOpenAI GPT and BERT. The comparisons be-\\ntween the model architectures are shown visually\\nin Figure 3. Note that in addition to the architec-\\nture differences, BERT and OpenAI GPT are Ô¨Åne-\\ntuning approaches, while ELMo is a feature-based\\napproach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='approach.\\nThe most comparable existing pre-training\\nmethod to BERT is OpenAI GPT, which trains a\\nleft-to-right Transformer LM on a large text cor-\\npus. In fact, many of the design decisions in BERT\\nwere intentionally made to make it as close to\\nGPT as possible so that the two methods could be\\nminimally compared. The core argument of this\\nwork is that the bi-directionality and the two pre-\\ntraining tasks presented in Section 3.1 account for\\nthe majority of the empirical improvements, but\\nwe do note that there are several other differences\\nbetween how BERT and GPT were trained:\\n‚Ä¢ GPT is trained on the BooksCorpus (800M\\nwords); BERT is trained on the BooksCor-\\npus (800M words) and Wikipedia (2,500M\\nwords).\\n‚Ä¢ GPT uses a sentence separator ( [SEP]) and\\nclassiÔ¨Åer token ( [CLS]) which are only in-\\ntroduced at Ô¨Åne-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n‚Ä¢ GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='troduced at Ô¨Åne-tuning time; BERT learns\\n[SEP], [CLS] and sentence A/B embed-\\ndings during pre-training.\\n‚Ä¢ GPT was trained for 1M steps with a batch\\nsize of 32,000 words; BERT was trained for\\n1M steps with a batch size of 128,000 words.\\n‚Ä¢ GPT used the same learning rate of 5e-5 for\\nall Ô¨Åne-tuning experiments; BERT chooses a\\ntask-speciÔ¨Åc Ô¨Åne-tuning learning rate which\\nperforms the best on the development set.\\nTo isolate the effect of these differences, we per-\\nform ablation experiments in Section 5.1 which\\ndemonstrate that the majority of the improvements\\nare in fact coming from the two pre-training tasks\\nand the bidirectionality they enable.\\nA.5 Illustrations of Fine-tuning on Different\\nTasks\\nThe illustration of Ô¨Åne-tuning BERT on different\\ntasks can be seen in Figure 4. Our task-speciÔ¨Åc\\nmodels are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='models are formed by incorporating BERT with\\none additional output layer, so a minimal num-\\nber of parameters need to be learned from scratch.\\nAmong the tasks, (a) and (b) are sequence-level\\ntasks while (c) and (d) are token-level tasks. In\\nthe Ô¨Ågure, E represents the input embedding, Ti\\nrepresents the contextual representation of tokeni,\\n[CLS] is the special symbol for classiÔ¨Åcation out-\\nput, and [SEP] is the special symbol to separate\\nnon-consecutive token sequences.\\nB Detailed Experimental Setup\\nB.1 Detailed Descriptions for the GLUE\\nBenchmark Experiments.\\nOur GLUE results in Table1 are obtained\\nfrom https://gluebenchmark.com/\\nleaderboard and https://blog.\\nopenai.com/language-unsupervised.\\nThe GLUE benchmark includes the following\\ndatasets, the descriptions of which were originally\\nsummarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiÔ¨Å-\\ncation task (Williams et al., 2018). Given a pair of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 13, 'page_label': '3', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='summarized in Wang et al. (2018a):\\nMNLI Multi-Genre Natural Language Inference\\nis a large-scale, crowdsourced entailment classiÔ¨Å-\\ncation task (Williams et al., 2018). Given a pair of\\nsentences, the goal is to predict whether the sec-\\nond sentence is an entailment, contradiction, or\\nneutral with respect to the Ô¨Årst one.\\nQQP Quora Question Pairs is a binary classiÔ¨Å-\\ncation task where the goal is to determine if two\\nquestions asked on Quora are semantically equiv-\\nalent (Chen et al., 2018).\\nQNLI Question Natural Language Inference is\\na version of the Stanford Question Answering\\nDataset (Rajpurkar et al., 2016) which has been\\nconverted to a binary classiÔ¨Åcation task (Wang\\net al., 2018a). The positive examples are (ques-\\ntion, sentence) pairs which do contain the correct\\nanswer, and the negative examples are (question,\\nsentence) from the same paragraph which do not\\ncontain the answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='BERT\\nE[CLS] E1  E[SEP]... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\n[CLS] Tok \\n1\\n [SEP]... Tok \\nN\\nTok \\n1 ... Tok\\nM\\nQuestion Paragraph\\nBERT\\nE[CLS] E1  E2  EN\\nC\\n T1\\n  T2\\n  TN\\nSingle Sentence \\n...\\n...\\nBERT\\nTok 1  Tok 2  Tok N...[CLS]\\nE[CLS] E1  E2  EN\\nC\\n T1\\n  T2\\n  TN\\nSingle Sentence \\nB-PERO O\\n...\\n...E[CLS] E1  E[SEP]\\nClass \\nLabel\\n... EN E1‚Äô ... EM‚Äô\\nC\\n T1\\n T[SEP]...\\n TN\\n T1‚Äô ...\\n TM‚Äô\\nStart/End Span\\nClass \\nLabel\\nBERT\\nTok 1  Tok 2  Tok N...[CLS] Tok 1[CLS][CLS] Tok \\n1\\n [SEP]... Tok \\nN\\nTok \\n1 ... Tok\\nM\\nSentence 1\\n...\\nSentence 2\\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\\nSST-2 The Stanford Sentiment Treebank is a\\nbinary single-sentence classiÔ¨Åcation task consist-\\ning of sentences extracted from movie reviews\\nwith human annotations of their sentiment (Socher\\net al., 2013).\\nCoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classiÔ¨Åcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically ‚Äúacceptable‚Äù or not (Warstadt'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='CoLA The Corpus of Linguistic Acceptability is\\na binary single-sentence classiÔ¨Åcation task, where\\nthe goal is to predict whether an English sentence\\nis linguistically ‚Äúacceptable‚Äù or not (Warstadt\\net al., 2018).\\nSTS-B The Semantic Textual Similarity Bench-\\nmark is a collection of sentence pairs drawn from\\nnews headlines and other sources (Cer et al.,\\n2017). They were annotated with a score from 1\\nto 5 denoting how similar the two sentences are in\\nterms of semantic meaning.\\nMRPC Microsoft Research Paraphrase Corpus\\nconsists of sentence pairs automatically extracted\\nfrom online news sources, with human annotations\\nfor whether the sentences in the pair are semanti-\\ncally equivalent (Dolan and Brockett, 2005).\\nRTE Recognizing Textual Entailment is a bi-\\nnary entailment task similar to MNLI, but with\\nmuch less training data (Bentivogli et al., 2009).14\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 14, 'page_label': '4', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='much less training data (Bentivogli et al., 2009).14\\nWNLI Winograd NLI is a small natural lan-\\nguage inference dataset (Levesque et al., 2011).\\nThe GLUE webpage notes that there are issues\\nwith the construction of this dataset, 15 and every\\ntrained system that‚Äôs been submitted to GLUE has\\nperformed worse than the 65.1 baseline accuracy\\nof predicting the majority class. We therefore ex-\\nclude this set to be fair to OpenAI GPT. For our\\nGLUE submission, we always predicted the ma-\\n14Note that we only report single-task Ô¨Åne-tuning results\\nin this paper. A multitask Ô¨Åne-tuning approach could poten-\\ntially push the performance even further. For example, we\\ndid observe substantial improvements on RTE from multi-\\ntask training with MNLI.\\n15https://gluebenchmark.com/faq'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='jority class.\\nC Additional Ablation Studies\\nC.1 Effect of Number of Training Steps\\nFigure 5 presents MNLI Dev accuracy after Ô¨Åne-\\ntuning from a checkpoint that has been pre-trained\\nfor ksteps. This allows us to answer the following\\nquestions:\\n1. Question: Does BERT really need such\\na large amount of pre-training (128,000\\nwords/batch * 1,000,000 steps) to achieve\\nhigh Ô¨Åne-tuning accuracy?\\nAnswer: Yes, BERT BASE achieves almost\\n1.0% additional accuracy on MNLI when\\ntrained on 1M steps compared to 500k steps.\\n2. Question: Does MLM pre-training converge\\nslower than LTR pre-training, since only 15%\\nof words are predicted in each batch rather\\nthan every word?\\nAnswer: The MLM model does converge\\nslightly slower than the LTR model. How-\\never, in terms of absolute accuracy the MLM\\nmodel begins to outperform the LTR model\\nalmost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='almost immediately.\\nC.2 Ablation for Different Masking\\nProcedures\\nIn Section 3.1, we mention that BERT uses a\\nmixed strategy for masking the target tokens when\\npre-training with the masked language model\\n(MLM) objective. The following is an ablation\\nstudy to evaluate the effect of different masking\\nstrategies.\\n200 400 600 800 1,000\\n76\\n78\\n80\\n82\\n84\\nPre-training Steps (Thousands)\\nMNLI Dev Accuracy\\nBERTBASE (Masked LM)\\nBERTBASE (Left-to-Right)\\nFigure 5: Ablation over number of training steps. This\\nshows the MNLI accuracy after Ô¨Åne-tuning, starting\\nfrom model parameters that have been pre-trained for\\nksteps. The x-axis is the value of k.\\nNote that the purpose of the masking strategies\\nis to reduce the mismatch between pre-training\\nand Ô¨Åne-tuning, as the [MASK] symbol never ap-\\npears during the Ô¨Åne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both Ô¨Åne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='pears during the Ô¨Åne-tuning stage. We report the\\nDev results for both MNLI and NER. For NER,\\nwe report both Ô¨Åne-tuning and feature-based ap-\\nproaches, as we expect the mismatch will be am-\\npliÔ¨Åed for the feature-based approach as the model\\nwill not have the chance to adjust the representa-\\ntions.\\nMasking Rates Dev Set Results\\nMASK SAME RND MNLI NER\\nFine-tune Fine-tune Feature-based\\n80% 10% 10% 84.2 95.4 94.9\\n100% 0% 0% 84.3 94.9 94.0\\n80% 0% 20% 84.1 95.2 94.6\\n80% 20% 0% 84.4 95.2 94.7\\n0% 20% 80% 83.7 94.8 94.6\\n0% 0% 100% 83.6 94.9 94.6\\nTable 8: Ablation over different masking strategies.\\nThe results are presented in Table 8. In the table,\\nMASK means that we replace the target token with\\nthe [MASK] symbol for MLM; SAME means that\\nwe keep the target token as is; R ND means that\\nwe replace the target token with another random\\ntoken.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciÔ¨Åc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-05-28T00:07:51+00:00', 'author': '', 'keywords': '', 'moddate': '2019-05-28T00:07:51+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'total_pages': 16, 'page': 15, 'page_label': '5', 'source_file': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'file_type': 'pdf'}, page_content='token.\\nThe numbers in the left part of the table repre-\\nsent the probabilities of the speciÔ¨Åc strategies used\\nduring MLM pre-training (BERT uses 80%, 10%,\\n10%). The right part of the paper represents the\\nDev set results. For the feature-based approach,\\nwe concatenate the last 4 layers of BERT as the\\nfeatures, which was shown to be the best approach\\nin Section 5.3.\\nFrom the table it can be seen that Ô¨Åne-tuning is\\nsurprisingly robust to different masking strategies.\\nHowever, as expected, using only the MASK strat-\\negy was problematic when applying the feature-\\nbased approach to NER. Interestingly, using only\\nthe R ND strategy performs much worse than our\\nstrategy as well.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='DeepSeekMath: Pushing the Limits of Mathematical\\nReasoning in Open Language Models\\nZhihong Shao1,2‚àó‚Ä†, Peiyi Wang1,3‚àó‚Ä†, Qihao Zhu1,3‚àó‚Ä†, Runxin Xu1, Junxiao Song1\\nXiao Bi1, Haowei Zhang1, Mingchuan Zhang1, Y.K. Li1, Y. Wu1, Daya Guo1‚àó\\n1DeepSeek-AI, 2Tsinghua University,3Peking University\\n{zhihongshao,wangpeiyi,zhuqh,guoday}@deepseek.com\\nhttps://github.com/deepseek-ai/DeepSeek-Math\\nAbstract\\nMathematical reasoning poses a significant challenge for language models due to its complex\\nand structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-\\ntraining DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common\\nCrawl, together with natural language and code data. DeepSeekMath 7B has achieved an\\nimpressive score of 51.7% on the competition-level MATH benchmark without relying on\\nexternal toolkits and voting techniques, approaching the performance level of Gemini-Ultra'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='impressive score of 51.7% on the competition-level MATH benchmark without relying on\\nexternal toolkits and voting techniques, approaching the performance level of Gemini-Ultra\\nand GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First,\\nwe harness the significant potential of publicly available web data through a meticulously\\nengineered data selection pipeline. Second, we introduce Group Relative Policy Optimization\\n(GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning\\nabilities while concurrently optimizing the memory usage of PPO.\\nFigure 1 |Top1 accuracy of open-source models on the competition-level MATH benchmark\\n(Hendrycks et al., 2021) without the use of external toolkits and voting techniques.\\n‚àóCore contributors.\\n‚Ä†Work done during internship at DeepSeek-AI.\\narXiv:2402.03300v3  [cs.CL]  27 Apr 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='1. Introduction\\nLarge language models (LLM) have revolutionized the approach to mathematical reasoning\\nin artificial intelligence, spurring significant advancements in both the quantitative reasoning\\nbenchmark (Hendrycks et al., 2021) and the geometry reasoning benchmark (Trinh et al., 2024).\\nMoreover, these models have proven instrumental in assisting humans in solving complex\\nmathematical problems (Tao, 2023). However, cutting-edge models such as GPT-4 (OpenAI,\\n2023) and Gemini-Ultra (Anil et al., 2023) are not publicly available, and the currently accessible\\nopen-source models considerably trail behind in performance.\\nIn this study, we introduce DeepSeekMath, a domain-specific language model that signifi-\\ncantly outperforms the mathematical capabilities of open-source models and approaches the\\nperformance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-\\nMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='performance level of GPT-4 on academic benchmarks. To achieve this, we create the DeepSeek-\\nMath Corpus, a large-scale high-quality pre-training corpus comprising 120B math tokens. This\\ndataset is extracted from the Common Crawl (CC) using a fastText-based classifier (Joulin et al.,\\n2016). In the initial iteration, the classifier is trained using instances from OpenWebMath (Paster\\net al., 2023) as positive examples, while incorporating a diverse selection of other web pages to\\nserve as negative examples. Subsequently, we employ the classifier to mine additional positive\\ninstances from the CC, which are further refined through human annotation. The classifier is\\nthen updated with this enhanced dataset to improve its performance. The evaluation results\\nindicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base\\n7B achieves 64.2% on GSM8K (Cobbe et al., 2021) and 36.2% on the competition-level MATH'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='indicate that the large-scale corpus is of high quality, as our base model DeepSeekMath-Base\\n7B achieves 64.2% on GSM8K (Cobbe et al., 2021) and 36.2% on the competition-level MATH\\ndataset (Hendrycks et al., 2021), outperforming Minerva 540B (Lewkowycz et al., 2022a). In\\naddition, the DeepSeekMath Corpus is multilingual, so we notice an improvement in Chinese\\nmathematical benchmarks (Wei et al., 2023; Zhong et al., 2023). We believe that our experience\\nin mathematical data processing is a starting point for the research community, and there is\\nsignificant room for improvement in the future.\\nDeepSeekMath-Base is initialized with DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), as\\nwe notice that starting from a code training model is a better choice compared to a general\\nLLM. Furthermore, we observe the math training also improves model capability on MMLU\\n(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='LLM. Furthermore, we observe the math training also improves model capability on MMLU\\n(Hendrycks et al., 2020) and BBH benchmarks (Suzgun et al., 2022), indicating it does not only\\nenhance the model‚Äôs mathematical abilities but also amplifies general reasoning capabilities.\\nAfter pre-training, we apply mathematical instruction tuning to DeepSeekMath-Base with\\nchain-of-thought (Wei et al., 2022), program-of-thought (Chen et al., 2022; Gao et al., 2023), and\\ntool-integrated reasoning (Gou et al., 2023) data. The resulting model DeepSeekMath-Instruct\\n7B beats all 7B counterparts and is comparable with 70B open-source instruction-tuned models.\\nFurthermore, we introduce the Group Relative Policy Optimization (GRPO), a variant rein-\\nforcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).\\nGRPO foregoes the critic model, instead estimating the baseline from group scores, significantly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='forcement learning (RL) algorithm of Proximal Policy Optimization (PPO) (Schulman et al., 2017).\\nGRPO foregoes the critic model, instead estimating the baseline from group scores, significantly\\nreducing training resources. By solely using a subset of English instruction tuning data, GRPO\\nobtains a substantial improvement over the strong DeepSeekMath-Instruct, including both\\nin-domain (GSM8K: 82.9% ‚Üí88.2%, MATH: 46.8% ‚Üí51.7%) and out-of-domain mathematical\\ntasks (e.g., CMATH: 84.6% ‚Üí88.8%) during the reinforcement learning phase. We also provide\\na unified paradigm to understand different methods, such as Rejection Sampling Fine-Tuning\\n(RFT) (Yuan et al., 2023a), Direct Preference Optimization (DPO) (Rafailov et al., 2023), PPO and\\nGRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as\\neither direct or simplified RL techniques. We also conduct extensive experiments, e.g., online'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 1, 'page_label': '2', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='GRPO. Based on such a unified paradigm, we find that all these methods are conceptualized as\\neither direct or simplified RL techniques. We also conduct extensive experiments, e.g., online\\nv.s. offline training, outcome v.s. process supervision, single-turn v.s. iterative RL and so on,\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='to deeply investigate the essential elements of this paradigm. At last, we explain why our RL\\nboosts the performance of instruction-tuned models, and further summarize potential directions\\nto achieve more effective RL based on this unified paradigm.\\n1.1. Contributions\\nOur contribution includes scalable math pre-training, along with the exploration and analysis of\\nreinforcement learning.\\nMath Pre-Training at Scale\\n‚Ä¢ Our research provides compelling evidence that the publicly accessible Common Crawl\\ndata contains valuable information for mathematical purposes. By implementing a metic-\\nulously designed data selection pipeline, we successfully construct the DeepSeekMath\\nCorpus, a high-quality dataset of 120B tokens from web pages filtered for mathemati-\\ncal content, which is almost 7 times the size of the math web pages used by Minerva\\n(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath\\n(Paster et al., 2023).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='cal content, which is almost 7 times the size of the math web pages used by Minerva\\n(Lewkowycz et al., 2022a) and 9 times the size of the recently released OpenWebMath\\n(Paster et al., 2023).\\n‚Ä¢ Our pre-trained base model DeepSeekMath-Base 7B achieves comparable performance\\nwith Minerva 540B (Lewkowycz et al., 2022a), indicating the number of parameters is not\\nthe only key factor in mathematical reasoning capability. A smaller model pre-trained on\\nhigh-quality data could achieve strong performance as well.\\n‚Ä¢ We share our findings from math training experiments. Code training prior to math\\ntraining improves models‚Äô ability to solve mathematical problems both with and without\\ntool use. This offers a partial answer to the long-standing question: does code training\\nimprove reasoning abilities?We believe it does, at least for mathematical reasoning.\\n‚Ä¢ Although training on arXiv papers is common, especially in many math-related papers, it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='improve reasoning abilities?We believe it does, at least for mathematical reasoning.\\n‚Ä¢ Although training on arXiv papers is common, especially in many math-related papers, it\\nbrings no notable improvements on all mathematical benchmarks adopted in this paper.\\nExploration and Analysis of Reinforcement Learning\\n‚Ä¢ We introduce Group Relative Policy Optimization (GRPO), an efficient and effective\\nreinforcement learning algorithm. GRPO foregoes the critic model, instead estimating\\nthe baseline from group scores, significantly reducing training resources compared to\\nProximal Policy Optimization (PPO).\\n‚Ä¢ We demonstrate that GRPO significantly enhances the performance of our instruction-\\ntuned model DeepSeekMath-Instruct, by solely using the instruction-tuning data. Further-\\nmore, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 2, 'page_label': '3', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='more, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,\\nPPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training,\\noutcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so\\non to deeply investigate the essential elements of this paradigm.\\n‚Ä¢ Based on our unified paradigm, we explore the reasons behind the effectiveness of rein-\\nforcement learning, and summarize several potential directions to achieve more effective\\nreinforcement learning of LLMs.\\n1.2. Summary of Evaluations and Metrics\\n‚Ä¢ English and Chinese Mathematical Reasoning: We conduct comprehensive assessments\\nof our models on English and Chinese benchmarks, covering mathematical problems\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='from grade-school level to college level. English benchmarks include GSM8K (Cobbe\\net al., 2021), MATH (Hendrycks et al., 2021), SAT (Azerbayev et al., 2023), OCW Courses\\n(Lewkowycz et al., 2022a), MMLU-STEM (Hendrycks et al., 2020). Chinese benchmarks\\ninclude MGSM-zh (Shi et al., 2023), CMATH (Wei et al., 2023), Gaokao-MathCloze (Zhong\\net al., 2023), and Gaokao-MathQA (Zhong et al., 2023). We evaluate models‚Äô ability\\nto generate self-contained text solutions without tool use, and also the ability to solve\\nproblems using Python.\\nOn English benchmarks, DeepSeekMath-Base is competitive with the closed-source Min-\\nerva 540B (Lewkowycz et al., 2022a), and surpasses all open-source base models (e.g., Mis-\\ntral 7B (Jiang et al., 2023) and Llemma-34B (Azerbayev et al., 2023)), regardless of whether\\nthey‚Äôve undergone math pre-training or not, often by a significant margin. Notably,\\nDeepSeekMath-Base is superior on Chinese benchmarks, likely because we don‚Äôt follow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='they‚Äôve undergone math pre-training or not, often by a significant margin. Notably,\\nDeepSeekMath-Base is superior on Chinese benchmarks, likely because we don‚Äôt follow\\nprevious works (Azerbayev et al., 2023; Lewkowycz et al., 2022a) to collect English-only\\nmath pre-training data, and also include high-quality non-English ones. With mathemati-\\ncal instruction tuning and reinforcement learning, the resulting DeepSeekMath-Instruct\\nand DeepSeekMath-RL demonstrate strong performance, obtaining an accuracy of over\\n50% on the competition-level MATH dataset for the first time within the open-source\\ncommunity.\\n‚Ä¢ Formal Mathematics: We evaluate DeepSeekMath-Base using the informal-to-formal\\ntheorem proving task from (Jiang et al., 2022) on miniF2F (Zheng et al., 2021) with Isabelle\\n(Wenzel et al., 2008) chosen to be the proof assistant. DeepSeekMath-Base demonstrates\\nstrong few-shot autoformalization performance.\\n‚Ä¢ Natural Language Understanding, Reasoning, and Code : To build a comprehensive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='strong few-shot autoformalization performance.\\n‚Ä¢ Natural Language Understanding, Reasoning, and Code : To build a comprehensive\\nprofile of models‚Äô general understanding, reasoning, and coding capabilities, we eval-\\nuate DeepSeekMath-Base on the Massive Multitask Language Understanding (MMLU)\\nbenchmark (Hendrycks et al., 2020) which encompasses 57 multiple-choice tasks covering\\ndiverse subjects, BIG-Bench Hard (BBH) (Suzgun et al., 2022) which consists of 23 chal-\\nlenging tasks that mostly require multi-step reasoning to solve, as well as HumanEval\\n(Chen et al., 2021) and MBPP (Austin et al., 2021) which are widely used to evaluate code\\nlanguage models. Math pre-training benefits both language understanding and reasoning\\nperformance.\\n2. Math Pre-Training\\n2.1. Data Collection and Decontamination\\nIn this section, we will outline the process of constructing the DeepSeekMath Corpus from\\nCommon Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='In this section, we will outline the process of constructing the DeepSeekMath Corpus from\\nCommon Crawl. As depicted in Figure 2, we present an iterative pipeline that demonstrates\\nhow to systematically gather a large-scale mathematical corpus from Common Crawl, starting\\nwith a seed corpus (e.g., a small but high-quality collection of math-related dataset). It‚Äôs worth\\nnoting that this approach is also applicable to other domains, such as coding.\\nFirst, we choose OpenWebMath (Paster et al., 2023), a collection of high-quality mathematical\\nweb texts, as our initial seed corpus. Using this corpus, we train a fastText model (Joulin et al.,\\n2016) to recall more OpenWebMath-like mathematical web pages. Specifically, we randomly\\nselect 500,000 data points from the seed corpus as positive training examples and another\\n500,000 web pages from Common Crawl as negative ones. We employ an open-source library1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 3, 'page_label': '4', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='select 500,000 data points from the seed corpus as positive training examples and another\\n500,000 web pages from Common Crawl as negative ones. We employ an open-source library1\\nfor training, configuring the vector dimension to 256, learning rate to 0.1, the maximum length\\n1https://fasttext.cc\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Math Seed\\n Math Corpus\\n1. Train a FastTextModel 2. Recall Math-Related Webpages From Common Crawl\\n3. Discover Math-Related Domains4. Annotate Math-Related URL Path From Labelers\\nDeduplicated Common Crawl40B HTML pages\\nFigure 2 |An iterative pipeline that collects mathematical web pages from Common Crawl.\\nof word n-gram to 3, the minimum number of word occurrences to 3, and the number of\\ntraining epochs to 3. To reduce the size of the original Common Crawl, we employ URL-based\\ndeduplication and near-deduplication techniques, resulting in 40B HTML web pages. We then\\nrecall mathematical web pages from deduplicated Common Crawl with the fastText model.\\nTo filter out low-quality mathematical content, we rank the collected pages according to their\\nscores predicted by the fastText model, and only preserve the top-ranking ones. The volume\\nof data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='scores predicted by the fastText model, and only preserve the top-ranking ones. The volume\\nof data preserved is assessed through pre-training experiments on the top 40B, 80B, 120B, and\\n160B tokens. In the first iteration, we choose to keep the top 40B tokens.\\nAfter the first iteration of data collection, numerous mathematical web pages remain un-\\ncollected, mainly because the fastText model is trained on a set of positive examples that lacks\\nsufficient diversity. We therefore identify additional mathematical web sources to enrich the seed\\ncorpus, so that we can optimize the fastText model. Specifically, we first organize the entire Com-\\nmon Crawl into disjoint domains; a domain is defined as web pages sharing the same base URL.\\nFor each domain, we calculate the percentage of web pages that are collected in the first iteration.\\nDomains where over 10% of the web pages have been collected are classified as math-related'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='For each domain, we calculate the percentage of web pages that are collected in the first iteration.\\nDomains where over 10% of the web pages have been collected are classified as math-related\\n(e.g., mathoverflow.net ). Subsequently, we manually annotate the URLs associated with\\nmathematical content within these identified domains (e.g., mathoverflow.net/questions).\\nWeb pages linked to these URLs, yet uncollected, will be added to the seed corpus. This ap-\\nproach enables us to gather more positive examples, thereby training an improved fastText\\nmodel capable of recalling more mathematical data in the subsequent iteration. After four\\niterations of data collection, we end up with 35.5M mathematical web pages, totaling 120B\\ntokens. In the fourth iteration, we notice that nearly 98% of the data has already been collected\\nin the third iteration, so we decide to cease data collection.\\nTo avoid benchmark contamination, we follow Guo et al. (2024) to filter out web pages'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 4, 'page_label': '5', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='in the third iteration, so we decide to cease data collection.\\nTo avoid benchmark contamination, we follow Guo et al. (2024) to filter out web pages\\ncontaining questions or answers from English mathematical benchmarks such as GSM8K (Cobbe\\net al., 2021) and MATH (Hendrycks et al., 2021) and Chinese benchmarks such as CMATH\\n(Wei et al., 2023) and AGIEval (Zhong et al., 2023). The filtering criteria are as follows: any\\ntext segment containing a 10-gram string that matches exactly with any sub-string from the\\nevaluation benchmarks is removed from our math training corpus. For benchmark texts that\\nare shorter than 10 grams but have at least 3 grams, we employ exact matching to filter out\\ncontaminated web pages.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2.2. Validating the Quality of the DeepSeekMath Corpus\\nWe run pre-training experiments to investigate how the DeepSeekMath Corpus is compared\\nwith the recently released math-training corpora:\\n‚Ä¢ MathPile (Wang et al., 2023c): a multi-source corpus (8.9B tokens) aggregated from\\ntextbooks, Wikipedia, ProofWiki, CommonCrawl, StackExchange, and arXiv, with the\\nmajority (over 85%) sourced from arXiv;\\n‚Ä¢ OpenWebMath (Paster et al., 2023): CommonCrawl data filtered for mathematical content,\\ntotaling 13.6B tokens;\\n‚Ä¢ Proof-Pile-2 (Azerbayev et al., 2023): a mathematical corpus consisting of OpenWeb-\\nMath, AlgebraicStack (10.3B tokens of mathematical code), and arXiv papers (28.0B to-\\nkens). When experimenting on Proof-Pile-2, we follow Azerbayev et al. (2023) to use an\\narXiv:Web:Code ratio of 2:4:1.\\n2.2.1. Training Setting\\nWe apply math training to a general pre-trained language model with 1.3B parameters, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='arXiv:Web:Code ratio of 2:4:1.\\n2.2.1. Training Setting\\nWe apply math training to a general pre-trained language model with 1.3B parameters, which\\nshares the same framework as the DeepSeek LLMs (DeepSeek-AI, 2024), denoted as DeepSeek-\\nLLM 1.3B. We separately train a model on each mathematical corpus for 150B tokens. All\\nexperiments are conducted using the efficient and light-weight HAI-LLM (High-flyer, 2023)\\ntraining framework. Following the training practice of DeepSeek LLMs, we use the AdamW\\noptimizer (Loshchilov and Hutter, 2017) with ùõΩ1 = 0.9, ùõΩ2 = 0.95, and weight_decay = 0.1, along\\nwith a multi-step learning rate schedule where the learning rate reaches the peak after 2,000\\nwarmup steps, decreases to its 31.6% after 80% of the training process, and further decreases to\\n10.0% of the peak after 90% of the training process. We set the maximum value of learning rate\\nto 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\\nMath Corpus Size'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='10.0% of the peak after 90% of the training process. We set the maximum value of learning rate\\nto 5.3e-4, and use a batch size of 4M tokens with a 4K context length.\\nMath Corpus Size\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SAT MMLU\\nSTEM CMATH Gaokao\\nMathCloze\\nGaokao\\nMathQA\\nNo Math Training N/A 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9%\\nMathPile 8.9B 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8%\\nOpenWebMath 13.6B 11.5% 8.9% 3.7% 31.3% 29.6% 16.8% 0.0% 14.2%\\nProof-Pile-2 51.9B 14.3% 11.2% 3.7% 43.8% 29.2% 19.9% 5.1% 11.7%\\nDeepSeekMath Corpus120.2B 23.8% 13.6% 4.8% 56.3% 33.1% 41.5% 5.9% 23.6%\\nTable 1 |Performance of DeepSeek-LLM 1.3B trained on different mathematical corpora, evalu-\\nated using few-shot chain-of-thought prompting. Corpus sizes are calculated using our tokenizer\\nwith a vocabulary size of 100K.\\n2.2.2. Evaluation Results\\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and\\nis the largest in size.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 5, 'page_label': '6', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='with a vocabulary size of 100K.\\n2.2.2. Evaluation Results\\nThe DeepSeekMath Corpus is of high quality, covers multilingual mathematical content, and\\nis the largest in size.\\n‚Ä¢ High-quality: We evaluate downstream performance on 8 mathematical benchmarks using\\nfew-shot chain-of-thought prompting Wei et al. (2022). As shown in Table 1, there is a clear\\nperformance lead of the model trained on the DeepSeekMath Corpus. Figure 3 shows that\\nthe model trained on the DeepSeekMath Corpus demonstrates better performance than\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Figure 3 |Benchmark curves of DeepSeek-LLM 1.3B trained on different mathematical corpora.\\nProof-Pile-2 at 50B tokens (1 full epoch of Proof-Pile-2), indicating the average quality of\\nDeepSeekMath Corpus is higher.\\n‚Ä¢ Multilingual: The DeepSeekMath Corpus encompasses data in multiple languages, pre-\\ndominantly featuring English and Chinese as the two most represented languages. As\\nshown in Table 1, training on the DeepSeekMath Corpus enhances mathematical reasoning\\nperformance in both English and Chinese. In contrast, existing mathematical corpora,\\nwhich are primarily English-centric, show limited improvement and may even hinder\\nperformance in Chinese mathematical reasoning.\\n‚Ä¢ Large-scale: The DeepSeekMath Corpus is several times larger than existing mathematical\\ncorpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-\\nMath Corpus, shows a steeper learning curve along with more lasting improvements. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 6, 'page_label': '7', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='corpora. As depicted in Figure 3, DeepSeek-LLM 1.3B, when trained on the DeepSeek-\\nMath Corpus, shows a steeper learning curve along with more lasting improvements. In\\ncontrast, the baseline corpora are much smaller, and have already been repeated multiple\\nrounds during training, with the resulting model performance quickly reaching a plateau.\\n2.3. Training and Evaluating DeepSeekMath-Base 7B\\nIn this section, we introduce DeepSeekMath-Base 7B, a base model with strong reasoning\\nabilities, especially in mathematics. Our model is initialized with DeepSeek-Coder-Base-v1.5 7B\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='(Guo et al., 2024) and trained for 500B tokens. The distribution of the data is as follows: 56%\\nis from the DeepSeekMath Corpus, 4% from AlgebraicStack, 10% from arXiv, 20% is Github\\ncode, and the remaining 10% is natural language data from Common Crawl in both English and\\nChinese. We mainly adopt the training setting specified in Section 2.2.1, except that we set the\\nmaximum value of the learning rate to 4.2e-4 and use a batch size of 10M tokens.\\nWe conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-\\nBase 7B, focusing on its ability to produce self-contained mathematical solutions without relying\\non external tools, solve mathematical problems using tools, and conduct formal theorem proving.\\nBeyond mathematics, we also provide a more general profile of the base model, including its\\nperformance of natural language understanding, reasoning, and programming skills.\\nMathematical Problem Solving with Step-by-Step Reasoning We evaluate DeepSeekMath-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='performance of natural language understanding, reasoning, and programming skills.\\nMathematical Problem Solving with Step-by-Step Reasoning We evaluate DeepSeekMath-\\nBase‚Äôs performance of solving mathematical problems using few-shot chain-of-thought prompt-\\ning (Wei et al., 2022), across eight benchmarks in English and Chinese. These benchmarks encom-\\npass quantitative reasoning (e.g., GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021),\\nand CMATH (Wei et al., 2023)) and multiple-choice problems (e.g., MMLU-STEM (Hendrycks\\net al., 2020) and Gaokao-MathQA (Zhong et al., 2023)), covering diverse fields of mathematics\\nfrom elementary to college-level complexity.\\nAs shown in Table 2, DeepSeekMath-Base 7B leads in performance across all eight bench-\\nmarks among the open-source base models (including the widely-used general model Mistral\\n7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='marks among the open-source base models (including the widely-used general model Mistral\\n7B (Jiang et al., 2023) and the recently released Llemma 34B (Azerbayev et al., 2023) which\\nunderwent math training on Proof-Pile-2 (Azerbayev et al., 2023)). Notably, on the competition-\\nlevel MATH dataset, DeepSeekMath-Base surpasses existing open-source base models by over\\n10% absolute, and outperforms Minerva 540B (Lewkowycz et al., 2022a), a closed-source base\\nmodel 77 times larger which builds on PaLM (Lewkowycz et al., 2022b) and is further trained\\non mathematical texts.\\nModel Size\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SAT MMLU\\nSTEM CMATH Gaokao\\nMathCloze\\nGaokao\\nMathQA\\nClosed-Source Base Model\\nMinerva 7B 16.2% 14.1% 7.7% - 35.6% - - -\\nMinerva 62B 52.4% 27.6% 12.0% - 53.9% - - -\\nMinerva 540B 58.8% 33.6% 17.6% - 63.9% - - -\\nOpen-Source Base Model\\nMistral 7B 40.3% 14.3% 9.2% 71.9% 51.1% 44.9% 5.1% 23.4%\\nLlemma 7B 37.4% 18.1% 6.3% 59.4% 43.1% 43.4% 11.9% 23.6%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 7, 'page_label': '8', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Minerva 540B 58.8% 33.6% 17.6% - 63.9% - - -\\nOpen-Source Base Model\\nMistral 7B 40.3% 14.3% 9.2% 71.9% 51.1% 44.9% 5.1% 23.4%\\nLlemma 7B 37.4% 18.1% 6.3% 59.4% 43.1% 43.4% 11.9% 23.6%\\nLlemma 34B 54.0% 25.3% 10.3% 71.9% 52.9% 56.1% 11.9% 26.2%\\nDeepSeekMath-Base 7B 64.2% 36.2% 15.4% 84.4% 56.5% 71.7% 20.3% 35.3%\\nTable 2 |Comparisons between DeepSeekMath-Base 7B and strong base models on English and\\nChinese mathematical benchmarks. Models are evaluated with chain-of-thought prompting.\\nMinerva results are quoted from Lewkowycz et al. (2022a).\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Mathematical Problem Solving with Tool Use We evaluate program-aided mathematical\\nreasoning on GSM8K and MATH using few-shot program-of-thought prompting (Chen et al.,\\n2022; Gao et al., 2023). Models are prompted to solve each problem by writing a Python program\\nwhere libraries such as math and sympy can be utilized for intricate computations. The execution\\nresult of the program is evaluated as the answer. As shown in Table 3, DeepSeekMath-Base 7B\\noutperforms the prior state-of-the-art Llemma 34B.\\nModel Size Problem Solving w/ Tools Informal-to-Formal Proving\\nGSM8K+Python MATH+Python miniF2F-valid miniF2F-test\\nMistral 7B 48.5% 18.2% 18.9% 18.0%\\nCodeLlama 7B 27.1% 17.2% 16.3% 17.6%\\nCodeLlama 34B 52.7% 23.5% 18.5% 18.0%\\nLlemma 7B 41.0% 18.6% 20.6% 22.1%\\nLlemma 34B 64.6% 26.3% 21.0% 21.3%\\nDeepSeekMath-Base 7B 66.9% 31.4% 25.8% 24.6%\\nTable 3 |Few-shot evaluation of base models‚Äô ability to solve mathematical problems using tools'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Llemma 34B 64.6% 26.3% 21.0% 21.3%\\nDeepSeekMath-Base 7B 66.9% 31.4% 25.8% 24.6%\\nTable 3 |Few-shot evaluation of base models‚Äô ability to solve mathematical problems using tools\\nand the ability to conduct informal-to-formal theorem proving in Isabelle.\\nFormal Mathematics Formal proof automation is beneficial to ensure the accuracy and relia-\\nbility of mathematical proofs and enhance efficiency, with increasing attention in recent years.\\nWe evaluate DeepSeekMath-Base 7B on the task of informal-to-formal proving from (Jiang et al.,\\n2022) which is to generate a formal proof based on an informal statement, a formal counterpart\\nof the statement, and an informal proof. We evaluate on miniF2F (Zheng et al., 2021), a bench-\\nmark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each\\nproblem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mark for formal Olympiad-level mathematics, and generate a formal proof in Isabelle for each\\nproblem with few-shot prompting. Following Jiang et al. (2022), we leverage models to generate\\nproof sketches, and execute the off-the-shelf automated prover Sledgehammer (Paulson, 2010)\\nto fill in the missing details. As shown in Table 3, DeepSeekMath-Base 7B demonstrates strong\\nperformance in proof autoformalization.\\nModel Size MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)\\nMistral 7B 62.4% 55.7% 28.0% 41.4%\\nDeepSeek-Coder-Base-v1.5‚Ä† 7B 42.9% 42.9% 40.2% 52.6%\\nDeepSeek-Coder-Base-v1.5 7B 49.1% 55.2% 43.2% 60.4%\\nDeepSeekMath-Base 7B 54.9% 59.5% 40.9% 52.6%\\nTable 4 |Evaluation on natural language understanding, reasoning, and code benchmarks.\\nDeepSeek-Coder-Base-v1.5‚Ä†is the checkpoint right before learning rate decay, which is used to\\ntrain DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 8, 'page_label': '9', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='DeepSeek-Coder-Base-v1.5‚Ä†is the checkpoint right before learning rate decay, which is used to\\ntrain DeepSeekMath-Base. On MMLU and BBH, we use few-shot chain-of-thought prompting.\\nOn HumanEval and MBPP , we evaluate model performance under the zero-shot setting and a\\nfew-shot setting, respectively.\\nNatural Language Understanding, Reasoning, and Code We evaluate model performance of\\nnatural language understanding on MMLU (Hendrycks et al., 2020), reasoning on BBH (Suzgun\\net al., 2022), and coding capabilities on HumanEval (Chen et al., 2021) and MBPP (Austin et al.,\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2021). As shown in Table 4, DeepSeekMath-Base 7B exhibits significant enhancements in per-\\nformance on MMLU and BBH over its precursor, DeepSeek-Coder-Base-v1.5 (Guo et al., 2024),\\nillustrating the positive impact of math training on language understanding and reasoning.\\nAdditionally, by including code tokens for continual training, DeepSeekMath-Base 7B effectively\\nmaintains the performance of DeepSeek-Coder-Base-v1.5 on the two coding benchmarks. Over-\\nall, DeepSeekMath-Base 7B significantly outperforms the general model Mistral 7B (Jiang et al.,\\n2023) on the three reasoning and coding benchmarks.\\n3. Supervised Fine-T uning\\n3.1. SFT Data Curation\\nWe construct a mathematical instruction-tuning dataset covering English and Chinese problems\\nfrom different mathematical fields and of varying complexity levels: problems are paired with\\nsolutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='from different mathematical fields and of varying complexity levels: problems are paired with\\nsolutions in chain-of-thought (CoT) (Wei et al., 2022), program-of-thought (PoT) (Chen et al.,\\n2022; Gao et al., 2023), and tool-integrated reasoning format (Gou et al., 2023). The total number\\nof training examples is 776K.\\n‚Ä¢ English mathematical datasets: We annotate GSM8K and MATH problems with tool-\\nintegrated solutions, and adopt a subset of MathInstruct (Yue et al., 2023) along with the\\ntraining set of Lila-OOD (Mishra et al., 2022) where problems are solved with CoT or\\nPoT. Our English collection covers diverse fields of mathematics, e.g., algebra, probability,\\nnumber theory, calculus, and geometry.\\n‚Ä¢ Chinese mathematical datasets: We collect Chinese K-12 mathematical problems spanning\\n76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-\\nintegrated reasoning format.\\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='76 sub-topics such as linear equations, with solutions annotated in both CoT and tool-\\nintegrated reasoning format.\\n3.2. Training and Evaluating DeepSeekMath-Instruct 7B\\nIn this section, we introduce DeepSeekMath-Instruct 7B which undergoes mathematical instruc-\\ntion tuning based on DeepSeekMath-Base. Training examples are randomly concatenated until\\nreaching a maximum context length of 4K tokens. We train the model for 500 steps with a batch\\nsize of 256 and a constant learning rate of 5e-5.\\nWe evaluate models‚Äô mathematical performance both without and with tool use, on 4\\nquantitative reasoning benchmarks in English and Chinese. We benchmark our model against\\nthe leading models of the time:\\n‚Ä¢ Closed-source models include: (1) the GPT family among which GPT-4 (OpenAI, 2023)\\nand GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil\\net al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1 3, as well as models recently'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 9, 'page_label': '10', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='and GPT-4 Code Interpreter 2 are the most capable ones, (2) Gemini Ultra and Pro (Anil\\net al., 2023), (3) Inflection-2 (Inflection AI, 2023), (4) Grok-1 3, as well as models recently\\nreleased by Chinese companies including (5) Baichuan-3 4, (6) the latest GLM-4 5 from the\\nGLM family (Du et al., 2022). These models are for general purposes, most of which have\\nundergone a series of alignment procedures.\\n‚Ä¢ Open-source models include: general models like (1) DeepSeek-LLM-Chat 67B (DeepSeek-\\nAI, 2024), (2) Qwen 72B (Bai et al., 2023), (3) SeaLLM-v2 7B (Nguyen et al., 2023), and (4)\\n2https://openai.com/blog/chatgpt-plugins#code-interpreter\\n3https://x.ai/model-card\\n4https://www.baichuan-ai.com\\n5https://open.bigmodel.cn/dev/api#glm-4\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ChatGLM3 6B (ChatGLM3 Team, 2023), as well as models with enhancements in mathemat-\\nics including (5) InternLM2-Math 20B 6 which builds on InternLM2 and underwent math\\ntraining followed by instruction tuning, (6) Math-Shepherd-Mistral 7B which applys PPO\\ntraining (Schulman et al., 2017) to Mistral 7B (Jiang et al., 2023) with a process-supervised\\nreward model, (7) the WizardMath series (Luo et al., 2023) which improves mathematical\\nreasoning in Mistral 7B and Llama-2 70B (Touvron et al., 2023) using evolve-instruct (i.e.,\\na version of instruction tuning that uses AI-evolved instructions) and PPO training with\\ntraining problems primarily sourced from GSM8K and MATH, (8) MetaMath 70B (Yu et al.,\\n2023) which is Llama-2 70B fine-tuned on an augmented version of GSM8K and MATH,\\n(9) ToRA 34B Gou et al. (2023) which is CodeLlama 34B fine-tuned to do tool-integrated\\nmathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B\\ninstruction-tuned on MathInstruct.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mathematical reasoning, (10) MAmmoTH 70B (Yue et al., 2023) which is Llama-2 70B\\ninstruction-tuned on MathInstruct.\\nAs shown in Table 5, under the evaluation setting where tool use is disallowed, DeepSeekMath-\\nInstruct 7B demonstrates strong performance of step-by-step reasoning. Notably, on the\\ncompetition-level MATH dataset, our model surpasses all open-source models and the ma-\\njority of proprietary models (e.g., Inflection-2 and Gemini Pro) by at least 9% absolute. This\\nis true even for models that are substantially larger (e.g., Qwen 72B) or have been specifi-\\ncally enhanced through math-focused reinforcement learning (e.g., WizardMath-v1.1 7B). While\\nDeepSeekMath-Instruct rivals the Chinese proprietary models GLM-4 and Baichuan-3 on MATH,\\nit still underperforms GPT-4 and Gemini Ultra.\\nUnder the evaluation setting where models are allowed to integrate natural language rea-\\nsoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Under the evaluation setting where models are allowed to integrate natural language rea-\\nsoning and program-based tool use for problem solving, DeepSeekMath-Instruct 7B approaches\\nan accuracy of 60% on MATH, surpassing all existing open-source models. On the other bench-\\nmarks, our model is competitive with DeepSeek-LLM-Chat 67B, the prior state-of-the-art that is\\n10 times larger.\\n4. Reinforcement Learning\\n4.1. Group Relative Policy Optimization\\nReinforcement learning (RL) has been proven to be effective in further improving the mathe-\\nmatical reasoning ability of LLMs after the Supervised Fine-Tuning (SFT) stage (Luo et al., 2023;\\nWang et al., 2023b). In this section, we introduce our efficient and effective RL algorithm, Group\\nRelative Policy Optimization (GRPO).\\n4.1.1. From PPO to GRPO\\nProximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is\\nwidely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 10, 'page_label': '11', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic RL algorithm that is\\nwidely used in the RL fine-tuning stage of LLMs (Ouyang et al., 2022). In particular, it optimizes\\nLLMs by maximizing the following surrogate objective:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉ(ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n, (1)\\nwhere ùúãùúÉ and ùúãùúÉùëúùëôùëë are the current and old policy models, and ùëû, ùëúare questions and outputs\\nsampled from the question dataset and the old policy ùúãùúÉùëúùëôùëë, respectively. ùúÄis a clipping-related\\nhyper-parameter introduced in PPO for stabilizing training. ùê¥ùë° is the advantage, which is\\ncomputed by applying Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based\\n6https://github.com/InternLM/InternLM-Math\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Model Size English Benchmarks Chinese Benchmarks\\nGSM8K MATH MGSM-zh CMATH\\nChain-of-Thought Reasoning\\nClosed-Source Model\\nGemini Ultra - 94.4% 53.2% - -\\nGPT-4 - 92.0% 52.9% - 86.0%\\nInflection-2 - 81.4% 34.8% - -\\nGPT-3.5 - 80.8% 34.1% - 73.8%\\nGemini Pro - 86.5% 32.6% - -\\nGrok-1 - 62.9% 23.9% - -\\nBaichuan-3 - 88.2% 49.2% - -\\nGLM-4 - 87.6% 47.9% - -\\nOpen-Source Model\\nInternLM2-Math 20B 82.6% 37.7% - -\\nQwen 72B 78.9% 35.2% - -\\nMath-Shepherd-Mistral 7B 84.1% 33.0% - -\\nWizardMath-v1.1 7B 83.2% 33.0% - -\\nDeepSeek-LLM-Chat 67B 84.1% 32.6% 74.0% 80.3%\\nMetaMath 70B 82.3% 26.6% 66.4% 70.9%\\nSeaLLM-v2 7B 78.2% 27.5% 64.8% -\\nChatGLM3 6B 72.3% 25.7% - -\\nWizardMath-v1.0 70B 81.6% 22.7% 64.8% 65.4%\\nDeepSeekMath-Instruct 7B 82.9% 46.8% 73.2% 84.6%\\nDeepSeekMath-RL 7B 88.2% 51.7% 79.6% 88.8%\\nTool-Integrated Reasoning\\nClosed-Source Model\\nGPT-4 Code Interpreter - 97.0% 69.7% - -\\nOpen-Source Model\\nInternLM2-Math 20B 80.7% 54.3% - -\\nDeepSeek-LLM-Chat 67B 86.7% 51.1% 76.4% 85.4%\\nToRA 34B 80.7% 50.8% 41.2% 53.4%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 11, 'page_label': '12', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Closed-Source Model\\nGPT-4 Code Interpreter - 97.0% 69.7% - -\\nOpen-Source Model\\nInternLM2-Math 20B 80.7% 54.3% - -\\nDeepSeek-LLM-Chat 67B 86.7% 51.1% 76.4% 85.4%\\nToRA 34B 80.7% 50.8% 41.2% 53.4%\\nMAmmoTH 70B 76.9% 41.8% - -\\nDeepSeekMath-Instruct 7B 83.7% 57.4% 72.0% 84.3%\\nDeepSeekMath-RL 7B 86.7% 58.8% 78.4% 87.6%\\nTable 5 |Performance of Open- and Closed-Source models with both Chain-of-Thought and\\nTool-Integrated Reasoning on English and Chinese Benchmarks. Scores in gray denote majority\\nvotes with 32 candidates; The others are Top1 scores. DeepSeekMath-RL 7B beats all open-\\nsource models from 7B to 70B, as well as the majority of closed-source models. Although\\nDeepSeekMath-RL 7B is only further trained on chain-of-thought-format instruction tuning data\\nof GSM8K and MATH, it improves over DeepSeekMath-Instruct 7B on all benchmarks.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëûùëû\\nùëúùëú!\\nùëúùëú\"\\nùëúùëú#\\nùëüùëü!\\nùëüùëü\"\\nùëüùëü#\\nùê¥ùê¥!\\nùê¥ùê¥\"\\nùê¥ùê¥#\\nùëûùëû ùëúùëú GAE ùê¥ùê¥\\nùëüùëü\\nùë£ùë£\\nReward \\nModel\\nPolicy \\nModel\\nValue \\nModel\\n‚Ä¶ ‚Ä¶ ‚Ä¶\\nPolicy \\nModel\\nReference \\nModel\\nReward \\nModel\\nPPO\\nGRPO\\nTrained\\nModels\\nFrozen\\nModelsReference \\nModel\\n‚äï\\nùêæùêæùêæùêæ\\nùêæùêæùêæùêæ\\nGroup \\nComputation\\nFigure 4 |Demonstration of PPO and our GRPO. GRPO foregoes the value model, instead\\nestimating the baseline from group scores, significantly reducing training resources.\\non the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì. Thus, in PPO, a value function needs to\\nbe trained alongside the policy model and to mitigate over-optimization of the reward model,\\nthe standard approach is to add a per-token KL penalty from a reference model in the reward at\\neach token (Ouyang et al., 2022), i.e.,\\nùëüùë° = ùëüùúë(ùëû, ùëú‚â§ùë°)‚àíùõΩlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùëüùëíùëì (ùëúùë°|ùëû, ùëú<ùë°), (2)\\nwhere ùëüùúë is the reward model, ùúãùëüùëíùëì is the reference model, which is usually the initial SFT model,\\nand ùõΩ is the coefficient of the KL penalty.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëüùë° = ùëüùúë(ùëû, ùëú‚â§ùë°)‚àíùõΩlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùëüùëíùëì (ùëúùë°|ùëû, ùëú<ùë°), (2)\\nwhere ùëüùúë is the reward model, ùúãùëüùëíùëì is the reference model, which is usually the initial SFT model,\\nand ùõΩ is the coefficient of the KL penalty.\\nAs the value function employed in PPO is typically another model of comparable size as\\nthe policy model, it brings a substantial memory and computational burden. Additionally,\\nduring RL training, the value function is treated as a baseline in the calculation of the advantage\\nfor variance reduction. While in the LLM context, usually only the last token is assigned a\\nreward score by the reward model, which may complicate the training of a value function that is\\naccurate at each token. To address this, as shown in Figure 4, we propose Group Relative Policy\\nOptimization (GRPO), which obviates the need for additional value function approximation as\\nin PPO, and instead uses the average reward of multiple sampled outputs, produced in response'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Optimization (GRPO), which obviates the need for additional value function approximation as\\nin PPO, and instead uses the average reward of multiple sampled outputs, produced in response\\nto the same question, as the baseline. More specifically, for each question ùëû, GRPO samples a\\ngroup of outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}from the old policy ùúãùúÉùëúùëôùëë and then optimizes the policy model\\nby maximizing the following objective:\\nJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉ(ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x1a\\nmin\\n\\x14 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nÀÜùê¥ùëñ,ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nÀÜùê¥ùëñ,ùë°\\n\\x15\\n‚àíùõΩDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\x1b\\n,\\n(3)\\nwhere ùúÄ and ùõΩ are hyper-parameters, and ÀÜùê¥ùëñ,ùë° is the advantage calculated based on relative\\nrewards of the outputs inside each group only, which will be detailed in the following subsec-\\ntions. The group relative way that GRPO leverages to calculate the advantages, aligns well with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 12, 'page_label': '13', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='rewards of the outputs inside each group only, which will be detailed in the following subsec-\\ntions. The group relative way that GRPO leverages to calculate the advantages, aligns well with\\nthe comparative nature of rewards models, as reward models are typically trained on datasets\\nof comparisons between outputs on the same question. Also note that, instead of adding KL\\npenalty in the reward, GRPO regularizes by directly adding the KL divergence between the\\ntrained policy and the reference policy to the loss, avoiding complicating the calculation of ÀÜùê¥ùëñ,ùë°.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Algorithm 1 Iterative Group Relative Policy Optimization\\nInput initial policy model ùúãùúÉinit ; reward models ùëüùúë; task prompts D; hyperparameters ùúÄ, ùõΩ, ùúá\\n1: policy model ùúãùúÉ ‚ÜêùúãùúÉinit\\n2: for iteration = 1, . . . , Ido\\n3: reference model ùúãùëüùëíùëì ‚ÜêùúãùúÉ\\n4: for step = 1, . . . , Mdo\\n5: Sample a batch Dùëè from D\\n6: Update the old policy model ùúãùúÉùëúùëôùëë ‚ÜêùúãùúÉ\\n7: Sample ùê∫ outputs {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (¬∑| ùëû)for each question ùëû ‚ààDùëè\\n8: Compute rewards {ùëüùëñ}ùê∫\\nùëñ=1 for each sampled output ùëúùëñ by running ùëüùúë\\n9: Compute ÀÜùê¥ùëñ,ùë° for the ùë°-th token of ùëúùëñ through group relative advantage estimation.\\n10: for GRPO iteration = 1, . . . ,ùúá do\\n11: Update the policy model ùúãùúÉ by maximizing the GRPO objective (Equation 21)\\n12: Update ùëüùúë through continuous training using a replay mechanism.\\nOutput ùúãùúÉ\\nAnd different from the KL penalty term used in (2), we estimate the KL divergence with the\\nfollowing unbiased estimator (Schulman, 2020):\\nDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\n=\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='following unbiased estimator (Schulman, 2020):\\nDùêæùêø\\n\\x02\\nùúãùúÉ||ùúãùëüùëíùëì\\n\\x03\\n=\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àí1, (4)\\nwhich is guaranteed to be positive.\\n4.1.2. Outcome Supervision RL with GRPO\\nFormally, for each question ùëû, a group of outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}are sampled from the old\\npolicy model ùúãùúÉùëúùëôùëë. A reward model is then used to score the outputs, yielding ùê∫ rewards\\nr = {ùëü1, ùëü2, ¬∑¬∑¬∑ , ùëüùê∫}correspondingly. Subsequently, these rewards are normalized by subtracting\\nthe group average and dividing by the group standard deviation. Outcome supervision provides\\nthe normalized reward at the end of each output ùëúùëñ and sets the advantages ÀÜùê¥ùëñ,ùë° of all tokens in\\nthe output as the normalized reward, i.e., ÀÜùê¥ùëñ,ùë° = eùëüùëñ = ùëüùëñ‚àímean(r)\\nstd(r) , and then optimizes the policy by\\nmaximizing the objective defined in equation (3).\\n4.1.3. Process Supervision RL with GRPO\\nOutcome supervision only provides a reward at the end of each output, which may not be'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='maximizing the objective defined in equation (3).\\n4.1.3. Process Supervision RL with GRPO\\nOutcome supervision only provides a reward at the end of each output, which may not be\\nsufficient and efficient to supervise the policy in complex mathematical tasks. Following Wang\\net al. (2023b), we also explore process supervision, which provides a reward at the end of\\neach reasoning step. Formally, given the question ùëûand ùê∫ sampled outputs {ùëú1, ùëú2, ¬∑¬∑¬∑ , ùëúùê∫}, a\\nprocess reward model is used to score each step of the outputs, yielding corresponding rewards:\\nR = {{ùëüùëñùëõùëëùëíùë•(1)\\n1 , ¬∑¬∑¬∑ , ùëüùëñùëõùëëùëíùë•(ùêæ1 )\\n1 }, ¬∑¬∑¬∑ , {ùëüùëñùëõùëëùëíùë•(1)\\nùê∫ , ¬∑¬∑¬∑ , ùëüùëñùëõùëëùëíùë•(ùêæùê∫)\\nùê∫ }}, where ùëñùëõùëëùëíùë•(ùëó)is the end token index\\nof the ùëó-th step, and ùêæùëñ is the total number of steps in the ùëñ-th output. We also normalize these\\nrewards with the average and the standard deviation, i.e.,eùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ =\\nùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ ‚àímean(R)\\nstd(R) . Subsequently,\\nthe process supervision calculates the advantage of each token as the sum of the normalized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 13, 'page_label': '14', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùëñ =\\nùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ ‚àímean(R)\\nstd(R) . Subsequently,\\nthe process supervision calculates the advantage of each token as the sum of the normalized\\nrewards from the following steps, i.e., ÀÜùê¥ùëñ,ùë° = √ç\\nùëñùëõùëëùëíùë•(ùëó)‚â•ùë°eùëüùëñùëõùëëùëíùë•(ùëó)\\nùëñ , and then optimizes the policy by\\nmaximizing the objective defined in equation (3).\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='4.1.4. Iterative RL with GRPO\\nAs the reinforcement learning training process progresses, the old reward model may not be\\nsufficient to supervise the current policy model. Therefore, we also explore the iterative RL\\nwith GRPO. As shown in Algorithm 1, in iterative GRPO, we generate new training sets for the\\nreward model based on the sampling results from the policy model and continually train the\\nold reward model using a replay mechanism that incorporates 10% of historical data. Then, we\\nset the reference model as the policy model, and continually train the policy model with the\\nnew reward model.\\n4.2. Training and Evaluating DeepSeekMath-RL\\nWe conduct RL based on DeepSeekMath-Instruct 7B. The training data of RL are chain-of-\\nthought-format questions related to GSM8K and MATH from the SFT data, which consists\\nof around 144K questions. We exclude other SFT questions to investigate the impact of RL\\non benchmarks that lack data throughout the RL phase. We construct the training set of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='of around 144K questions. We exclude other SFT questions to investigate the impact of RL\\non benchmarks that lack data throughout the RL phase. We construct the training set of\\nreward models following (Wang et al., 2023b). We train our initial reward model based on the\\nDeepSeekMath-Base 7B with a learning rate of 2e-5. For GRPO, we set the learning rate of the\\npolicy model as 1e-6. The KL coefficient is 0.04. For each question, we sample 64 outputs. The\\nmax length is set to 1024, and the training batch size is 1024. The policy model only has a single\\nupdate following each exploration stage. We evaluate DeepSeekMath-RL 7B on benchmarks\\nfollowing DeepSeekMath-Instruct 7B. For DeepSeekMath-RL 7B, GSM8K and MATH with\\nchain-of-thought reasoning can be regarded as in-domain tasks and all the other benchmarks\\ncan be regarded as out-of-domain tasks.\\nTable 5 demonstrates the performance of open- and closed-source models with both chain-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='can be regarded as out-of-domain tasks.\\nTable 5 demonstrates the performance of open- and closed-source models with both chain-\\nof-thought and tool-integrated reasoning on English and Chinese benchmarks. We find that:\\n1) DeepSeekMath-RL 7B attains accuracies of 88.2% and 51.7% on GSM8K and MATH, respec-\\ntively, utilizing chain-of-thought reasoning. This performance surpasses that of all open-source\\nmodels in the 7B to 70B range, as well as the majority of closed-source models. 2) Crucially,\\nDeepSeekMath-RL 7B is only trained on chain-of-thought-format instruction tuning data of\\nGSM8K and MATH, starting from DeepSeekMath-Instruct 7B. Despite the constrained scope\\nof its training data, it outperforms DeepSeekMath-Instruct 7B across all evaluation metrics,\\nshowcasing the effectiveness of reinforcement learning.\\n5. Discussion\\nIn this section, we will share our findings in pre-training and RL experiments.\\n5.1. Lessons Learnt in Pre-Training'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 14, 'page_label': '15', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='showcasing the effectiveness of reinforcement learning.\\n5. Discussion\\nIn this section, we will share our findings in pre-training and RL experiments.\\n5.1. Lessons Learnt in Pre-Training\\nWe first share our experience in pre-training. Unless otherwise specified, we will adhere to\\nthe training settings outlined in Section 2.2.1. It is worth noting that, when referring to the\\nDeepSeekMath Corpus in this section, we use an 89B-token dataset from the second iteration of\\nthe data collection process.\\n5.1.1. Code Training Benefits Mathematical Reasoning\\nA popular yet unverified hypothesis suggests that code training improves reasoning. We attempt\\nto offer a partial response to this, particularly within the mathematical domain: code training\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Training Setting Training Tokens w/o Tool Use w/ Tool Use\\nGeneral Code Math GSM8K MATH CMATH GSM8K+Python MATH+Python\\nNo Continual Training ‚Äì ‚Äì ‚Äì 2.9% 3.0% 12.3% 2.7% 2.3%\\nTwo-Stage Training\\nStage 1: General Training 400B ‚Äì ‚Äì 2.9% 3.2% 14.8% 3.3% 2.3%\\nStage 2: Math Training ‚Äì ‚Äì 150B 19.1% 14.4% 37.2% 14.3% 6.7%\\nStage 1: Code Training ‚Äì 400B ‚Äì 5.9% 3.6% 19.9% 12.4% 10.0%\\nStage 2: Math Training ‚Äì ‚Äì 150B 21.9% 15.3% 39.7% 17.4% 9.4%\\nOne-Stage Training\\nMath Training ‚Äì ‚Äì 150B 20.5% 13.1% 37.6% 11.4% 6.5%\\nCode & Math Mixed Training ‚Äì 400B 150B 17.6% 12.1% 36.3% 19.7% 13.5%\\nTable 6 |Investigation of how code affects mathematical reasoning under different training\\nsettings. We experiment with DeepSeek-LLM 1.3B, and evaluate its mathematical reasoning\\nperformance without and with tool use via few-shot chain-of-thought prompting and few-shot\\nprogram-of-thought prompting, respectively.\\nimproves models‚Äô ability to do mathematical reasoning both with and without tool use.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='program-of-thought prompting, respectively.\\nimproves models‚Äô ability to do mathematical reasoning both with and without tool use.\\nTo study how code training affects mathematical reasoning, we experimented with the\\nfollowing two-stage training and one-stage training settings:\\nTwo-Stage Training\\n‚Ä¢ Code Training for 400B Tokens‚ÜíMath Training for 150B Tokens: We train DeepSeek-\\nLLM 1.3B for 400B code tokens followed by 150B math tokens;\\n‚Ä¢ General Training for 400B Tokens ‚ÜíMath Training for 150B Tokens : As a control\\nexperiment, we also experiment with general tokens (sampled from a large-scale general\\ncorpus created by DeepSeek-AI) instead of code tokens in the first stage of training, in an\\nattempt to investigate the advantages of code tokens over general tokens in improving\\nmathematical reasoning.\\nOne-Stage Training\\n‚Ä¢ Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\\n‚Ä¢ Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='One-Stage Training\\n‚Ä¢ Math Training for 150B Tokens: We train DeepSeek-LLM 1.3B for 150B math tokens;\\n‚Ä¢ Training on a mixture of 400B Code Tokens and 150B Math Tokens: Math training fol-\\nlowing code training degrades coding performance. We investigate whether code tokens,\\nwhen mixed with math tokens for one-stage training, would still improve mathematical\\nreasoning and also alleviate the problem of catastrophic forgetting.\\nResults Table 6 and Table 7 demonstrate the downstream performance under different training\\nsettings.\\nCode training benefits program-aided mathematical reasoning, both under the two-stage\\ntraining and one-stage training settings. As shown in Table 6, under the two-stage training\\nsetting, code training alone already significantly enhances the ability to solve GSM8K and\\nMATH problems using Python. Math training in the second stage yields further improvements.\\nInterestingly, under the one-stage training setting, mixing code tokens and math tokens effec-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 15, 'page_label': '16', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='MATH problems using Python. Math training in the second stage yields further improvements.\\nInterestingly, under the one-stage training setting, mixing code tokens and math tokens effec-\\ntively mitigates the issue of catastrophic forgetting that arises from two-stage training, and also\\nsynergizes coding (Table 7) and program-aided mathematical reasoning (Table 6).\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Training Setting Training Tokens MMLU BBH HumanEval (Pass@1) MBPP (Pass@1)\\nGeneral Code Math\\nNo Continual Training ‚Äì ‚Äì ‚Äì 24.5% 28.1% 12.2% 13.0%\\nTwo-Stage Training\\nStage 1: General Training 400B ‚Äì ‚Äì 25.9% 27.7% 15.2% 13.6%\\nStage 2: Math Training ‚Äì ‚Äì 150B 33.1% 32.7% 12.8% 13.2%\\nStage 1: Code Training ‚Äì 400B ‚Äì 25.0% 31.5% 25.0% 40.0%\\nStage 2: Math Training ‚Äì ‚Äì 150B 36.2% 35.3% 12.2% 17.0%\\nOne-Stage Training\\nMath Training ‚Äì ‚Äì 150B 32.3% 32.5% 11.6% 13.2%\\nCode & Math Mixed Training ‚Äì 400B 150B 33.5% 35.6% 29.3% 39.4%\\nTable 7 |Investigation of how different settings of code and math training affect model perfor-\\nmance of language understanding, reasoning, and coding. We experiment with DeepSeek-LLM\\n1.3B. We evaluate the models on MMLU and BBH using few-shot chain-of-thought prompting.\\nOn HumanEval and MBPP , we conduct zero-shot and few-shot evaluations, respectively.\\nModel Size ArXiv Corpus\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SATMMLU\\nSTEMCMATHGaokao\\nMathCloze\\nGaokao'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Model Size ArXiv Corpus\\nEnglish Benchmarks Chinese Benchmarks\\nGSM8K MATH OCW SATMMLU\\nSTEMCMATHGaokao\\nMathCloze\\nGaokao\\nMathQA\\nDeepSeek-LLM 1.3B\\nNo Math Training 2.9% 3.0% 2.9% 15.6% 19.5% 12.3% 0.8% 17.9%\\nMathPile 2.7% 3.3% 2.2% 12.5% 15.7% 1.2% 0.0% 2.8%\\nArXiv-RedPajama 3.3% 3.4% 4.0% 9.4% 9.0% 7.4% 0.8% 2.3%\\nDeepSeek-Coder-Base-v1.5 7B\\nNo Math Training 29.0% 12.5% 6.6% 40.6% 38.1% 45.9% 5.9% 21.1%\\nMathPile 23.6% 11.5% 7.0% 46.9% 35.8% 37.9% 4.2% 25.6%\\nArXiv-RedPajama 28.1% 11.1% 7.7% 50.0% 35.2% 42.6% 7.6% 24.8%\\nTable 8 |Effect of math training on different arXiv datasets. Model performance is evaluated\\nwith few-shot chain-of-thought prompting.\\nArXiv Corpus miniF2F-valid miniF2F-test\\nNo Math Training 20.1% 21.7%\\nMathPile 16.8% 16.4%\\nArXiv-RedPajama 14.8% 11.9%\\nTable 9 |Effect of math training on different arXiv corpora, the base model being DeepSeek-\\nCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ArXiv-RedPajama 14.8% 11.9%\\nTable 9 |Effect of math training on different arXiv corpora, the base model being DeepSeek-\\nCoder-Base-v1.5 7B. We evaluate informal-to-formal proving in Isabelle.\\nCode training also improves mathematical reasoning without tool use. Under the two-stage\\ntraining setting, the initial stage of code training already results in moderate enhancements.\\nIt also boosts the efficiency of the subsequent math training, eventually leading to the best\\nperformance. However, combining code tokens and math tokens for one-stage training com-\\npromises mathematical reasoning without tool use. One conjecture is that DeepSeek-LLM 1.3B,\\ndue to its limited scale, lacks the capacity to fully assimilate both code and mathematical data\\nsimultaneously.\\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 16, 'page_label': '17', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='simultaneously.\\n5.1.2. ArXiv Papers Seem Ineffective in Improving Mathematical Reasoning\\nArXiv papers are commonly included as a component of math pre-training data (Azerbayev\\net al., 2023; Lewkowycz et al., 2022a; Polu and Sutskever, 2020; Wang et al., 2023c). However,\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='detailed analysis regarding their impact on mathematical reasoning has not been extensively\\nconducted. Perhaps counter-intuitively, according to our experiments, arXiv papers seem\\nineffective in improving mathematical reasoning. We experiment with models of different sizes,\\nincluding DeepSeek-LLM 1.3B and DeepSeek-Coder-Base-v1.5 7B (Guo et al., 2024), using arXiv\\ncorpora that underwent varied processing pipelines:\\n‚Ä¢ MathPile (Wang et al., 2023c): an 8.9B-token corpus developed with cleaning and filtering\\nheuristic rules, over 85% of which are scientific arXiv papers;\\n‚Ä¢ ArXiv-RedPajama (Computer, 2023): the entirety of arXiv LaTeX files with preambles,\\ncomments, macros, and bibliographies removed, totaling 28.0B tokens.\\nIn our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-\\nCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='In our experiments, we separately train DeepSeek-LLM 1.3B for 150B tokens and DeepSeek-\\nCoder-Base-v1.5 7B for 40B tokens on each arXiv corpus. It seems that arXiv papers are ineffective\\nin improving mathematical reasoning. When trained on a arXiv-only corpus, both models dis-\\nplay no notable improvements or even deterioration across various mathematical benchmarks of\\ndifferent complexities employed in this study. These benchmarks include quantitative reasoning\\ndatasets like GSM8K and MATH (Table 8), multiple-choice challenges like MMLU-STEM (Table\\n8), and formal mathematics like miniF2F (Table 9).\\nHowever, this conclusion has its limitations and should be taken with a grain of salt. We\\nhave not yet studied:\\n‚Ä¢ The impact of arXiv tokens on specific math-related tasks not included in this research,\\nsuch as informalization of theorems which is to convert formal statements or proofs to\\ntheir informal versions;\\n‚Ä¢ The effect of arXiv tokens when combined with other types of data;'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='such as informalization of theorems which is to convert formal statements or proofs to\\ntheir informal versions;\\n‚Ä¢ The effect of arXiv tokens when combined with other types of data;\\n‚Ä¢ Whether the benefits of arXiv papers would manifest themselves at a larger model scale.\\nThus, further exploration is required, which we leave for future studies.\\n5.2. Insights of Reinforcement Learning\\n5.2.1. Towards to a Unified Paradigm\\nIn this section, we provide a unified paradigm to analyze different training methods, such as\\nSFT, RFT, DPO, PPO, GRPO, and further conduct experiments to explore the factors of the\\nunified paradigm. Generally, the gradient with respect to the parameter ùúÉof a training method\\ncan be written as:\\n‚àáùúÉJA(ùúÉ)= E[(ùëû, ùëú)‚àºD|       {z       }\\nùê∑ùëéùë°ùëé ùëÜùëúùë¢ùëüùëêùëí\\n]\\n¬©\\xad\\xad\\xad\\n¬´\\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂A(ùëû, ùëú, ùë°, ùúãùëüùëì )\\n|               {z               }\\nùê∫ùëüùëéùëëùëñùëíùëõùë° ùê∂ùëúùëíùëìùëìùëñùëêùëñùëíùëõùë°\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n¬™¬Æ¬Æ¬Æ\\n¬¨\\n. (5)\\nThere exist three key components: 1) Data SourceD, which determines the training data; 2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 17, 'page_label': '18', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ùê∫ùê∂A(ùëû, ùëú, ùë°, ùúãùëüùëì )\\n|               {z               }\\nùê∫ùëüùëéùëëùëñùëíùëõùë° ùê∂ùëúùëíùëìùëìùëñùëêùëñùëíùëõùë°\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n¬™¬Æ¬Æ¬Æ\\n¬¨\\n. (5)\\nThere exist three key components: 1) Data SourceD, which determines the training data; 2)\\nReward Functionùúãùëüùëì , which is the source of the training reward signal; 3) Algorithm A: which\\nprocesses the training data and the reward signal to the gradient coefficient ùê∫ùê∂ that determines\\nthe magnitude of the penalty or reinforcement for the data. We analyze several representative\\nmethods based on such a unified paradigm:\\n‚Ä¢ Supervised Fine-tuning (SFT): SFT fine-tunes pretrained model on human selected SFT\\ndata.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Methods Data Source Reward Function Gradient Coefficient\\nSFT ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ) - 1\\nRFT ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû) Rule Equation 10\\nDPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû) Rule Equation 14\\nOnline RFT ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû) Rule Equation 10\\nPPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû) Model Equation 18\\nGRPO ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉ(ùëÇ|ùëû) Model Equation 21\\nTable 10 |The data source and gradient coefficient of different methods. ùëÉùë†ùëìùë° denotes the data\\ndistribution of supervised fine-tuning datasets. ùúãùúÉùë†ùëìùë° and ùúãùúÉ denote the supervised fine-tuned\\nmodel and the real-time policy model during the online training process, respectively.\\n0 2000 4000 6000 8000\\nSteps\\n56\\n58\\n60\\n62\\n64\\n66Acc (%)\\nGSM8K\\n0 2000 4000 6000 8000\\nSteps\\n27\\n28\\n29\\n30Acc (%)\\nMATH\\nRFT Online RFT GRPO+OS GRPO+PS\\nFigure 5 |Performance of the DeepSeekMath-Instruct 1.3B model, which was further trained\\nusing various methods, on two benchmarks.\\n‚Ä¢ Rejection Sampling Fine-tuning (RFT) : RFT further fine-tunes the SFT model on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='using various methods, on two benchmarks.\\n‚Ä¢ Rejection Sampling Fine-tuning (RFT) : RFT further fine-tunes the SFT model on the\\nfiltered outputs sampled from the SFT model based on SFT questions. RFT filters the\\noutputs based on the correctness of their answers.\\n‚Ä¢ Direct Preference Optimization (DPO): DPO further refines the SFT model by fine-tuning\\nit on augmented outputs sampled from the SFT model, using pair-wise DPO loss.\\n‚Ä¢ Online Rejection Sampling Fine-tuning (Online RFT): Different from RFT, Online RFT\\ninitiates the policy model using the SFT model and refines it by fine-tuning with the\\naugmented outputs sampled from the real-time policy model.\\n‚Ä¢ PPO/GRPO: PPO/GRPO initializes the policy model using the SFT model and reinforces\\nit with the outputs sampled from the real-time policy model.\\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a\\nmore detailed derivation process.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 18, 'page_label': '19', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='it with the outputs sampled from the real-time policy model.\\nWe summarize the components of these methods in Table 10. Please refer to Appendix A.1 for a\\nmore detailed derivation process.\\nObservation about Data Source We divide the data source into two categories, online sam-\\npling, and offline sampling. Online sampling denotes that the training data is from the explo-\\nration results of the real-time training policy model, while offline sampling denotes that the\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='0 1300 2300 3300 4300 5300\\nSteps\\n83\\n84\\n85\\n86\\n87\\n88\\n89Acc (%)\\nGSM8K\\n0 1300 2300 3300 4300 5300\\nSteps\\n47\\n48\\n49\\n50\\n51\\n52Acc (%)\\nMATH\\nIteration-0 Iteration-1 Iteration-2\\nFigure 6 |Performance of iterative reinforcement learning with DeepSeekMath-Instruct 7B on\\ntwo benchmarks.\\ntraining data is from the sampling results of the initial SFT model. RFT and DPO follow the\\noffline style, while Online RFT and GRPO follow the online style.\\nAs shown in Figure 5, we find that the Online RFT significantly outperforms RFT on two\\nbenchmarks. Specifically, Online RFT is comparable to RFT in the early stage of training but\\ngains an absolute advantage in the later stage, demonstrating the superiority of online training.\\nThis is intuitive, as in the initial stage, the actor and the SFT model exhibit close resemblance,\\nwith the sampled data revealing only minor differences. In the later stage, however, the data\\nsampled from the actor will exhibit more significant differences, and real-time data sampling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='with the sampled data revealing only minor differences. In the later stage, however, the data\\nsampled from the actor will exhibit more significant differences, and real-time data sampling\\nwill offer greater advantages.\\nObservation about Gradient Coefficient The algorithm processes the reward signal to the\\ngradient coefficient to update the model parameter. We divide the reward function as ‚ÄòRule‚Äô\\nand ‚ÄòModel‚Äô in our experiments. Rule refers to judging the quality of a response based on\\nthe correctness of the answer, and Model denotes that we train a reward model to score each\\nresponse. The training data of the reward model is based on the rule judgment. Equations 10\\nand 21 highlight a key difference between GRPO and Online RFT: GRPO uniquely adjusts its\\ngradient coefficient based on the reward value provided by the reward model. This allows for\\ndifferential reinforcement and penalization of responses according to their varying magnitudes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 19, 'page_label': '20', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='gradient coefficient based on the reward value provided by the reward model. This allows for\\ndifferential reinforcement and penalization of responses according to their varying magnitudes.\\nIn contrast, Online RFT lacks this feature; it does not penalize incorrect responses and uniformly\\nreinforces all responses with correct answers at the same level of intensity.\\nAs demonstrated in Figure 5, GRPO surpasses online RFT, thereby highlighting the efficiency\\nof altering positive and negative gradient coefficients. In addition, GRPO+PS shows superior\\nperformance compared to GRPO+OS, indicating the benefits of using fine-grained, step-aware\\ngradient coefficients. Furthermore, we explore the iterative RL, in our experiments, we conduct\\ntwo rounds of iteration. As shown in Figure 6, we notice that the iterative RL significantly\\nimproves the performance, especially at the first iteration.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='1 4 8 16 32 64\\nK: The number of candidates\\n82\\n84\\n86\\n88\\n90\\n92\\n94\\n96\\n98Acc (%)\\nGSM8K\\n1 4 8 16 32 64\\nK: The number of candidates\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85Acc (%)\\nMATH\\nMaj@K-Instruct Maj@K-RL Pass@K-Instruct Pass@K-RL\\nFigure 7 |The Maj@K and Pass@K of SFT and RL DeepSeekMath 7B on GSM8K and MATH\\n(temperature 0.7). It was noted that RL enhances Maj@K but not Pass@K.\\n5.2.2. Why RL Works?\\nIn this paper, we conduct reinforcement learning based on a subset of instruction tuning\\ndata, and it achieves significant performance enhancement upon the instruction tuning model.\\nTo further explain why reinforcement learning works. We evaluate the Pass@K and Maj@K\\naccuracy of the Instruct and RL models on two benchmarks. As shown in Figure 7, RL enhances\\nMaj@K‚Äôs performance but not Pass@K. These findings indicate that RL enhances the model‚Äôs\\noverall performance by rendering the output distribution more robust, in other words, it seems'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Maj@K‚Äôs performance but not Pass@K. These findings indicate that RL enhances the model‚Äôs\\noverall performance by rendering the output distribution more robust, in other words, it seems\\nthat the improvement is attributed to boosting the correct response from TopK rather than\\nthe enhancement of fundamental capabilities. Similarly, (Wang et al., 2023a) identified a\\nmisalignment problem in reasoning tasks within the SFT model, showing that the reasoning\\nperformance of SFT models can be improved through a series of preference alignment strategies\\n(Song et al., 2023; Wang et al., 2023a; Yuan et al., 2023b).\\n5.2.3. How to Achieve More Effective RL?\\nWe demonstrate RL works pretty well in mathematical reasoning tasks. We also provide a unified\\nparadigm to understand different representative training methods. Within this paradigm, all\\nmethods are conceptualized as either direct or simplified RL techniques. As summarized in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='paradigm to understand different representative training methods. Within this paradigm, all\\nmethods are conceptualized as either direct or simplified RL techniques. As summarized in\\nEquation 5, there exist three key components: Data Source, Algorithm, and Reward Function.\\nWe provide some potential future directions about the three components.\\nData Source Data source is the raw material of all training methods. In the context of RL, we\\nspecifically refer to the data source as the unlabeled questions with the outputs sampled from\\nthe policy model. In this paper, we only use the questions from the instruction tuning stage and\\na naive nucleus sampling to sample outputs. We think this is a potential reason that our RL\\npipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline\\non out-of-distribution question prompts, in conjunction with advanced sampling (decoding)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 20, 'page_label': '21', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='pipeline only improves the Maj@K performance. In the future, we will explore our RL pipeline\\non out-of-distribution question prompts, in conjunction with advanced sampling (decoding)\\nstrategies, like those based on tree-search methods (Yao et al., 2023). Also, theefficient inference\\ntechniques (Kwon et al., 2023; Leviathan et al., 2023; Xia et al., 2023, 2024), which determines\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='the exploration efficiency of policy models, also play an exceedingly important role.\\nAlgorithms Algorithms process the data and reward signal to the gradient coefficient to update\\nthe model parameter. Based on Equation 5, to some extent, all methods now fully TRUST the\\nsignal of the reward function to increase or decrease the conditional probability of a certain\\ntoken. However, it is impossible to ensure the reward signal is always reliable, especially in\\nextremely complex tasks. For example, even the PRM800K datasets (Lightman et al., 2023),\\nwhich have been carefully annotated by well-trained annotators, still contain approximately 20%\\nof incorrectly annotations7. To this end, we will explore the reinforcement learning algorithm\\nthat is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,\\n2023) alignment methods will bring a fundamental change to the learning algorithms.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='that is robust against noisy reward signals. We believe such WEAK-TO-STRONG (Burns et al.,\\n2023) alignment methods will bring a fundamental change to the learning algorithms.\\nReward Function Reward function is the source of the training signal. In RL, the reward\\nfunction is usually the neural reward model. We think there exist three important directions for\\nreward models: 1) How to enhance the generalization ability of the reward model. The reward\\nmodel must be effectively generalized to handle out-of-distribution questions and advanced\\ndecoding outputs; otherwise, reinforcement learning may merely stabilize the distribution of\\nLLMs rather than improve their fundamental capabilities; 2) How to reflect the uncertainty\\nof reward model. The uncertainty could potentially act as a linking bridge between the weak\\nreward model and the weak-to-strong learning algorithms; 3) How to efficiently build high-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='of reward model. The uncertainty could potentially act as a linking bridge between the weak\\nreward model and the weak-to-strong learning algorithms; 3) How to efficiently build high-\\nquality process reward models that can provide fine-grained training signals for the reasoning\\nprocess (Lightman et al., 2023; Wang et al., 2023b).\\n6. Conclusion, Limitation, and Future Work\\nWe present DeepSeekMath, which outperforms all open-source models on the competition-\\nlevel MATH benchmark and approaches the performance of closed models. DeepSeekMath is\\ninitialized with DeepSeek-Coder-v1.5 7B and undergoes continual training for 500B tokens, with\\na significant component of the training data being 120B math tokens sourced from Common\\nCrawl. Our extensive ablation study shows web pages offer significant potential for high-quality\\nmathematical data, while arXiv may not as beneficial as we expected. We introduce Group'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Crawl. Our extensive ablation study shows web pages offer significant potential for high-quality\\nmathematical data, while arXiv may not as beneficial as we expected. We introduce Group\\nRelative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), which\\ncan notably improve mathematical reasoning capabilities with less memory consumption. The\\nexperiment results show that GRPO is effective even if DeepSeekMath-Instruct 7B has reached\\na high score on benchmarks. We also provide a unified paradigm to understand a series of\\nmethods and summarize several potential directions for more effective reinforcement learning.\\nAlthough DeepSeekMath achieves impressive scores on quantitative reasoning benchmarks,\\nits capability on geometry and theorem-proof are relatively weaker than closed models. For\\ninstance, in our dry run, the model cannot handle problems related to triangles and ellipses,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 21, 'page_label': '22', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='its capability on geometry and theorem-proof are relatively weaker than closed models. For\\ninstance, in our dry run, the model cannot handle problems related to triangles and ellipses,\\nwhich may indicate data selection bias in pre-training and fine-tuning. In addition, restricted\\nby the model scale, DeepSeekMath is worse than GPT-4 on few-shot capability. GPT-4 could\\nimprove its performance with few-shot inputs, while DeepSeekMath shows similar performance\\nin zero-shot and few-shot evaluation. In the future, we will further improve our engineered\\ndata selection pipeline to construct more high-quality pre-trained corpus. In addition, we will\\nexplore the potential directions (Section 5.2.3) for more effective reinforcement learning of LLMs.\\n7https://github.com/openai/prm800k/issues/12#issuecomment-1728491852\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='References\\nR. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\\nK. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou, J. Schrittwieser, A. Glaese, J. Chen,\\nE. Pitler, T. P . Lillicrap, A. Lazaridou, O. Firat, J. Molloy, M. Isard, P . R. Barham, T. Hennigan,\\nB. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,\\nK. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,\\nB. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,\\nM. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable multimodal\\nmodels. CoRR, abs/2312.11805, 2023. doi: 10.48550/ARXIV.2312.11805. URL https:\\n//doi.org/10.48550/arXiv.2312.11805.\\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\\n2021.\\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Bider-\\nman, and S. Welleck. Llemma: An open language model for mathematics. arXiv preprint\\narXiv:2310.10631, 2023.\\nJ. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, et al. Qwen\\ntechnical report. arXiv preprint arXiv:2309.16609, 2023.\\nC. Burns, P . Izmailov, J. H. Kirchner, B. Baker, L. Gao, L. Aschenbrenner, Y. Chen, A. Ecoffet,\\nM. Joglekar, J. Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with\\nweak supervision. arXiv preprint arXiv:2312.09390, 2023.\\nChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c\\nom/THUDM/ChatGLM3.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='ChatGLM3 Team. Chatglm3 series: Open bilingual chat llms, 2023. URL https://github.c\\nom/THUDM/ChatGLM3.\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,\\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P . Tillet,\\nF. P . Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\\nA. N. Carr, J. Leike, J. Achiam, V . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\\nM. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\\nURL https://arxiv.org/abs/2107.03374.\\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 22, 'page_label': '23', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='URL https://arxiv.org/abs/2107.03374.\\nW. Chen, X. Ma, X. Wang, and W. W. Cohen. Program of thoughts prompting: Disentangling\\ncomputation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022. doi:\\n10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\\nK. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168, 2021.\\nT. Computer. Redpajama: an open dataset for training large language models, Oct. 2023. URL\\nhttps://github.com/togethercomputer/RedPajama-Data.\\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,\\nabs/2401.02954, 2024. doi: 10.48550/ARXIV.2401.02954. URL https://doi.org/10.485\\n50/arXiv.2401.02954.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. Glm: General language model\\npretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 320‚Äì335,\\n2022.\\nL. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang, J. Callan, and G. Neubig. PAL: program-\\naided language models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and\\nJ. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July\\n2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,\\npages 10764‚Äì10799. PMLR, 2023. URL https://proceedings.mlr.press/v202/gao23f.\\nhtml.\\nZ. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, M. Huang, N. Duan, and W. Chen. Tora: A tool-\\nintegrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.\\ndoi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745\\n2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='integrated reasoning agent for mathematical problem solving. CoRR, abs/2309.17452, 2023.\\ndoi: 10.48550/ARXIV.2309.17452. URL https://doi.org/10.48550/arXiv.2309.1745\\n2.\\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming\\n‚Äì the rise of code intelligence, 2024.\\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\\n2021.\\nHigh-flyer. Hai-llm: È´òÊïà‰∏îËΩªÈáèÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂ∑•ÂÖ∑, 2023. URL https://www.high-flyer.c\\nn/en/blog/hai-llm.\\nInflection AI. Inflection-2, 2023. URL https://inflection.ai/inflection-2.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2021.\\nHigh-flyer. Hai-llm: È´òÊïà‰∏îËΩªÈáèÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂ∑•ÂÖ∑, 2023. URL https://www.high-flyer.c\\nn/en/blog/hai-llm.\\nInflection AI. Inflection-2, 2023. URL https://inflection.ai/inflection-2.\\nA. Q. Jiang, S. Welleck, J. P . Zhou, W. Li, J. Liu, M. Jamnik, T. Lacroix, Y. Wu, and G. Lample. Draft,\\nsketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint\\narXiv:2210.12283, 2022.\\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\\nA. Joulin, E. Grave, P . Bojanowski, M. Douze, H. J√©gou, and T. Mikolov. Fasttext. zip: Compress-\\ning text classification models. arXiv preprint arXiv:1612.03651, 2016.\\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\\nEfficient memory management for large language model serving with pagedattention. In'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 23, 'page_label': '24', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\\nEfficient memory management for large language model serving with pagedattention. In\\nProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative\\ndecoding. In International Conference on Machine Learning, pages 19274‚Äì19286. PMLR,\\n2023.\\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . Ramasesh, A. Slone,\\nC. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with\\nlanguage models. Advances in Neural Information Processing Systems, 35:3843‚Äì3857, 2022a.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . V . Ramasesh, A. Slone,\\nC. Anil, I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V . Misra. Solving\\nquantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal,\\nD. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems\\n35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New\\nOrleans, LA, USA, November 28 - December 9, 2022, 2022b. URL http://papers.nips.\\ncc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstr\\nact-Conference.html.\\nH. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI. Sutskever, and K. Cobbe. Let‚Äôs verify step by step. arXiv preprint arXiv:2305.20050, 2023.\\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101, 2017.\\nH. Luo, Q. Sun, C. Xu, P . Zhao, J. Lou, C. Tao, X. Geng, Q. Lin, S. Chen, and D. Zhang.\\nWizardmath: Empowering mathematical reasoning for large language models via reinforced\\nevol-instruct. arXiv preprint arXiv:2308.09583, 2023.\\nS. Mishra, M. Finlayson, P . Lu, L. Tang, S. Welleck, C. Baral, T. Rajpurohit, O. Tafjord, A. Sab-\\nharwal, P . Clark, and A. Kalyan. LILA: A unified benchmark for mathematical reasoning.\\nIn Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on\\nEmpirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab\\nEmirates, December 7-11, 2022, pages 5807‚Äì5832. Association for Computational Linguistics,\\n2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/\\n2022.emnlp-main.392.\\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='2022. doi: 10.18653/V1/2022.EMNLP-MAIN.392. URL https://doi.org/10.18653/v1/\\n2022.emnlp-main.392.\\nX. Nguyen, W. Zhang, X. Li, M. M. Aljunied, Q. Tan, L. Cheng, G. Chen, Y. Deng, S. Yang,\\nC. Liu, H. Zhang, and L. Bing. Seallms - large language models for southeast asia. CoRR,\\nabs/2312.00738, 2023. doi: 10.48550/ARXIV.2312.00738. URL https://doi.org/10.485\\n50/arXiv.2312.00738.\\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems, 35:27730‚Äì27744, 2022.\\nK. Paster, M. D. Santos, Z. Azerbayev, and J. Ba. Openwebmath: An open dataset of high-quality\\nmathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL\\nhttps://doi.org/10.48550/arXiv.2310.06786.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 24, 'page_label': '25', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mathematical web text. CoRR, abs/2310.06786, 2023. doi: 10.48550/ARXIV.2310.06786. URL\\nhttps://doi.org/10.48550/arXiv.2310.06786.\\nL. C. Paulson. Three years of experience with sledgehammer, a practical link between auto-\\nmatic and interactive theorem provers. In R. A. Schmidt, S. Schulz, and B. Konev, editors,\\nProceedings of the 2nd Workshopon Practical Aspects of Automated Reasoning, PAAR-2010,\\nEdinburgh, Scotland, UK, July 14, 2010, volume 9 of EPiC Series in Computing, pages 1‚Äì10.\\nEasyChair, 2010. doi: 10.29007/TNFD. URL https://doi.org/10.29007/tnfd.\\nS. Polu and I. Sutskever. Generative language modeling for automated theorem proving. CoRR,\\nabs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference\\noptimization: Your language model is secretly a reward model. 2023.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='J. Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/kl-app\\nrox.html.\\nJ. Schulman, P . Moritz, S. Levine, M. Jordan, and P . Abbeel. High-dimensional continuous\\ncontrol using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\\nJ. Schulman, F. Wolski, P . Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\\nD. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.\\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\\nfR3wGCk-IXp.\\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for\\nhuman alignment. arXiv preprint arXiv:2306.17492, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='fR3wGCk-IXp.\\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for\\nhuman alignment. arXiv preprint arXiv:2306.17492, 2023.\\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V . Le,\\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\\nthem. arXiv preprint arXiv:2210.09261, 2022.\\nT. Tao. Embracing change and resetting expectations, 2023. URL https://unlocked.micro\\nsoft.com/ai-anthology/terence-tao/.\\nH. Touvron, L. Martin, K. Stone, P . Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\\nP . Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,\\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\\nR. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P . S. Koura,\\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P . Mishra,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='R. Hou, H. Inan, M. Kardas, V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P . S. Koura,\\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P . Mishra,\\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P . Xu, Z. Yan,\\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288,\\n2023. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.\\n09288.\\nT. H. Trinh, Y. Wu, Q. V . Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature, 625(7995):476‚Äì482, 2024.\\nP . Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models\\nbetter reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 25, 'page_label': '26', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='P . Wang, L. Li, L. Chen, F. Song, B. Lin, Y. Cao, T. Liu, and Z. Sui. Making large language models\\nbetter reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023a.\\nP . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify\\nand reinforce llms step-by-step without human annotations. CoRR, abs/2312.08935, 2023b.\\nZ. Wang, R. Xia, and P . Liu. Generative AI for math: Part I - mathpile: A billion-token-scale\\npretraining corpus for math. CoRR, abs/2312.17120, 2023c. doi: 10.48550/ARXIV.2312.17120.\\nURL https://doi.org/10.48550/arXiv.2312.17120.\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V . Le, and D. Zhou.\\nChain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022.\\nURL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf\\n4f15af0f7b31abca4-Abstract-Conference.html.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='T. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese\\nelementary school math test?, 2023.\\nM. Wenzel, L. C. Paulson, and T. Nipkow. The isabelle framework. In O. A. Mohamed, C. A.\\nMu√±oz, and S. Tahar, editors, Theorem Proving in Higher Order Logics, 21st International\\nConference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings, volume 5170\\nof Lecture Notes in Computer Science, pages 33‚Äì38. Springer, 2008. doi: 10.1007/978-3-540-7\\n1067-7\\\\_7. URL https://doi.org/10.1007/978-3-540-71067-7_7 .\\nH. Xia, T. Ge, P . Wang, S.-Q. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting\\nspeculative execution for accelerating seq2seq generation. In H. Bouamor, J. Pino, and K. Bali,\\neditors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909‚Äì\\n3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20\\n23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='3925, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/20\\n23.findings-emnlp.257. URL https://aclanthology.org/2023.findings-emnlp.257.\\nH. Xia, Z. Yang, Q. Dong, P . Wang, Y. Li, T. Ge, T. Liu, W. Li, and Z. Sui. Unlocking efficiency\\nin large language model inference: A comprehensive survey of speculative decoding. arXiv\\npreprint arXiv:2401.07851, 2024.\\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:\\nDeliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,\\n2023.\\nL. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu.\\nMetamath: Bootstrap your own mathematical questions for large language models. CoRR,\\nabs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485\\n50/arXiv.2309.12284.\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10.485\\n50/arXiv.2309.12284.\\nZ. Yuan, H. Yuan, C. Li, G. Dong, C. Tan, and C. Zhou. Scaling relationship on learning\\nmathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023a.\\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align\\nlanguage models with human feedback without tears. arXiv preprint arXiv:2304.05302, 2023b.\\nX. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen. Mammoth: Building\\nmath generalist models through hybrid instruction tuning. CoRR, abs/2309.05653, 2023. doi:\\n10.48550/ARXIV.2309.05653. URL https://doi.org/10.48550/arXiv.2309.05653.\\nK. Zheng, J. M. Han, and S. Polu. Minif2f: a cross-system benchmark for formal olympiad-level\\nmathematics. arXiv preprint arXiv:2109.00110, 2021.\\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 26, 'page_label': '27', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='mathematics. arXiv preprint arXiv:2109.00110, 2021.\\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A\\nhuman-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.\\ndoi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A. Appendix\\nA.1. Analysis of Reinforcement Learning\\nWe provide the detailed derivation of the data source and gradient coefficient (algorithm and\\nreward function) across various methods, including SFT, RFT, Online RFT, DPO, PPO, and\\nGRPO.\\nA.1.1. Supervised Fine-tuning\\nThe objective of Supervised Fine-tuning is maximizing the following objective:\\nJùëÜùêπùëá(ùúÉ)= E[ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (6)\\nThe gradient of JùëÜùêπùëá(ùúÉ)is:\\n‚àáùúÉJùëÜùêπùëá = E[ùëû, ùëú‚àºùëÉùë†ùëìùë° (ùëÑ, ùëÇ)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\n‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (7)\\nData Source: The dataset employed for SFT. Reward Function: This can be regarded as human\\nselection. Gradient Coefficient: always set to 1.\\nA.1.2. Rejection Sampling Fine-tuning\\nRejection Sampling Fine-tuning first samples multiple outputs from the supervised fine-tuned\\nLLMs for each question, and then trains LLMs on the sampled outputs with the correct answer.\\nFormally, the objective of RFT is to maximize the following objectives:\\nJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 27, 'page_label': '28', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='Formally, the objective of RFT is to maximize the following objectives:\\nJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (8)\\nThe gradient of JùëÖùêπùëá(ùúÉ)is:\\n‚àáùúÉJùëÖùêπùëá(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (9)\\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\\nRule (whether the answer is correct or not). Gradient Coefficient:\\nùê∫ùê∂ùëÖùêπùëá(ùëû, ùëú, ùë°)= I(ùëú)=\\n(\\n1 the answer of o is correct\\n0 the answer of o is incorrect (10)\\nA.1.3. Online Rejection Sampling Fine-tuning\\nThe only difference between RFT and Online RFT is that the outputs of Online RFT are sampled\\nfrom the real-time policy model ùúãùúÉ, rather than from the SFT model ùúãùúÉùë†ùëìùë° . Therefore, the gradient\\nof online RFT is:\\n‚àáùúÉJùëÇùëõùëÖùêπùëá (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉ(ùëÇ|ùëû)]\\n \\n1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nI(ùëú)‚àáùúÉ log ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\n!\\n. (11)\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='A.1.4. Direct Preference Optimization (DPO)\\nThe objective of DPO is:\\nJùê∑ùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]log ùúé¬©\\xad\\n¬´\\nùõΩ 1\\n|ùëú+|\\n|ùëú+|‚àëÔ∏Å\\nùë°=1\\nlog\\nùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\nùúãref(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)‚àíùõΩ 1\\n|ùëú‚àí|\\n|ùëú‚àí|‚àëÔ∏Å\\nùë°=1\\nlog\\nùúãùúÉ(ùëú‚àí\\n<ùë°|ùëû, ùëú‚àí\\n<ùë°)\\nùúãref(ùëú‚àí\\n<ùë°|ùëû, ùëú‚àí\\n<ùë°)\\n¬™¬Æ\\n¬¨\\n(12)\\nThe gradient of Jùê∑ùëÉùëÇ(ùúÉ)is:\\n‚àáùúÉJùê∑ùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú+, ùëú‚àí‚àºùúãùë†ùëìùë° (ùëÇ|ùëû)]¬©\\xad\\n¬´\\n1\\n|ùëú+|\\n|ùëú+|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)‚àáùúÉlog ùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\n‚àí 1\\n|ùëú‚àí|\\n|ùëú‚àí|‚àëÔ∏Å\\nùë°=1\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)‚àáùúÉlog ùúãùúÉ(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)¬™¬Æ\\n¬¨\\n(13)\\nData Source: question in SFT dataset with outputs sampled from SFT model. Reward Function:\\nhuman preference in the general domain (can be ‚ÄòRule‚Äô in mathematical tasks). Gradient\\nCoefficient:\\nùê∫ùê∂ùê∑ùëÉùëÇ(ùëû, ùëú, ùë°)= ùúé\\n\\x12\\nùõΩlog\\nùúãùúÉ(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)\\nùúãref(ùëú‚àí\\nùë° |ùëû, ùëú‚àí\\n<ùë°)‚àíùõΩlog\\nùúãùúÉ(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\nùúãref(ùëú+\\nùë° |ùëû, ùëú+\\n<ùë°)\\n\\x13\\n(14)\\nA.1.5. Proximal Policy Optimization (PPO)\\nThe objective of PPO is:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n. (15)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='The objective of PPO is:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nmin\\n\\x14 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°, clip\\n\\x12 ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°), 1‚àíùúÄ, 1+ùúÄ\\n\\x13\\nùê¥ùë°\\n\\x15\\n. (15)\\nTo simplify the analysis, it is assumed that the model only has a single update following each\\nexploration stage, thereby ensuring that ùúãùúÉùëúùëôùëë = ùúãùúÉ. In this case, we can remove the min and clip\\noperation:\\nJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùë°|ùëû, ùëú<ùë°)ùê¥ùë°. (16)\\nThe gradient of JùëÉùëÉùëÇ(ùúÉ)is:\\n‚àáùúÉJùëÉùëÉùëÇ(ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), ùëú‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]1\\n|ùëú|\\n|ùëú|‚àëÔ∏Å\\nùë°=1\\nùê¥ùë°‚àáùúÉlog ùúãùúÉ(ùëúùë°|ùëû, ùëú<ùë°) (17)\\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\\nreward model. Gradient Coefficient:\\nùê∫ùê∂ùëÉùëÉùëÇ(ùëû, ùëú, ùë°, ùúãùúÉùëüùëö)= ùê¥ùë°, (18)\\nwhere ùê¥ùë° is the advantage, which is computed by applying Generalized Advantage Estimation\\n(GAE) (Schulman et al., 2015), based on the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì.\\nA.1.6. Group Relative Policy Optimization (GRPO)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 28, 'page_label': '29', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='(GAE) (Schulman et al., 2015), based on the rewards {ùëü‚â•ùë°}and a learned value function ùëâùúì.\\nA.1.6. Group Relative Policy Optimization (GRPO)\\nThe objective of GRPO is (assume ùúãùúÉùëúùëôùëë = ùúãùúÉ for simplified analysis):\\nJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x14 ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉùëúùëôùëë (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nÀÜùê¥ùëñ,ùë° ‚àíùõΩ(\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àílog\\nùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°) ‚àí1)\\n\\x15\\n.\\n(19)\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-30T00:25:21+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-30T00:25:21+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/Deepseek Math.pdf', 'total_pages': 30, 'page': 29, 'page_label': '30', 'source_file': 'Deepseek Math.pdf', 'file_type': 'pdf'}, page_content='The gradient of Jùê∫ùëÖùëÉùëÇ (ùúÉ)is:\\n‚àáùúÉJùê∫ùëÖùëÉùëÇ (ùúÉ)= E[ùëû‚àºùëÉùë†ùëìùë° (ùëÑ), {ùëúùëñ}ùê∫\\nùëñ=1 ‚àºùúãùúÉùëúùëôùëë (ùëÇ|ùëû)]\\n1\\nùê∫\\nùê∫‚àëÔ∏Å\\nùëñ=1\\n1\\n|ùëúùëñ|\\n|ùëúùëñ|‚àëÔ∏Å\\nùë°=1\\n\\x14\\nÀÜùê¥ùëñ,ùë° +ùõΩ\\n\\x12ùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëúùëñ,<ùë°) ‚àí1\\n\\x13\\x15\\n‚àáùúÉlog ùúãùúÉ(ùëúùëñ,ùë°|ùëû, ùëúùëñ,<ùë°).\\n(20)\\nData Source: question in SFT dataset with outputs sampled from policy model. Reward Function:\\nreward model. Gradient Coefficient:\\nùê∫ùê∂ùê∫ùëÖùëÉùëÇ (ùëû, ùëú, ùë°, ùúãùúÉùëüùëö)= ÀÜùê¥ùëñ,ùë° +ùõΩ\\n\\x12ùúãùëüùëíùëì (ùëúùëñ,ùë°|ùëúùëñ,<ùë°)\\nùúãùúÉ(ùëúùëñ,ùë°|ùëúùëñ,<ùë°) ‚àí1\\n\\x13\\n, (21)\\nwhere ÀÜùê¥ùëñ,ùë° is computed based on the group reward scores.\\n30')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2acd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 364 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:03<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (364, 384)\n",
      "Adding 364 documents to vector store...\n",
      "Successfully added 364 documents to vector store\n",
      "Total documents in collection: 906\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadd974",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "304acd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf86a3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x3174f42f0>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c3cde65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Exploration and Analysis of Reinforcement Learning'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 75.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_47740a54_119',\n",
       "  'content': 'more, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,\\nPPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training,\\noutcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so\\non to deeply investigate the essential elements of this paradigm.\\n‚Ä¢ Based on our unified paradigm, we explore the reasons behind the effectiveness of rein-\\nforcement learning, and summarize several potential directions to achieve more effective\\nreinforcement learning of LLMs.\\n1.2. Summary of Evaluations and Metrics\\n‚Ä¢ English and Chinese Mathematical Reasoning: We conduct comprehensive assessments\\nof our models on English and Chinese benchmarks, covering mathematical problems\\n3',\n",
       "  'metadata': {'moddate': '2024-04-30T00:25:21+00:00',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'source': '../data/pdf_files/Deepseek Math.pdf',\n",
       "   'author': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'source_file': 'Deepseek Math.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'title': '',\n",
       "   'trapped': '/False',\n",
       "   'page': 2,\n",
       "   'total_pages': 30,\n",
       "   'content_length': 854,\n",
       "   'creationdate': '2024-04-30T00:25:21+00:00',\n",
       "   'page_label': '3',\n",
       "   'doc_index': 119,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'},\n",
       "  'similarity_score': 0.010370969772338867,\n",
       "  'distance': 0.9896290302276611,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_e7c50531_265',\n",
       "  'content': 'more, we observe enhancements in the out-of-domain performance during the reinforce-\\nment learning process.\\n‚Ä¢ We provide a unified paradigm to understand different methods, such as RFT, DPO,\\nPPO, and GRPO. We also conduct extensive experiments, e.g., online v.s. offline training,\\noutcome v.s. process supervision, single-turn v.s. iterative reinforcement learning, and so\\non to deeply investigate the essential elements of this paradigm.\\n‚Ä¢ Based on our unified paradigm, we explore the reasons behind the effectiveness of rein-\\nforcement learning, and summarize several potential directions to achieve more effective\\nreinforcement learning of LLMs.\\n1.2. Summary of Evaluations and Metrics\\n‚Ä¢ English and Chinese Mathematical Reasoning: We conduct comprehensive assessments\\nof our models on English and Chinese benchmarks, covering mathematical problems\\n3',\n",
       "  'metadata': {'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'content_length': 854,\n",
       "   'total_pages': 30,\n",
       "   'doc_index': 265,\n",
       "   'source': '../data/pdf_files/Deepseek Math.pdf',\n",
       "   'creationdate': '2024-04-30T00:25:21+00:00',\n",
       "   'subject': '',\n",
       "   'trapped': '/False',\n",
       "   'page_label': '3',\n",
       "   'source_file': 'Deepseek Math.pdf',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'title': '',\n",
       "   'page': 2,\n",
       "   'author': '',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-30T00:25:21+00:00'},\n",
       "  'similarity_score': 0.010370969772338867,\n",
       "  'distance': 0.9896290302276611,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Exploration and Analysis of Reinforcement Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742814a4",
   "metadata": {},
   "source": [
    "### RAG Pipeline - VectorDB to LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    ## Retrieve the context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba202cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Exploration and Analysis of Reinforcement Learning'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 74.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text describes an exploration and analysis of reinforcement learning for Large Language Models (LLMs), focusing on a unified paradigm to understand different methods and conducting extensive experiments to investigate essential elements.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"Exploration and Analysis of Reinforcement Learning\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fd78a",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb06c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'The GloVe model'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 66.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found.\n",
      "Sources: []\n",
      "Confidence: 0.0\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300]+'...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely. \\nContext:{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"The GloVe model\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a83768f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is data trained on BERT'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 25.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "ing pre-training, the model is trained on unlabeled\n",
      "data over different pre-training tasks. For Ô¨Åne-\n",
      "tuning, the BERT model is Ô¨Årst initialized with\n",
      "the pre-trained parame"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ters, and all of the param-\n",
      "eters are Ô¨Åne-tuned using labeled data from the\n",
      "downstream tasks. Each downstream task has sep-\n",
      "arate Ô¨Åne-tuned models, even though they are ini-\n",
      "tialized with the same pre-trained parameters. The\n",
      "question-answering example in Figure 1 will serve\n",
      "as a running example for this section.\n",
      "A distinctive feature of BERT is its uniÔ¨Åed ar-\n",
      "chitecture across different tasks. There is mini-\n",
      "mal difference between the pre-trained architec-\n",
      "ture and the Ô¨Ånal downstream architecture.\n",
      "Model Architecture BERT‚Äôs model architec-\n",
      "ture is a multi-layer bidirectional Transformer en-\n",
      "coder based on the original implementation de-\n",
      "scribed in Vaswani et al. (2017) and released in\n",
      "the tensor2tensor library.1 Because the use\n",
      "of Transformers has become common and our im-\n",
      "\n",
      "ing pre-training, the model is trained on unlabeled\n",
      "data over different pre-training tasks. For Ô¨Åne-\n",
      "tuning, the BERT model is Ô¨Årst initialized with\n",
      "the pre-trained parameters, and all of the param-\n",
      "eters are Ô¨Åne-tuned using labeled data from the\n",
      "downstream tasks. Each downstream task has sep-\n",
      "arate Ô¨Åne-tuned models, even though they are ini-\n",
      "tialized with the same pre-trained parameters. The\n",
      "question-answering example in Figure 1 will serve\n",
      "as a running example for this section.\n",
      "A distinctive feature of BERT is its uniÔ¨Åed ar-\n",
      "chitecture across different tasks. There is mini-\n",
      "mal difference between the pre-trained architec-\n",
      "ture and the Ô¨Ånal downstream architecture.\n",
      "Model Architecture BERT‚Äôs model architec-\n",
      "ture is a multi-layer bidirectional Transformer en-\n",
      "coder based on the original implementation de-\n",
      "scribed in Vaswani et al. (2017) and released in\n",
      "the tensor2tensor library.1 Because the use\n",
      "of Transformers has become common and our im-\n",
      "\n",
      "ing pre-training, the model is trained on unlabeled\n",
      "data over different pre-training tasks. For Ô¨Åne-\n",
      "tuning, the BERT model is Ô¨Årst initialized with\n",
      "the pre-trained parameters, and all of the param-\n",
      "eters are Ô¨Åne-tuned using labeled data from the\n",
      "downstream tasks. Each downstream task has sep-\n",
      "arate Ô¨Åne-tuned models, even though they are ini-\n",
      "tialized with the same pre-trained parameters. The\n",
      "question-answering example in Figure 1 will serve\n",
      "as a running example for this section.\n",
      "A distinctive feature of BERT is its uniÔ¨Åed ar-\n",
      "chitecture across different tasks. There is mini-\n",
      "mal difference between the pre-trained architec-\n",
      "ture and the Ô¨Ånal downstream architecture.\n",
      "Model Architecture BERT‚Äôs model architec-\n",
      "ture is a multi-layer bidirectional Transformer en-\n",
      "coder based on the original implementation de-\n",
      "scribed in Vaswani et al. (2017) and released in\n",
      "the tensor2tensor library.1 Because the use\n",
      "of Transformers has become common and our im-\n",
      "\n",
      "Question: What is data trained on BERT\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: Unlabeled data over different pre-training tasks and labeled data from downstream tasks.\n",
      "\n",
      "Citations:\n",
      "\n",
      "[1] BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf (page 2)\n",
      "[2] BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf (page 2)\n",
      "[3] BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf (page 2)\n",
      "Summary: This refers to a combination of two types of data used in machine learning: \n",
      "\n",
      "1. Unlabeled data from various pre-training tasks, which helps the model learn general features and patterns.\n",
      "2. Labeled data from downstream tasks, which is used to fine-tune the model for a specific task.\n",
      "History: {'question': 'What is data trained on BERT', 'answer': 'Unlabeled data over different pre-training tasks and labeled data from downstream tasks.', 'sources': [{'source': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'page': 2, 'score': 0.22946995496749878, 'preview': 'ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT mo...'}, {'source': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'page': 2, 'score': 0.22946995496749878, 'preview': 'ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT mo...'}, {'source': 'BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'page': 2, 'score': 0.22946995496749878, 'preview': 'ing pre-training, the model is trained on unlabeled\\ndata over different pre-training tasks. For Ô¨Åne-\\ntuning, the BERT mo...'}], 'summary': 'This refers to a combination of two types of data used in machine learning: \\n\\n1. Unlabeled data from various pre-training tasks, which helps the model learn general features and patterns.\\n2. Labeled data from downstream tasks, which is used to fine-tune the model for a specific task.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = [] # Store query history\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120]+'...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "        \n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "        \n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "        \n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"What is data trained on BERT\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef241b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
